2022-03-30 17:05:45,795 - depth - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.11 (default, Jul 27 2021, 14:32:16) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: Tesla V100-SXM2-32GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.1, V10.1.243
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.6.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.3
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.7.0
OpenCV: 4.5.5
MMCV: 1.3.13
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.1
Depth: 0.0.0+5253be7
------------------------------------------------------------

2022-03-30 17:05:45,796 - depth - INFO - Distributed training: True
2022-03-30 17:05:46,066 - depth - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
backbone_norm_cfg = dict(type='LN', requires_grad=True)
conv_stem_norm_cfg = dict(type='BN', requires_grad=True)
model = dict(
    type='DepthEncoderDecoder',
    pretrained='nfs/checkpoints/swin_large_patch4_window7_224_22k.pth',
    backbone=dict(
        type='DepthFormerSwin',
        pretrain_img_size=224,
        embed_dims=192,
        patch_size=4,
        window_size=7,
        mlp_ratio=4,
        depths=[2, 2, 18, 2],
        num_heads=[6, 12, 24, 48],
        strides=(4, 2, 2, 2),
        out_indices=(0, 1, 2, 3),
        qkv_bias=True,
        qk_scale=None,
        patch_norm=True,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.3,
        use_abs_pos_embed=False,
        act_cfg=dict(type='GELU'),
        norm_cfg=dict(type='LN', requires_grad=True),
        pretrain_style='official',
        conv_norm_cfg=dict(type='BN', requires_grad=True),
        depth=50,
        num_stages=0),
    decode_head=dict(
        type='DenseDepthHead',
        in_channels=[64, 192, 384, 768, 1536],
        up_sample_channels=[64, 192, 384, 768, 1536],
        channels=64,
        align_corners=True,
        loss_decode=dict(type='SigLoss', valid_mask=True, loss_weight=1.0),
        act_cfg=dict(type='LeakyReLU', inplace=True),
        min_depth=0.001,
        max_depth=80),
    train_cfg=dict(),
    test_cfg=dict(mode='whole'),
    neck=dict(
        type='HAHIHeteroNeck',
        positional_encoding=dict(type='SinePositionalEncoding', num_feats=256),
        in_channels=[64, 192, 384, 768, 1536],
        out_channels=[64, 192, 384, 768, 1536],
        embedding_dim=512,
        scales=[1, 1, 1, 1, 1]))
dataset_type = 'KITTIDataset'
data_root = 'data/kitti'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (352, 704)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='DepthLoadAnnotations'),
    dict(type='KBCrop', depth=True),
    dict(type='RandomRotate', prob=0.5, degree=2.5),
    dict(type='RandomFlip', prob=0.5),
    dict(type='RandomCrop', crop_size=(352, 704)),
    dict(
        type='ColorAug',
        prob=0.5,
        gamma_range=[0.9, 1.1],
        brightness_range=[0.9, 1.1],
        color_range=[0.9, 1.1]),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'depth_gt'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='KBCrop', depth=False),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1216, 352),
        flip=True,
        flip_direction='horizontal',
        transforms=[
            dict(type='RandomFlip', direction='horizontal'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='KITTIDataset',
        data_root='data/kitti',
        img_dir='input',
        ann_dir='gt_depth',
        depth_scale=256,
        split='kitti_eigen_train.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='DepthLoadAnnotations'),
            dict(type='KBCrop', depth=True),
            dict(type='RandomRotate', prob=0.5, degree=2.5),
            dict(type='RandomFlip', prob=0.5),
            dict(type='RandomCrop', crop_size=(352, 704)),
            dict(
                type='ColorAug',
                prob=0.5,
                gamma_range=[0.9, 1.1],
                brightness_range=[0.9, 1.1],
                color_range=[0.9, 1.1]),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'depth_gt'])
        ],
        garg_crop=True,
        eigen_crop=False,
        min_depth=0.001,
        max_depth=80),
    val=dict(
        type='KITTIDataset',
        data_root='data/kitti',
        img_dir='input',
        ann_dir='gt_depth',
        depth_scale=256,
        split='kitti_eigen_test.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='KBCrop', depth=False),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1216, 352),
                flip=True,
                flip_direction='horizontal',
                transforms=[
                    dict(type='RandomFlip', direction='horizontal'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        garg_crop=True,
        eigen_crop=False,
        min_depth=0.001,
        max_depth=80),
    test=dict(
        type='KITTIDataset',
        data_root='data/kitti',
        img_dir='input',
        ann_dir='gt_depth',
        depth_scale=256,
        split='kitti_eigen_test.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='KBCrop', depth=False),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1216, 352),
                flip=True,
                flip_direction='horizontal',
                transforms=[
                    dict(type='RandomFlip', direction='horizontal'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        garg_crop=True,
        eigen_crop=False,
        min_depth=0.001,
        max_depth=80))
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook', by_epoch=False),
        dict(type='TensorboardLoggerHook')
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
max_lr = 0.0001
optimizer = dict(
    type='AdamW',
    lr=0.0001,
    betas=(0.9, 0.999),
    weight_decay=0.01,
    paramwise_cfg=dict(
        custom_keys=dict(
            absolute_pos_embed=dict(decay_mult=0.0),
            relative_position_bias_table=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0))))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=12800,
    warmup_ratio=0.001,
    min_lr_ratio=1e-08,
    by_epoch=False)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
runner = dict(type='IterBasedRunner', max_iters=38400)
checkpoint_config = dict(by_epoch=False, max_keep_ckpts=2, interval=1600)
evaluation = dict(
    by_epoch=False,
    start=0,
    interval=800,
    pre_eval=True,
    rule='less',
    save_best='abs_rel',
    greater_keys=('a1', 'a2', 'a3'),
    less_keys=('abs_rel', 'rmse'))
work_dir = 'nfs/saves/depthformer/depthformer_swinl_22k_kitti'
gpu_ids = range(0, 1)

2022-03-30 17:05:48,653 - depth - INFO - Use load_from_local loader
Name of parameter - Initialization information

backbone.patch_embed.projection.weight - torch.Size([192, 3, 4, 4]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.patch_embed.projection.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.patch_embed.norm.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.patch_embed.norm.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.norm1.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.norm1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.attn.w_msa.qkv.bias - torch.Size([576]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.attn.w_msa.proj.weight - torch.Size([192, 192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.attn.w_msa.proj.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.norm2.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.norm2.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.ffn.layers.0.0.weight - torch.Size([768, 192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.ffn.layers.0.0.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.ffn.layers.1.weight - torch.Size([192, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.ffn.layers.1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.norm1.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.norm1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.attn.w_msa.qkv.bias - torch.Size([576]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.attn.w_msa.proj.weight - torch.Size([192, 192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.attn.w_msa.proj.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.norm2.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.norm2.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.ffn.layers.0.0.weight - torch.Size([768, 192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.ffn.layers.0.0.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.ffn.layers.1.weight - torch.Size([192, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.ffn.layers.1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.downsample.norm.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.downsample.norm.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.downsample.reduction.weight - torch.Size([384, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.downsample.norm.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.downsample.norm.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.downsample.reduction.weight - torch.Size([768, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.downsample.norm.weight - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.downsample.norm.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.downsample.reduction.weight - torch.Size([1536, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.norm1.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.norm1.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 48]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.attn.w_msa.qkv.weight - torch.Size([4608, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.attn.w_msa.qkv.bias - torch.Size([4608]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.attn.w_msa.proj.weight - torch.Size([1536, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.attn.w_msa.proj.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.norm2.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.norm2.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.ffn.layers.0.0.weight - torch.Size([6144, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.ffn.layers.0.0.bias - torch.Size([6144]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.ffn.layers.1.weight - torch.Size([1536, 6144]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.ffn.layers.1.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.norm1.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.norm1.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 48]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.attn.w_msa.qkv.weight - torch.Size([4608, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.attn.w_msa.qkv.bias - torch.Size([4608]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.attn.w_msa.proj.weight - torch.Size([1536, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.attn.w_msa.proj.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.norm2.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.norm2.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.ffn.layers.0.0.weight - torch.Size([6144, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.ffn.layers.0.0.bias - torch.Size([6144]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.ffn.layers.1.weight - torch.Size([1536, 6144]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.ffn.layers.1.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.norm0.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm0.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm3.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm3.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_depth.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_depth.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.0.conv.weight - torch.Size([1536, 1536, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.0.conv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.1.convA.conv.weight - torch.Size([768, 2304, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.1.convA.conv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.1.convB.conv.weight - torch.Size([768, 768, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.1.convB.conv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.2.convA.conv.weight - torch.Size([384, 1152, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.2.convA.conv.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.2.convB.conv.weight - torch.Size([384, 384, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.2.convB.conv.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.3.convA.conv.weight - torch.Size([192, 576, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.3.convA.conv.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.3.convB.conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.3.convB.conv.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.4.convA.conv.weight - torch.Size([64, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.4.convA.conv.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.4.convB.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.4.convB.conv.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.level_embed - torch.Size([4, 512]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.lateral_convs.0.conv.weight - torch.Size([64, 64, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.lateral_convs.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.1.conv.weight - torch.Size([192, 192, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.lateral_convs.1.bn.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.1.bn.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.2.conv.weight - torch.Size([384, 384, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.lateral_convs.2.bn.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.2.bn.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.3.conv.weight - torch.Size([768, 768, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.lateral_convs.3.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.3.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.4.conv.weight - torch.Size([1536, 1536, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.lateral_convs.4.bn.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.4.bn.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_proj.0.conv.weight - torch.Size([512, 192, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.trans_proj.0.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_proj.0.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_proj.1.conv.weight - torch.Size([512, 384, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.trans_proj.1.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_proj.1.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_proj.2.conv.weight - torch.Size([512, 768, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.trans_proj.2.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_proj.2.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_proj.3.conv.weight - torch.Size([512, 1536, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.trans_proj.3.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_proj.3.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_fusion.0.conv.weight - torch.Size([192, 704, 3, 3]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.trans_fusion.0.bn.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_fusion.0.bn.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_fusion.1.conv.weight - torch.Size([384, 896, 3, 3]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.trans_fusion.1.bn.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_fusion.1.bn.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_fusion.2.conv.weight - torch.Size([768, 1280, 3, 3]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.trans_fusion.2.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_fusion.2.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_fusion.3.conv.weight - torch.Size([1536, 2048, 3, 3]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.trans_fusion.3.bn.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_fusion.3.bn.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.conv_proj.0.conv.weight - torch.Size([512, 64, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.conv_proj.0.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.conv_proj.0.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.conv_fusion.0.conv.weight - torch.Size([64, 576, 3, 3]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.conv_fusion.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.conv_fusion.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.reference_points.weight - torch.Size([2, 512]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.multi_att.sampling_offsets.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.multi_att.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.multi_att.attention_weights.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.multi_att.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.multi_att.value_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.multi_att.value_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.multi_att.output_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.multi_att.output_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.self_attn.sampling_offsets.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.self_attn.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.self_attn.attention_weights.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.self_attn.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.self_attn.value_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.self_attn.value_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.self_attn.output_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.self_attn.output_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  
2022-03-30 17:05:50,153 - depth - INFO - DepthEncoderDecoder(
  (backbone): DepthFormerSwin(
    (patch_embed): PatchEmbed(
      (projection): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    )
    (drop_after_pos): Dropout(p=0.0, inplace=False)
    (stages): ModuleList(
      (0): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=192, out_features=768, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=768, out_features=192, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=192, out_features=768, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=768, out_features=192, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (downsample): PatchMerging(
          (sampler): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=768, out_features=384, bias=False)
        )
      )
      (1): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=384, out_features=1536, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=1536, out_features=384, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=384, out_features=1536, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=1536, out_features=384, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (downsample): PatchMerging(
          (sampler): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
        )
      )
      (2): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (3): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (4): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (5): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (6): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (7): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (8): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (9): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (10): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (11): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (12): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (13): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (14): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (15): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (16): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (17): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (downsample): PatchMerging(
          (sampler): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)
          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=3072, out_features=1536, bias=False)
        )
      )
      (3): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=1536, out_features=4608, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1536, out_features=1536, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1536, out_features=6144, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=6144, out_features=1536, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=1536, out_features=4608, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1536, out_features=1536, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1536, out_features=6144, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=6144, out_features=1536, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
      )
    )
    (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (_conv_stem_relu): ReLU(inplace=True)
    (_conv_stem_maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (decode_head): DenseDepthHead(
    align_corners=True
    (loss_decode): SigLoss()
    (conv_depth): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu): ReLU()
    (sigmoid): Sigmoid()
    (conv_list): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): UpSample(
        (convA): ConvModule(
          (conv): Conv2d(2304, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (convB): ConvModule(
          (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (2): UpSample(
        (convA): ConvModule(
          (conv): Conv2d(1152, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (convB): ConvModule(
          (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (3): UpSample(
        (convA): ConvModule(
          (conv): Conv2d(576, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (convB): ConvModule(
          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (4): UpSample(
        (convA): ConvModule(
          (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (convB): ConvModule(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
  )
  (neck): HAHIHeteroNeck(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (3): ConvModule(
        (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (4): ConvModule(
        (conv): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (trans_proj): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (3): ConvModule(
        (conv): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (trans_fusion): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(704, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(896, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(1280, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (3): ConvModule(
        (conv): Conv2d(2048, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (conv_proj): Sequential(
      (0): ConvModule(
        (conv): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (conv_fusion): Sequential(
      (0): ConvModule(
        (conv): Conv2d(576, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (trans_positional_encoding): SinePositionalEncoding(num_feats=256, temperature=10000, normalize=False, scale=6.283185307179586, eps=1e-06)
    (conv_positional_encoding): SinePositionalEncoding(num_feats=256, temperature=10000, normalize=False, scale=6.283185307179586, eps=1e-06)
    (reference_points): Linear(in_features=512, out_features=2, bias=True)
    (multi_att): MultiScaleDeformableAttention(
      (dropout): Dropout(p=0.1, inplace=False)
      (sampling_offsets): Linear(in_features=512, out_features=512, bias=True)
      (attention_weights): Linear(in_features=512, out_features=256, bias=True)
      (value_proj): Linear(in_features=512, out_features=512, bias=True)
      (output_proj): Linear(in_features=512, out_features=512, bias=True)
    )
    (self_attn): MultiScaleDeformableAttention(
      (dropout): Dropout(p=0.1, inplace=False)
      (sampling_offsets): Linear(in_features=512, out_features=512, bias=True)
      (attention_weights): Linear(in_features=512, out_features=256, bias=True)
      (value_proj): Linear(in_features=512, out_features=512, bias=True)
      (output_proj): Linear(in_features=512, out_features=512, bias=True)
    )
  )
)
2022-03-30 17:05:50,284 - depth - INFO - Loaded 23158 images. Totally 0 invalid pairs are filtered
2022-03-30 17:05:55,452 - depth - INFO - Loaded 652 images. Totally 45 invalid pairs are filtered
2022-03-30 17:05:55,453 - depth - INFO - Start running, host: lizhenyu2@lzy-dev, work_dir: /nfs/lizhenyu2/codes/Monocular-Depth-Estimation-Toolbox/nfs/saves/depthformer/depthformer_swinl_22k_kitti
2022-03-30 17:05:55,453 - depth - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2022-03-30 17:05:55,454 - depth - INFO - workflow: [('train', 1)], max: 38400 iters
2022-03-30 17:06:42,729 - depth - INFO - Iter [50/38400]	lr: 4.824e-07, eta: 8:18:21, time: 0.780, data_time: 0.011, memory: 25023, decode.loss_depth: 0.6012, loss: 0.6012, grad_norm: 8.7810
2022-03-30 17:07:17,437 - depth - INFO - Iter [100/38400]	lr: 8.726e-07, eta: 7:50:24, time: 0.694, data_time: 0.006, memory: 25023, decode.loss_depth: 0.4429, loss: 0.4429, grad_norm: 4.1764
2022-03-30 17:07:52,107 - depth - INFO - Iter [150/38400]	lr: 1.263e-06, eta: 7:40:34, time: 0.694, data_time: 0.005, memory: 25023, decode.loss_depth: 0.3364, loss: 0.3364, grad_norm: 3.9348
2022-03-30 17:08:26,289 - depth - INFO - Iter [200/38400]	lr: 1.653e-06, eta: 7:33:44, time: 0.683, data_time: 0.004, memory: 25023, decode.loss_depth: 0.2672, loss: 0.2672, grad_norm: 3.9822
2022-03-30 17:09:00,835 - depth - INFO - Iter [250/38400]	lr: 2.043e-06, eta: 7:30:20, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.2384, loss: 0.2384, grad_norm: 3.2473
2022-03-30 17:09:35,156 - depth - INFO - Iter [300/38400]	lr: 2.433e-06, eta: 7:27:27, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.2214, loss: 0.2214, grad_norm: 3.3966
2022-03-30 17:10:10,011 - depth - INFO - Iter [350/38400]	lr: 2.823e-06, eta: 7:26:11, time: 0.697, data_time: 0.013, memory: 25023, decode.loss_depth: 0.2072, loss: 0.2072, grad_norm: 3.2018
2022-03-30 17:10:44,497 - depth - INFO - Iter [400/38400]	lr: 3.213e-06, eta: 7:24:30, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1962, loss: 0.1962, grad_norm: 2.8804
2022-03-30 17:11:18,767 - depth - INFO - Iter [450/38400]	lr: 3.603e-06, eta: 7:22:45, time: 0.685, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1884, loss: 0.1884, grad_norm: 2.7749
2022-03-30 17:11:53,046 - depth - INFO - Iter [500/38400]	lr: 3.993e-06, eta: 7:21:16, time: 0.686, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1833, loss: 0.1833, grad_norm: 2.5691
2022-03-30 17:12:27,511 - depth - INFO - Iter [550/38400]	lr: 4.383e-06, eta: 7:20:09, time: 0.689, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1747, loss: 0.1747, grad_norm: 2.7267
2022-03-30 17:13:01,877 - depth - INFO - Iter [600/38400]	lr: 4.772e-06, eta: 7:19:01, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1722, loss: 0.1722, grad_norm: 2.2622
2022-03-30 17:13:36,292 - depth - INFO - Iter [650/38400]	lr: 5.162e-06, eta: 7:18:02, time: 0.688, data_time: 0.006, memory: 25023, decode.loss_depth: 0.1637, loss: 0.1637, grad_norm: 2.5774
2022-03-30 17:14:10,681 - depth - INFO - Iter [700/38400]	lr: 5.551e-06, eta: 7:17:04, time: 0.688, data_time: 0.006, memory: 25023, decode.loss_depth: 0.1613, loss: 0.1613, grad_norm: 2.1892
2022-03-30 17:14:45,058 - depth - INFO - Iter [750/38400]	lr: 5.940e-06, eta: 7:16:09, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1594, loss: 0.1594, grad_norm: 2.1447
2022-03-30 17:15:19,406 - depth - INFO - Iter [800/38400]	lr: 6.329e-06, eta: 7:15:16, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1547, loss: 0.1547, grad_norm: 2.2785
2022-03-30 17:16:01,863 - depth - INFO - Summary:
2022-03-30 17:16:01,864 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
| 0.9041 | 0.9845 | 0.997 |  0.097  | 3.1598 | 0.0411 |  0.1366  | 12.7434 | 0.3773 |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
2022-03-30 17:16:14,782 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_800.pth.
2022-03-30 17:16:14,783 - depth - INFO - Best abs_rel is 0.0970 at 800 iter.
2022-03-30 17:16:14,783 - depth - INFO - Iter(val) [82]	a1: 0.9041, a2: 0.9845, a3: 0.9970, abs_rel: 0.09696344286203384, rmse: 3.1597652435302734, log_10: 0.0411381870508194, rmse_log: 0.136619433760643, silog: 12.7434, sq_rel: 0.37727677822113037
2022-03-30 17:16:49,129 - depth - INFO - Iter [850/38400]	lr: 6.718e-06, eta: 7:55:10, time: 1.794, data_time: 1.112, memory: 25023, decode.loss_depth: 0.1513, loss: 0.1513, grad_norm: 2.2421
2022-03-30 17:17:23,480 - depth - INFO - Iter [900/38400]	lr: 7.107e-06, eta: 7:52:02, time: 0.687, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1483, loss: 0.1483, grad_norm: 2.5454
2022-03-30 17:17:57,920 - depth - INFO - Iter [950/38400]	lr: 7.495e-06, eta: 7:49:13, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1468, loss: 0.1468, grad_norm: 2.3302
2022-03-30 17:18:32,240 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 17:18:32,240 - depth - INFO - Iter [1000/38400]	lr: 7.884e-06, eta: 7:46:33, time: 0.686, data_time: 0.006, memory: 25023, decode.loss_depth: 0.1454, loss: 0.1454, grad_norm: 2.5551
2022-03-30 17:19:06,811 - depth - INFO - Iter [1050/38400]	lr: 8.272e-06, eta: 7:44:14, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1413, loss: 0.1413, grad_norm: 2.5072
2022-03-30 17:19:41,212 - depth - INFO - Iter [1100/38400]	lr: 8.660e-06, eta: 7:41:59, time: 0.688, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1369, loss: 0.1369, grad_norm: 2.1952
2022-03-30 17:20:15,848 - depth - INFO - Iter [1150/38400]	lr: 9.048e-06, eta: 7:40:00, time: 0.693, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1339, loss: 0.1339, grad_norm: 2.0787
2022-03-30 17:20:50,401 - depth - INFO - Iter [1200/38400]	lr: 9.435e-06, eta: 7:38:06, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1345, loss: 0.1345, grad_norm: 2.1797
2022-03-30 17:21:25,173 - depth - INFO - Iter [1250/38400]	lr: 9.822e-06, eta: 7:36:24, time: 0.695, data_time: 0.006, memory: 25023, decode.loss_depth: 0.1339, loss: 0.1339, grad_norm: 2.1859
2022-03-30 17:21:59,454 - depth - INFO - Iter [1300/38400]	lr: 1.021e-05, eta: 7:34:34, time: 0.686, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1346, loss: 0.1346, grad_norm: 2.5196
2022-03-30 17:22:34,141 - depth - INFO - Iter [1350/38400]	lr: 1.060e-05, eta: 7:33:00, time: 0.694, data_time: 0.007, memory: 25023, decode.loss_depth: 0.1309, loss: 0.1309, grad_norm: 2.2095
2022-03-30 17:23:08,859 - depth - INFO - Iter [1400/38400]	lr: 1.098e-05, eta: 7:31:31, time: 0.694, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1295, loss: 0.1295, grad_norm: 2.3754
2022-03-30 17:23:52,672 - depth - INFO - Iter [1450/38400]	lr: 1.137e-05, eta: 7:33:58, time: 0.877, data_time: 0.194, memory: 25023, decode.loss_depth: 0.1303, loss: 0.1303, grad_norm: 2.3378
2022-03-30 17:24:27,322 - depth - INFO - Iter [1500/38400]	lr: 1.175e-05, eta: 7:32:27, time: 0.693, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1260, loss: 0.1260, grad_norm: 2.0946
2022-03-30 17:25:01,643 - depth - INFO - Iter [1550/38400]	lr: 1.214e-05, eta: 7:30:52, time: 0.687, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1240, loss: 0.1240, grad_norm: 2.6648
2022-03-30 17:25:36,027 - depth - INFO - Saving checkpoint at 1600 iterations
2022-03-30 17:25:49,096 - depth - INFO - Iter [1600/38400]	lr: 1.253e-05, eta: 7:34:23, time: 0.949, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1242, loss: 0.1242, grad_norm: 2.0034
2022-03-30 17:26:28,769 - depth - INFO - Summary:
2022-03-30 17:26:28,770 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.9323 | 0.9906 | 0.9985 |  0.0836 | 2.8534 | 0.0355 |  0.1176  | 10.7469 | 0.2987 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-03-30 17:26:41,029 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_1600.pth.
2022-03-30 17:26:41,029 - depth - INFO - Best abs_rel is 0.0836 at 1600 iter.
2022-03-30 17:26:41,030 - depth - INFO - Iter(val) [82]	a1: 0.9323, a2: 0.9906, a3: 0.9985, abs_rel: 0.08359050005674362, rmse: 2.8533949851989746, log_10: 0.03550034388899803, rmse_log: 0.11759841442108154, silog: 10.7469, sq_rel: 0.2987452745437622
2022-03-30 17:27:15,832 - depth - INFO - Iter [1650/38400]	lr: 1.291e-05, eta: 7:52:12, time: 1.734, data_time: 1.044, memory: 25023, decode.loss_depth: 0.1201, loss: 0.1201, grad_norm: 1.8315
2022-03-30 17:27:50,300 - depth - INFO - Iter [1700/38400]	lr: 1.330e-05, eta: 7:50:05, time: 0.689, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1213, loss: 0.1213, grad_norm: 2.1677
2022-03-30 17:28:25,012 - depth - INFO - Iter [1750/38400]	lr: 1.368e-05, eta: 7:48:09, time: 0.695, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1169, loss: 0.1169, grad_norm: 1.9686
2022-03-30 17:28:59,445 - depth - INFO - Iter [1800/38400]	lr: 1.406e-05, eta: 7:46:11, time: 0.688, data_time: 0.006, memory: 25023, decode.loss_depth: 0.1176, loss: 0.1176, grad_norm: 1.6954
2022-03-30 17:29:34,252 - depth - INFO - Iter [1850/38400]	lr: 1.445e-05, eta: 7:44:26, time: 0.696, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1177, loss: 0.1177, grad_norm: 2.3288
2022-03-30 17:30:09,095 - depth - INFO - Iter [1900/38400]	lr: 1.483e-05, eta: 7:42:45, time: 0.697, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1144, loss: 0.1144, grad_norm: 1.7695
2022-03-30 17:30:43,789 - depth - INFO - Iter [1950/38400]	lr: 1.521e-05, eta: 7:41:05, time: 0.694, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1160, loss: 0.1160, grad_norm: 1.9402
2022-03-30 17:31:18,369 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 17:31:18,369 - depth - INFO - Iter [2000/38400]	lr: 1.560e-05, eta: 7:39:25, time: 0.691, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1174, loss: 0.1174, grad_norm: 2.0859
2022-03-30 17:31:52,798 - depth - INFO - Iter [2050/38400]	lr: 1.598e-05, eta: 7:37:46, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1151, loss: 0.1151, grad_norm: 1.7357
2022-03-30 17:32:27,398 - depth - INFO - Iter [2100/38400]	lr: 1.636e-05, eta: 7:36:14, time: 0.692, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1106, loss: 0.1106, grad_norm: 1.9772
2022-03-30 17:33:01,911 - depth - INFO - Iter [2150/38400]	lr: 1.674e-05, eta: 7:34:42, time: 0.690, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1119, loss: 0.1119, grad_norm: 2.2316
2022-03-30 17:33:36,466 - depth - INFO - Iter [2200/38400]	lr: 1.712e-05, eta: 7:33:14, time: 0.691, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1122, loss: 0.1122, grad_norm: 1.7884
2022-03-30 17:34:10,845 - depth - INFO - Iter [2250/38400]	lr: 1.750e-05, eta: 7:31:45, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1128, loss: 0.1128, grad_norm: 1.9191
2022-03-30 17:34:45,211 - depth - INFO - Iter [2300/38400]	lr: 1.788e-05, eta: 7:30:19, time: 0.687, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1084, loss: 0.1084, grad_norm: 2.2180
2022-03-30 17:35:19,706 - depth - INFO - Iter [2350/38400]	lr: 1.826e-05, eta: 7:28:56, time: 0.690, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1084, loss: 0.1084, grad_norm: 2.1747
2022-03-30 17:35:54,340 - depth - INFO - Iter [2400/38400]	lr: 1.864e-05, eta: 7:27:38, time: 0.693, data_time: 0.004, memory: 25023, decode.loss_depth: 0.1066, loss: 0.1066, grad_norm: 1.7493
2022-03-30 17:36:35,130 - depth - INFO - Summary:
2022-03-30 17:36:35,131 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.9225 | 0.9918 | 0.9989 |  0.0901 | 2.6774 | 0.0369 |  0.1198  | 10.3345 | 0.2956 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-03-30 17:36:35,131 - depth - INFO - Iter(val) [82]	a1: 0.9225, a2: 0.9918, a3: 0.9989, abs_rel: 0.09013716131448746, rmse: 2.677417039871216, log_10: 0.036907799541950226, rmse_log: 0.11984628438949585, silog: 10.3345, sq_rel: 0.29556477069854736
2022-03-30 17:37:09,696 - depth - INFO - Iter [2450/38400]	lr: 1.902e-05, eta: 7:36:19, time: 1.507, data_time: 0.820, memory: 25023, decode.loss_depth: 0.1057, loss: 0.1057, grad_norm: 1.6174
2022-03-30 17:37:44,374 - depth - INFO - Iter [2500/38400]	lr: 1.940e-05, eta: 7:34:52, time: 0.693, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1052, loss: 0.1052, grad_norm: 1.7093
2022-03-30 17:38:18,979 - depth - INFO - Iter [2550/38400]	lr: 1.978e-05, eta: 7:33:26, time: 0.692, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1043, loss: 0.1043, grad_norm: 1.8443
2022-03-30 17:38:53,556 - depth - INFO - Iter [2600/38400]	lr: 2.015e-05, eta: 7:32:02, time: 0.692, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1057, loss: 0.1057, grad_norm: 1.9213
2022-03-30 17:39:27,997 - depth - INFO - Iter [2650/38400]	lr: 2.053e-05, eta: 7:30:38, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1038, loss: 0.1038, grad_norm: 1.9000
2022-03-30 17:40:02,612 - depth - INFO - Iter [2700/38400]	lr: 2.091e-05, eta: 7:29:17, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1035, loss: 0.1035, grad_norm: 1.6157
2022-03-30 17:40:37,508 - depth - INFO - Iter [2750/38400]	lr: 2.128e-05, eta: 7:28:03, time: 0.699, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1020, loss: 0.1020, grad_norm: 1.6885
2022-03-30 17:41:12,108 - depth - INFO - Iter [2800/38400]	lr: 2.166e-05, eta: 7:26:45, time: 0.692, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1015, loss: 0.1015, grad_norm: 1.6658
2022-03-30 17:41:46,764 - depth - INFO - Iter [2850/38400]	lr: 2.203e-05, eta: 7:25:30, time: 0.693, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1010, loss: 0.1010, grad_norm: 1.6213
2022-03-30 17:42:30,599 - depth - INFO - Iter [2900/38400]	lr: 2.241e-05, eta: 7:26:09, time: 0.877, data_time: 0.180, memory: 25023, decode.loss_depth: 0.1009, loss: 0.1009, grad_norm: 1.7817
2022-03-30 17:43:05,349 - depth - INFO - Iter [2950/38400]	lr: 2.278e-05, eta: 7:24:56, time: 0.695, data_time: 0.005, memory: 25023, decode.loss_depth: 0.1008, loss: 0.1008, grad_norm: 2.0330
2022-03-30 17:43:39,947 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 17:43:39,948 - depth - INFO - Iter [3000/38400]	lr: 2.315e-05, eta: 7:23:42, time: 0.692, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0995, loss: 0.0995, grad_norm: 1.7687
2022-03-30 17:44:14,299 - depth - INFO - Iter [3050/38400]	lr: 2.353e-05, eta: 7:22:27, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0991, loss: 0.0991, grad_norm: 1.6306
2022-03-30 17:44:48,604 - depth - INFO - Iter [3100/38400]	lr: 2.390e-05, eta: 7:21:13, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0999, loss: 0.0999, grad_norm: 2.0064
2022-03-30 17:45:23,199 - depth - INFO - Iter [3150/38400]	lr: 2.427e-05, eta: 7:20:03, time: 0.692, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0984, loss: 0.0984, grad_norm: 1.9468
2022-03-30 17:45:57,559 - depth - INFO - Saving checkpoint at 3200 iterations
2022-03-30 17:46:11,156 - depth - INFO - Iter [3200/38400]	lr: 2.464e-05, eta: 7:21:21, time: 0.959, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0991, loss: 0.0991, grad_norm: 1.6819
2022-03-30 17:46:50,805 - depth - INFO - Summary:
2022-03-30 17:46:50,806 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+--------+--------+
| 0.9415 | 0.9941 | 0.999 |  0.0817 | 2.5398 | 0.0338 |  0.1102  | 9.5481 | 0.2582 |
+--------+--------+-------+---------+--------+--------+----------+--------+--------+
2022-03-30 17:47:05,770 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_3200.pth.
2022-03-30 17:47:05,771 - depth - INFO - Best abs_rel is 0.0817 at 3200 iter.
2022-03-30 17:47:05,771 - depth - INFO - Iter(val) [82]	a1: 0.9415, a2: 0.9941, a3: 0.9990, abs_rel: 0.08165336400270462, rmse: 2.5397610664367676, log_10: 0.03381827473640442, rmse_log: 0.11015467345714569, silog: 9.5481, sq_rel: 0.25815072655677795
2022-03-30 17:47:40,252 - depth - INFO - Iter [3250/38400]	lr: 2.501e-05, eta: 7:30:00, time: 1.781, data_time: 1.096, memory: 25023, decode.loss_depth: 0.0980, loss: 0.0980, grad_norm: 1.8656
2022-03-30 17:48:14,781 - depth - INFO - Iter [3300/38400]	lr: 2.538e-05, eta: 7:28:40, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0940, loss: 0.0940, grad_norm: 1.4433
2022-03-30 17:48:49,207 - depth - INFO - Iter [3350/38400]	lr: 2.575e-05, eta: 7:27:21, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0955, loss: 0.0955, grad_norm: 1.6445
2022-03-30 17:49:23,455 - depth - INFO - Iter [3400/38400]	lr: 2.612e-05, eta: 7:26:01, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0960, loss: 0.0960, grad_norm: 1.6508
2022-03-30 17:49:57,816 - depth - INFO - Iter [3450/38400]	lr: 2.648e-05, eta: 7:24:43, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0955, loss: 0.0955, grad_norm: 1.7388
2022-03-30 17:50:31,985 - depth - INFO - Iter [3500/38400]	lr: 2.685e-05, eta: 7:23:25, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0959, loss: 0.0959, grad_norm: 1.4701
2022-03-30 17:51:06,414 - depth - INFO - Iter [3550/38400]	lr: 2.722e-05, eta: 7:22:11, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0932, loss: 0.0932, grad_norm: 1.6371
2022-03-30 17:51:41,077 - depth - INFO - Iter [3600/38400]	lr: 2.758e-05, eta: 7:21:00, time: 0.693, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0956, loss: 0.0956, grad_norm: 1.5637
2022-03-30 17:52:15,467 - depth - INFO - Iter [3650/38400]	lr: 2.795e-05, eta: 7:19:47, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0945, loss: 0.0945, grad_norm: 1.6120
2022-03-30 17:52:50,016 - depth - INFO - Iter [3700/38400]	lr: 2.831e-05, eta: 7:18:37, time: 0.691, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0914, loss: 0.0914, grad_norm: 1.4246
2022-03-30 17:53:24,794 - depth - INFO - Iter [3750/38400]	lr: 2.867e-05, eta: 7:17:30, time: 0.696, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0910, loss: 0.0910, grad_norm: 1.2459
2022-03-30 17:53:59,304 - depth - INFO - Iter [3800/38400]	lr: 2.904e-05, eta: 7:16:22, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0919, loss: 0.0919, grad_norm: 1.4343
2022-03-30 17:54:33,684 - depth - INFO - Iter [3850/38400]	lr: 2.940e-05, eta: 7:15:13, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0928, loss: 0.0928, grad_norm: 1.6488
2022-03-30 17:55:08,146 - depth - INFO - Iter [3900/38400]	lr: 2.976e-05, eta: 7:14:06, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0909, loss: 0.0909, grad_norm: 1.4333
2022-03-30 17:55:42,429 - depth - INFO - Iter [3950/38400]	lr: 3.012e-05, eta: 7:12:58, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0924, loss: 0.0924, grad_norm: 1.7072
2022-03-30 17:56:16,853 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 17:56:16,853 - depth - INFO - Iter [4000/38400]	lr: 3.048e-05, eta: 7:11:52, time: 0.688, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0907, loss: 0.0907, grad_norm: 1.7826
2022-03-30 17:56:57,297 - depth - INFO - Summary:
2022-03-30 17:56:57,298 - depth - INFO - 
+-------+--------+--------+---------+-------+--------+----------+--------+--------+
|   a1  |   a2   |   a3   | abs_rel |  rmse | log_10 | rmse_log | silog  | sq_rel |
+-------+--------+--------+---------+-------+--------+----------+--------+--------+
| 0.957 | 0.9947 | 0.9989 |  0.0662 | 2.464 | 0.0291 |  0.0982  | 9.0381 | 0.2062 |
+-------+--------+--------+---------+-------+--------+----------+--------+--------+
2022-03-30 17:57:11,254 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_4000.pth.
2022-03-30 17:57:11,254 - depth - INFO - Best abs_rel is 0.0662 at 4000 iter.
2022-03-30 17:57:11,255 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 17:57:11,255 - depth - INFO - Iter(val) [82]	a1: 0.9570, a2: 0.9947, a3: 0.9989, abs_rel: 0.06624646484851837, rmse: 2.463979482650757, log_10: 0.029111159965395927, rmse_log: 0.09823380410671234, silog: 9.0381, sq_rel: 0.20619863271713257
2022-03-30 17:57:45,782 - depth - INFO - Iter [4050/38400]	lr: 3.084e-05, eta: 7:18:29, time: 1.778, data_time: 1.094, memory: 25023, decode.loss_depth: 0.0891, loss: 0.0891, grad_norm: 1.4110
2022-03-30 17:58:20,229 - depth - INFO - Iter [4100/38400]	lr: 3.120e-05, eta: 7:17:19, time: 0.689, data_time: 0.009, memory: 25023, decode.loss_depth: 0.0899, loss: 0.0899, grad_norm: 1.3768
2022-03-30 17:58:54,460 - depth - INFO - Iter [4150/38400]	lr: 3.155e-05, eta: 7:16:07, time: 0.685, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0888, loss: 0.0888, grad_norm: 1.2583
2022-03-30 17:59:28,856 - depth - INFO - Iter [4200/38400]	lr: 3.191e-05, eta: 7:14:58, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0885, loss: 0.0885, grad_norm: 1.1883
2022-03-30 18:00:03,246 - depth - INFO - Iter [4250/38400]	lr: 3.227e-05, eta: 7:13:50, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0870, loss: 0.0870, grad_norm: 1.1784
2022-03-30 18:00:37,656 - depth - INFO - Iter [4300/38400]	lr: 3.262e-05, eta: 7:12:42, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0897, loss: 0.0897, grad_norm: 1.4641
2022-03-30 18:01:21,253 - depth - INFO - Iter [4350/38400]	lr: 3.298e-05, eta: 7:12:47, time: 0.872, data_time: 0.188, memory: 25023, decode.loss_depth: 0.0879, loss: 0.0879, grad_norm: 1.1483
2022-03-30 18:01:55,745 - depth - INFO - Iter [4400/38400]	lr: 3.333e-05, eta: 7:11:41, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0881, loss: 0.0881, grad_norm: 1.3162
2022-03-30 18:02:30,349 - depth - INFO - Iter [4450/38400]	lr: 3.368e-05, eta: 7:10:36, time: 0.692, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0872, loss: 0.0872, grad_norm: 1.2866
2022-03-30 18:03:04,910 - depth - INFO - Iter [4500/38400]	lr: 3.403e-05, eta: 7:09:32, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0862, loss: 0.0862, grad_norm: 1.2948
2022-03-30 18:03:39,601 - depth - INFO - Iter [4550/38400]	lr: 3.438e-05, eta: 7:08:29, time: 0.693, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0858, loss: 0.0858, grad_norm: 1.2107
2022-03-30 18:04:14,321 - depth - INFO - Iter [4600/38400]	lr: 3.473e-05, eta: 7:07:27, time: 0.695, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0858, loss: 0.0858, grad_norm: 1.2764
2022-03-30 18:04:48,713 - depth - INFO - Iter [4650/38400]	lr: 3.508e-05, eta: 7:06:24, time: 0.688, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0873, loss: 0.0873, grad_norm: 1.5421
2022-03-30 18:05:23,068 - depth - INFO - Iter [4700/38400]	lr: 3.543e-05, eta: 7:05:20, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0854, loss: 0.0854, grad_norm: 1.9200
2022-03-30 18:05:57,435 - depth - INFO - Iter [4750/38400]	lr: 3.578e-05, eta: 7:04:18, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0856, loss: 0.0856, grad_norm: 1.3067
2022-03-30 18:06:31,941 - depth - INFO - Saving checkpoint at 4800 iterations
2022-03-30 18:06:46,835 - depth - INFO - Iter [4800/38400]	lr: 3.613e-05, eta: 7:05:01, time: 0.989, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0838, loss: 0.0838, grad_norm: 1.3150
2022-03-30 18:07:26,761 - depth - INFO - Summary:
2022-03-30 18:07:26,762 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+-------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log | silog | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+-------+--------+
| 0.9617 | 0.9952 | 0.999 |  0.0641 | 2.4186 | 0.0272 |  0.0939  | 8.659 | 0.2235 |
+--------+--------+-------+---------+--------+--------+----------+-------+--------+
2022-03-30 18:07:41,172 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_4800.pth.
2022-03-30 18:07:41,172 - depth - INFO - Best abs_rel is 0.0641 at 4800 iter.
2022-03-30 18:07:41,173 - depth - INFO - Iter(val) [82]	a1: 0.9617, a2: 0.9952, a3: 0.9990, abs_rel: 0.06407353281974792, rmse: 2.4186079502105713, log_10: 0.027244945988059044, rmse_log: 0.09389382600784302, silog: 8.6590, sq_rel: 0.22353334724903107
2022-03-30 18:08:15,707 - depth - INFO - Iter [4850/38400]	lr: 3.647e-05, eta: 7:10:15, time: 1.777, data_time: 1.091, memory: 25023, decode.loss_depth: 0.0842, loss: 0.0842, grad_norm: 1.2633
2022-03-30 18:08:50,132 - depth - INFO - Iter [4900/38400]	lr: 3.682e-05, eta: 7:09:09, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0835, loss: 0.0835, grad_norm: 1.2347
2022-03-30 18:09:24,768 - depth - INFO - Iter [4950/38400]	lr: 3.716e-05, eta: 7:08:05, time: 0.692, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0830, loss: 0.0830, grad_norm: 1.1316
2022-03-30 18:09:59,277 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 18:09:59,277 - depth - INFO - Iter [5000/38400]	lr: 3.750e-05, eta: 7:07:00, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0834, loss: 0.0834, grad_norm: 1.1900
2022-03-30 18:10:33,914 - depth - INFO - Iter [5050/38400]	lr: 3.784e-05, eta: 7:05:57, time: 0.692, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0846, loss: 0.0846, grad_norm: 1.6829
2022-03-30 18:11:08,160 - depth - INFO - Iter [5100/38400]	lr: 3.819e-05, eta: 7:04:53, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0833, loss: 0.0833, grad_norm: 1.2995
2022-03-30 18:11:42,830 - depth - INFO - Iter [5150/38400]	lr: 3.853e-05, eta: 7:03:51, time: 0.693, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0809, loss: 0.0809, grad_norm: 1.2422
2022-03-30 18:12:17,214 - depth - INFO - Iter [5200/38400]	lr: 3.886e-05, eta: 7:02:48, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0844, loss: 0.0844, grad_norm: 1.4205
2022-03-30 18:12:51,506 - depth - INFO - Iter [5250/38400]	lr: 3.920e-05, eta: 7:01:45, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0828, loss: 0.0828, grad_norm: 1.1533
2022-03-30 18:13:26,331 - depth - INFO - Iter [5300/38400]	lr: 3.954e-05, eta: 7:00:46, time: 0.696, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0818, loss: 0.0818, grad_norm: 1.3971
2022-03-30 18:14:01,063 - depth - INFO - Iter [5350/38400]	lr: 3.988e-05, eta: 6:59:47, time: 0.695, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0811, loss: 0.0811, grad_norm: 0.9188
2022-03-30 18:14:35,606 - depth - INFO - Iter [5400/38400]	lr: 4.021e-05, eta: 6:58:47, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0820, loss: 0.0820, grad_norm: 1.4258
2022-03-30 18:15:09,890 - depth - INFO - Iter [5450/38400]	lr: 4.054e-05, eta: 6:57:46, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0819, loss: 0.0819, grad_norm: 1.2582
2022-03-30 18:15:44,490 - depth - INFO - Iter [5500/38400]	lr: 4.088e-05, eta: 6:56:48, time: 0.693, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0831, loss: 0.0831, grad_norm: 1.2645
2022-03-30 18:16:18,974 - depth - INFO - Iter [5550/38400]	lr: 4.121e-05, eta: 6:55:49, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0823, loss: 0.0823, grad_norm: 1.3014
2022-03-30 18:16:53,188 - depth - INFO - Iter [5600/38400]	lr: 4.154e-05, eta: 6:54:49, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0811, loss: 0.0811, grad_norm: 1.1550
2022-03-30 18:17:33,325 - depth - INFO - Summary:
2022-03-30 18:17:33,326 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9659 | 0.9962 | 0.9992 |  0.0605 | 2.2876 | 0.0261 |   0.09   | 8.3435 | 0.1818 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 18:17:47,329 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_5600.pth.
2022-03-30 18:17:47,329 - depth - INFO - Best abs_rel is 0.0605 at 5600 iter.
2022-03-30 18:17:47,330 - depth - INFO - Iter(val) [82]	a1: 0.9659, a2: 0.9962, a3: 0.9992, abs_rel: 0.06053147092461586, rmse: 2.287606716156006, log_10: 0.026086360216140747, rmse_log: 0.08995003998279572, silog: 8.3435, sq_rel: 0.18175749480724335
2022-03-30 18:18:21,966 - depth - INFO - Iter [5650/38400]	lr: 4.187e-05, eta: 6:59:05, time: 1.776, data_time: 1.088, memory: 25023, decode.loss_depth: 0.0806, loss: 0.0806, grad_norm: 1.0084
2022-03-30 18:18:56,526 - depth - INFO - Iter [5700/38400]	lr: 4.220e-05, eta: 6:58:05, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0802, loss: 0.0802, grad_norm: 1.0932
2022-03-30 18:19:30,903 - depth - INFO - Iter [5750/38400]	lr: 4.253e-05, eta: 6:57:04, time: 0.687, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0797, loss: 0.0797, grad_norm: 1.0590
2022-03-30 18:20:14,569 - depth - INFO - Iter [5800/38400]	lr: 4.285e-05, eta: 6:56:56, time: 0.873, data_time: 0.177, memory: 25023, decode.loss_depth: 0.0794, loss: 0.0794, grad_norm: 1.3316
2022-03-30 18:20:49,026 - depth - INFO - Iter [5850/38400]	lr: 4.318e-05, eta: 6:55:56, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0794, loss: 0.0794, grad_norm: 1.0543
2022-03-30 18:21:23,779 - depth - INFO - Iter [5900/38400]	lr: 4.350e-05, eta: 6:54:58, time: 0.695, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0777, loss: 0.0777, grad_norm: 0.8905
2022-03-30 18:21:58,039 - depth - INFO - Iter [5950/38400]	lr: 4.383e-05, eta: 6:53:57, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0789, loss: 0.0789, grad_norm: 1.3265
2022-03-30 18:22:32,473 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 18:22:32,473 - depth - INFO - Iter [6000/38400]	lr: 4.415e-05, eta: 6:52:58, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0790, loss: 0.0790, grad_norm: 1.3838
2022-03-30 18:23:06,734 - depth - INFO - Iter [6050/38400]	lr: 4.447e-05, eta: 6:51:59, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0773, loss: 0.0773, grad_norm: 1.1498
2022-03-30 18:23:41,523 - depth - INFO - Iter [6100/38400]	lr: 4.479e-05, eta: 6:51:03, time: 0.696, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0779, loss: 0.0779, grad_norm: 1.0501
2022-03-30 18:24:15,959 - depth - INFO - Iter [6150/38400]	lr: 4.511e-05, eta: 6:50:05, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0791, loss: 0.0791, grad_norm: 1.0535
2022-03-30 18:24:50,267 - depth - INFO - Iter [6200/38400]	lr: 4.543e-05, eta: 6:49:07, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0778, loss: 0.0778, grad_norm: 1.1052
2022-03-30 18:25:24,826 - depth - INFO - Iter [6250/38400]	lr: 4.575e-05, eta: 6:48:10, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0768, loss: 0.0768, grad_norm: 1.0971
2022-03-30 18:25:59,405 - depth - INFO - Iter [6300/38400]	lr: 4.606e-05, eta: 6:47:14, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0765, loss: 0.0765, grad_norm: 0.9972
2022-03-30 18:26:33,877 - depth - INFO - Iter [6350/38400]	lr: 4.638e-05, eta: 6:46:18, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0765, loss: 0.0765, grad_norm: 1.0048
2022-03-30 18:27:08,325 - depth - INFO - Saving checkpoint at 6400 iterations
2022-03-30 18:27:22,886 - depth - INFO - Iter [6400/38400]	lr: 4.669e-05, eta: 6:46:35, time: 0.981, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0776, loss: 0.0776, grad_norm: 1.1201
2022-03-30 18:28:02,605 - depth - INFO - Summary:
2022-03-30 18:28:02,606 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9628 | 0.9964 | 0.9993 |  0.071  | 2.2822 | 0.0299 |  0.0952  | 8.1883 | 0.2023 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 18:28:02,607 - depth - INFO - Iter(val) [82]	a1: 0.9628, a2: 0.9964, a3: 0.9993, abs_rel: 0.07099869102239609, rmse: 2.282172918319702, log_10: 0.02985321544110775, rmse_log: 0.09517304599285126, silog: 8.1883, sq_rel: 0.20234806835651398
2022-03-30 18:28:37,121 - depth - INFO - Iter [6450/38400]	lr: 4.700e-05, eta: 6:48:56, time: 1.484, data_time: 0.799, memory: 25023, decode.loss_depth: 0.0759, loss: 0.0759, grad_norm: 0.9540
2022-03-30 18:29:11,452 - depth - INFO - Iter [6500/38400]	lr: 4.731e-05, eta: 6:47:57, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0757, loss: 0.0757, grad_norm: 1.2044
2022-03-30 18:29:45,691 - depth - INFO - Iter [6550/38400]	lr: 4.762e-05, eta: 6:46:59, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0754, loss: 0.0754, grad_norm: 1.0102
2022-03-30 18:30:20,133 - depth - INFO - Iter [6600/38400]	lr: 4.793e-05, eta: 6:46:02, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0767, loss: 0.0767, grad_norm: 1.0220
2022-03-30 18:30:54,751 - depth - INFO - Iter [6650/38400]	lr: 4.824e-05, eta: 6:45:06, time: 0.692, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0768, loss: 0.0768, grad_norm: 1.0521
2022-03-30 18:31:29,293 - depth - INFO - Iter [6700/38400]	lr: 4.855e-05, eta: 6:44:10, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0754, loss: 0.0754, grad_norm: 1.1063
2022-03-30 18:32:03,766 - depth - INFO - Iter [6750/38400]	lr: 4.885e-05, eta: 6:43:14, time: 0.689, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0766, loss: 0.0766, grad_norm: 1.0226
2022-03-30 18:32:38,225 - depth - INFO - Iter [6800/38400]	lr: 4.916e-05, eta: 6:42:18, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0769, loss: 0.0769, grad_norm: 0.9402
2022-03-30 18:33:12,416 - depth - INFO - Iter [6850/38400]	lr: 4.946e-05, eta: 6:41:22, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0741, loss: 0.0741, grad_norm: 0.9508
2022-03-30 18:33:46,698 - depth - INFO - Iter [6900/38400]	lr: 4.976e-05, eta: 6:40:26, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0769, loss: 0.0769, grad_norm: 1.1489
2022-03-30 18:34:21,017 - depth - INFO - Iter [6950/38400]	lr: 5.006e-05, eta: 6:39:30, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0755, loss: 0.0755, grad_norm: 1.0461
2022-03-30 18:34:55,364 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 18:34:55,364 - depth - INFO - Iter [7000/38400]	lr: 5.036e-05, eta: 6:38:35, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0759, loss: 0.0759, grad_norm: 1.2661
2022-03-30 18:35:29,788 - depth - INFO - Iter [7050/38400]	lr: 5.066e-05, eta: 6:37:41, time: 0.689, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0754, loss: 0.0754, grad_norm: 0.9924
2022-03-30 18:36:04,229 - depth - INFO - Iter [7100/38400]	lr: 5.095e-05, eta: 6:36:47, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0744, loss: 0.0744, grad_norm: 1.0615
2022-03-30 18:36:38,710 - depth - INFO - Iter [7150/38400]	lr: 5.125e-05, eta: 6:35:53, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0737, loss: 0.0737, grad_norm: 0.9986
2022-03-30 18:37:12,948 - depth - INFO - Iter [7200/38400]	lr: 5.154e-05, eta: 6:34:59, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0741, loss: 0.0741, grad_norm: 1.1817
2022-03-30 18:37:53,456 - depth - INFO - Summary:
2022-03-30 18:37:53,457 - depth - INFO - 
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
| 0.9601 | 0.9953 | 0.9991 |  0.0624 | 2.486 | 0.0281 |  0.0947  | 8.3783 | 0.1996 |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
2022-03-30 18:37:53,460 - depth - INFO - Iter(val) [82]	a1: 0.9601, a2: 0.9953, a3: 0.9991, abs_rel: 0.06239607185125351, rmse: 2.486004590988159, log_10: 0.02806129679083824, rmse_log: 0.09473311901092529, silog: 8.3783, sq_rel: 0.1996273696422577
2022-03-30 18:38:37,064 - depth - INFO - Iter [7250/38400]	lr: 5.184e-05, eta: 6:37:39, time: 1.682, data_time: 0.988, memory: 25023, decode.loss_depth: 0.0731, loss: 0.0731, grad_norm: 0.9621
2022-03-30 18:39:11,437 - depth - INFO - Iter [7300/38400]	lr: 5.213e-05, eta: 6:36:44, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0735, loss: 0.0735, grad_norm: 0.8957
2022-03-30 18:39:45,735 - depth - INFO - Iter [7350/38400]	lr: 5.242e-05, eta: 6:35:49, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0734, loss: 0.0734, grad_norm: 0.9507
2022-03-30 18:40:20,323 - depth - INFO - Iter [7400/38400]	lr: 5.271e-05, eta: 6:34:56, time: 0.692, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0725, loss: 0.0725, grad_norm: 0.9775
2022-03-30 18:40:54,906 - depth - INFO - Iter [7450/38400]	lr: 5.300e-05, eta: 6:34:02, time: 0.692, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0754, loss: 0.0754, grad_norm: 1.0475
2022-03-30 18:41:29,851 - depth - INFO - Iter [7500/38400]	lr: 5.328e-05, eta: 6:33:11, time: 0.699, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0727, loss: 0.0727, grad_norm: 0.8830
2022-03-30 18:42:04,263 - depth - INFO - Iter [7550/38400]	lr: 5.357e-05, eta: 6:32:17, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0733, loss: 0.0733, grad_norm: 1.0454
2022-03-30 18:42:39,099 - depth - INFO - Iter [7600/38400]	lr: 5.385e-05, eta: 6:31:26, time: 0.697, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0732, loss: 0.0732, grad_norm: 0.8808
2022-03-30 18:43:13,476 - depth - INFO - Iter [7650/38400]	lr: 5.413e-05, eta: 6:30:32, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0729, loss: 0.0729, grad_norm: 0.9947
2022-03-30 18:43:47,963 - depth - INFO - Iter [7700/38400]	lr: 5.441e-05, eta: 6:29:40, time: 0.690, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0717, loss: 0.0717, grad_norm: 0.8887
2022-03-30 18:44:22,369 - depth - INFO - Iter [7750/38400]	lr: 5.469e-05, eta: 6:28:47, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0723, loss: 0.0723, grad_norm: 0.9315
2022-03-30 18:44:57,232 - depth - INFO - Iter [7800/38400]	lr: 5.497e-05, eta: 6:27:57, time: 0.697, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0714, loss: 0.0714, grad_norm: 0.8948
2022-03-30 18:45:31,718 - depth - INFO - Iter [7850/38400]	lr: 5.525e-05, eta: 6:27:05, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0703, loss: 0.0703, grad_norm: 0.8715
2022-03-30 18:46:06,003 - depth - INFO - Iter [7900/38400]	lr: 5.552e-05, eta: 6:26:13, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0730, loss: 0.0730, grad_norm: 1.0291
2022-03-30 18:46:40,307 - depth - INFO - Iter [7950/38400]	lr: 5.580e-05, eta: 6:25:20, time: 0.686, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0702, loss: 0.0702, grad_norm: 0.8521
2022-03-30 18:47:14,768 - depth - INFO - Saving checkpoint at 8000 iterations
2022-03-30 18:47:29,767 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 18:47:29,767 - depth - INFO - Iter [8000/38400]	lr: 5.607e-05, eta: 6:25:26, time: 0.989, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0705, loss: 0.0705, grad_norm: 0.8359
2022-03-30 18:48:09,606 - depth - INFO - Summary:
2022-03-30 18:48:09,607 - depth - INFO - 
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
| 0.9667 | 0.9963 | 0.9993 |  0.0604 | 2.233 | 0.0258 |  0.0881  | 7.9974 | 0.1851 |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
2022-03-30 18:48:23,614 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_8000.pth.
2022-03-30 18:48:23,615 - depth - INFO - Best abs_rel is 0.0604 at 8000 iter.
2022-03-30 18:48:23,615 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 18:48:23,616 - depth - INFO - Iter(val) [82]	a1: 0.9667, a2: 0.9963, a3: 0.9993, abs_rel: 0.060439612716436386, rmse: 2.233022689819336, log_10: 0.025791671127080917, rmse_log: 0.08812106400728226, silog: 7.9974, sq_rel: 0.18510140478610992
2022-03-30 18:48:57,904 - depth - INFO - Iter [8050/38400]	lr: 5.634e-05, eta: 6:27:57, time: 1.762, data_time: 1.082, memory: 25023, decode.loss_depth: 0.0716, loss: 0.0716, grad_norm: 0.9133
2022-03-30 18:49:32,054 - depth - INFO - Iter [8100/38400]	lr: 5.661e-05, eta: 6:27:03, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0718, loss: 0.0718, grad_norm: 0.9044
2022-03-30 18:50:06,337 - depth - INFO - Iter [8150/38400]	lr: 5.688e-05, eta: 6:26:10, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0697, loss: 0.0697, grad_norm: 0.8072
2022-03-30 18:50:40,635 - depth - INFO - Iter [8200/38400]	lr: 5.715e-05, eta: 6:25:17, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0723, loss: 0.0723, grad_norm: 0.9029
2022-03-30 18:51:14,709 - depth - INFO - Iter [8250/38400]	lr: 5.741e-05, eta: 6:24:23, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0700, loss: 0.0700, grad_norm: 0.7525
2022-03-30 18:51:49,013 - depth - INFO - Iter [8300/38400]	lr: 5.768e-05, eta: 6:23:30, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0695, loss: 0.0695, grad_norm: 0.8272
2022-03-30 18:52:23,532 - depth - INFO - Iter [8350/38400]	lr: 5.794e-05, eta: 6:22:39, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0704, loss: 0.0704, grad_norm: 1.0409
2022-03-30 18:52:57,928 - depth - INFO - Iter [8400/38400]	lr: 5.820e-05, eta: 6:21:47, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0703, loss: 0.0703, grad_norm: 0.7359
2022-03-30 18:53:32,151 - depth - INFO - Iter [8450/38400]	lr: 5.846e-05, eta: 6:20:55, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0699, loss: 0.0699, grad_norm: 0.9691
2022-03-30 18:54:06,627 - depth - INFO - Iter [8500/38400]	lr: 5.872e-05, eta: 6:20:04, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0698, loss: 0.0698, grad_norm: 0.9717
2022-03-30 18:54:40,872 - depth - INFO - Iter [8550/38400]	lr: 5.898e-05, eta: 6:19:12, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0694, loss: 0.0694, grad_norm: 1.0191
2022-03-30 18:55:15,134 - depth - INFO - Iter [8600/38400]	lr: 5.923e-05, eta: 6:18:20, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0703, loss: 0.0703, grad_norm: 1.0017
2022-03-30 18:55:49,256 - depth - INFO - Iter [8650/38400]	lr: 5.949e-05, eta: 6:17:29, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0684, loss: 0.0684, grad_norm: 0.8309
2022-03-30 18:56:32,826 - depth - INFO - Iter [8700/38400]	lr: 5.974e-05, eta: 6:17:10, time: 0.872, data_time: 0.176, memory: 25023, decode.loss_depth: 0.0691, loss: 0.0691, grad_norm: 0.7728
2022-03-30 18:57:07,459 - depth - INFO - Iter [8750/38400]	lr: 5.999e-05, eta: 6:16:20, time: 0.693, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0692, loss: 0.0692, grad_norm: 0.8712
2022-03-30 18:57:41,922 - depth - INFO - Iter [8800/38400]	lr: 6.024e-05, eta: 6:15:29, time: 0.689, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0693, loss: 0.0693, grad_norm: 0.9028
2022-03-30 18:58:22,413 - depth - INFO - Summary:
2022-03-30 18:58:22,414 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9666 | 0.9969 | 0.9994 |  0.0639 | 2.2214 | 0.027  |  0.0893  | 7.9304 | 0.1811 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 18:58:22,415 - depth - INFO - Iter(val) [82]	a1: 0.9666, a2: 0.9969, a3: 0.9994, abs_rel: 0.06391540169715881, rmse: 2.221418619155884, log_10: 0.027025023475289345, rmse_log: 0.08934804797172546, silog: 7.9304, sq_rel: 0.18110015988349915
2022-03-30 18:58:56,821 - depth - INFO - Iter [8850/38400]	lr: 6.049e-05, eta: 6:16:54, time: 1.498, data_time: 0.815, memory: 25023, decode.loss_depth: 0.0698, loss: 0.0698, grad_norm: 0.9728
2022-03-30 18:59:31,629 - depth - INFO - Iter [8900/38400]	lr: 6.074e-05, eta: 6:16:05, time: 0.696, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0687, loss: 0.0687, grad_norm: 0.9918
2022-03-30 19:00:06,260 - depth - INFO - Iter [8950/38400]	lr: 6.098e-05, eta: 6:15:15, time: 0.693, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0676, loss: 0.0676, grad_norm: 0.7924
2022-03-30 19:00:40,883 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 19:00:40,883 - depth - INFO - Iter [9000/38400]	lr: 6.123e-05, eta: 6:14:25, time: 0.692, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0703, loss: 0.0703, grad_norm: 0.9865
2022-03-30 19:01:15,435 - depth - INFO - Iter [9050/38400]	lr: 6.147e-05, eta: 6:13:34, time: 0.691, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0682, loss: 0.0682, grad_norm: 0.6591
2022-03-30 19:01:49,955 - depth - INFO - Iter [9100/38400]	lr: 6.171e-05, eta: 6:12:44, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0673, loss: 0.0673, grad_norm: 0.6212
2022-03-30 19:02:24,261 - depth - INFO - Iter [9150/38400]	lr: 6.195e-05, eta: 6:11:54, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0684, loss: 0.0684, grad_norm: 0.7839
2022-03-30 19:02:58,442 - depth - INFO - Iter [9200/38400]	lr: 6.219e-05, eta: 6:11:03, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0683, loss: 0.0683, grad_norm: 1.0126
2022-03-30 19:03:32,785 - depth - INFO - Iter [9250/38400]	lr: 6.242e-05, eta: 6:10:13, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0692, loss: 0.0692, grad_norm: 0.8303
2022-03-30 19:04:06,907 - depth - INFO - Iter [9300/38400]	lr: 6.266e-05, eta: 6:09:23, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0675, loss: 0.0675, grad_norm: 0.9181
2022-03-30 19:04:41,282 - depth - INFO - Iter [9350/38400]	lr: 6.289e-05, eta: 6:08:33, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0668, loss: 0.0668, grad_norm: 0.9178
2022-03-30 19:05:15,410 - depth - INFO - Iter [9400/38400]	lr: 6.312e-05, eta: 6:07:43, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0677, loss: 0.0677, grad_norm: 0.8557
2022-03-30 19:05:49,794 - depth - INFO - Iter [9450/38400]	lr: 6.335e-05, eta: 6:06:54, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0678, loss: 0.0678, grad_norm: 0.7585
2022-03-30 19:06:23,935 - depth - INFO - Iter [9500/38400]	lr: 6.358e-05, eta: 6:06:04, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0686, loss: 0.0686, grad_norm: 0.8738
2022-03-30 19:06:58,065 - depth - INFO - Iter [9550/38400]	lr: 6.381e-05, eta: 6:05:14, time: 0.683, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0674, loss: 0.0674, grad_norm: 0.9359
2022-03-30 19:07:32,182 - depth - INFO - Saving checkpoint at 9600 iterations
2022-03-30 19:07:46,741 - depth - INFO - Iter [9600/38400]	lr: 6.403e-05, eta: 6:05:08, time: 0.974, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0659, loss: 0.0659, grad_norm: 0.8509
2022-03-30 19:08:26,005 - depth - INFO - Summary:
2022-03-30 19:08:26,006 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9692 | 0.9964 | 0.9992 |  0.0586 | 2.1827 | 0.0255 |  0.0867  | 8.0001 | 0.1722 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 19:08:39,339 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_9600.pth.
2022-03-30 19:08:39,339 - depth - INFO - Best abs_rel is 0.0586 at 9600 iter.
2022-03-30 19:08:39,340 - depth - INFO - Iter(val) [82]	a1: 0.9692, a2: 0.9964, a3: 0.9992, abs_rel: 0.058602262288331985, rmse: 2.182676076889038, log_10: 0.025533372536301613, rmse_log: 0.08674241602420807, silog: 8.0001, sq_rel: 0.17215891182422638
2022-03-30 19:09:13,607 - depth - INFO - Iter [9650/38400]	lr: 6.426e-05, eta: 6:06:56, time: 1.737, data_time: 1.059, memory: 25023, decode.loss_depth: 0.0669, loss: 0.0669, grad_norm: 0.8267
2022-03-30 19:09:47,895 - depth - INFO - Iter [9700/38400]	lr: 6.448e-05, eta: 6:06:06, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0667, loss: 0.0667, grad_norm: 0.9731
2022-03-30 19:10:22,036 - depth - INFO - Iter [9750/38400]	lr: 6.470e-05, eta: 6:05:15, time: 0.683, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0677, loss: 0.0677, grad_norm: 0.9363
2022-03-30 19:10:56,176 - depth - INFO - Iter [9800/38400]	lr: 6.492e-05, eta: 6:04:25, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0673, loss: 0.0673, grad_norm: 0.8619
2022-03-30 19:11:30,542 - depth - INFO - Iter [9850/38400]	lr: 6.514e-05, eta: 6:03:35, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0655, loss: 0.0655, grad_norm: 0.7712
2022-03-30 19:12:04,851 - depth - INFO - Iter [9900/38400]	lr: 6.535e-05, eta: 6:02:46, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0665, loss: 0.0665, grad_norm: 0.9685
2022-03-30 19:12:38,980 - depth - INFO - Iter [9950/38400]	lr: 6.557e-05, eta: 6:01:56, time: 0.683, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0665, loss: 0.0665, grad_norm: 0.6428
2022-03-30 19:13:13,149 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 19:13:13,149 - depth - INFO - Iter [10000/38400]	lr: 6.578e-05, eta: 6:01:07, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0672, loss: 0.0672, grad_norm: 0.8592
2022-03-30 19:13:47,452 - depth - INFO - Iter [10050/38400]	lr: 6.599e-05, eta: 6:00:18, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0663, loss: 0.0663, grad_norm: 0.7514
2022-03-30 19:14:21,711 - depth - INFO - Iter [10100/38400]	lr: 6.620e-05, eta: 5:59:29, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0660, loss: 0.0660, grad_norm: 0.7606
2022-03-30 19:15:05,137 - depth - INFO - Iter [10150/38400]	lr: 6.641e-05, eta: 5:59:05, time: 0.868, data_time: 0.186, memory: 25023, decode.loss_depth: 0.0652, loss: 0.0652, grad_norm: 0.6622
2022-03-30 19:15:39,633 - depth - INFO - Iter [10200/38400]	lr: 6.661e-05, eta: 5:58:17, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0665, loss: 0.0665, grad_norm: 0.7436
2022-03-30 19:16:14,251 - depth - INFO - Iter [10250/38400]	lr: 6.682e-05, eta: 5:57:30, time: 0.693, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0641, loss: 0.0641, grad_norm: 0.7179
2022-03-30 19:16:48,415 - depth - INFO - Iter [10300/38400]	lr: 6.702e-05, eta: 5:56:41, time: 0.683, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0638, loss: 0.0638, grad_norm: 0.7499
2022-03-30 19:17:22,725 - depth - INFO - Iter [10350/38400]	lr: 6.722e-05, eta: 5:55:52, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0656, loss: 0.0656, grad_norm: 0.7480
2022-03-30 19:17:57,032 - depth - INFO - Iter [10400/38400]	lr: 6.742e-05, eta: 5:55:04, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0642, loss: 0.0642, grad_norm: 0.6680
2022-03-30 19:18:37,359 - depth - INFO - Summary:
2022-03-30 19:18:37,363 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9683 | 0.9964 | 0.9992 |  0.0583 | 2.2014 | 0.0253 |  0.0867  | 7.9631 | 0.1734 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 19:18:51,735 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_10400.pth.
2022-03-30 19:18:51,735 - depth - INFO - Best abs_rel is 0.0583 at 10400 iter.
2022-03-30 19:18:51,736 - depth - INFO - Iter(val) [82]	a1: 0.9683, a2: 0.9964, a3: 0.9992, abs_rel: 0.058252424001693726, rmse: 2.201406240463257, log_10: 0.02534462884068489, rmse_log: 0.08673298358917236, silog: 7.9631, sq_rel: 0.17340591549873352
2022-03-30 19:19:26,031 - depth - INFO - Iter [10450/38400]	lr: 6.762e-05, eta: 5:56:42, time: 1.780, data_time: 1.102, memory: 25023, decode.loss_depth: 0.0645, loss: 0.0645, grad_norm: 0.7719
2022-03-30 19:20:00,397 - depth - INFO - Iter [10500/38400]	lr: 6.782e-05, eta: 5:55:54, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0652, loss: 0.0652, grad_norm: 0.7359
2022-03-30 19:20:34,700 - depth - INFO - Iter [10550/38400]	lr: 6.801e-05, eta: 5:55:05, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0643, loss: 0.0643, grad_norm: 0.9090
2022-03-30 19:21:09,103 - depth - INFO - Iter [10600/38400]	lr: 6.820e-05, eta: 5:54:17, time: 0.688, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0648, loss: 0.0648, grad_norm: 0.8724
2022-03-30 19:21:43,566 - depth - INFO - Iter [10650/38400]	lr: 6.840e-05, eta: 5:53:29, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0648, loss: 0.0648, grad_norm: 0.6689
2022-03-30 19:22:17,620 - depth - INFO - Iter [10700/38400]	lr: 6.859e-05, eta: 5:52:40, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0660, loss: 0.0660, grad_norm: 0.9933
2022-03-30 19:22:51,814 - depth - INFO - Iter [10750/38400]	lr: 6.877e-05, eta: 5:51:51, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0650, loss: 0.0650, grad_norm: 1.0095
2022-03-30 19:23:26,279 - depth - INFO - Iter [10800/38400]	lr: 6.896e-05, eta: 5:51:04, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0636, loss: 0.0636, grad_norm: 0.7577
2022-03-30 19:24:00,363 - depth - INFO - Iter [10850/38400]	lr: 6.914e-05, eta: 5:50:15, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0647, loss: 0.0647, grad_norm: 0.8191
2022-03-30 19:24:34,798 - depth - INFO - Iter [10900/38400]	lr: 6.933e-05, eta: 5:49:27, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0648, loss: 0.0648, grad_norm: 0.8541
2022-03-30 19:25:08,789 - depth - INFO - Iter [10950/38400]	lr: 6.951e-05, eta: 5:48:39, time: 0.680, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0635, loss: 0.0635, grad_norm: 0.8780
2022-03-30 19:25:42,845 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 19:25:42,846 - depth - INFO - Iter [11000/38400]	lr: 6.969e-05, eta: 5:47:51, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0637, loss: 0.0637, grad_norm: 0.7510
2022-03-30 19:26:17,094 - depth - INFO - Iter [11050/38400]	lr: 6.987e-05, eta: 5:47:03, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0648, loss: 0.0648, grad_norm: 0.8149
2022-03-30 19:26:51,430 - depth - INFO - Iter [11100/38400]	lr: 7.004e-05, eta: 5:46:16, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0625, loss: 0.0625, grad_norm: 0.7259
2022-03-30 19:27:25,736 - depth - INFO - Iter [11150/38400]	lr: 7.022e-05, eta: 5:45:29, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0613, loss: 0.0613, grad_norm: 0.7402
2022-03-30 19:28:00,366 - depth - INFO - Saving checkpoint at 11200 iterations
2022-03-30 19:28:14,971 - depth - INFO - Iter [11200/38400]	lr: 7.039e-05, eta: 5:45:18, time: 0.985, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0623, loss: 0.0623, grad_norm: 0.6529
2022-03-30 19:28:54,699 - depth - INFO - Summary:
2022-03-30 19:28:54,700 - depth - INFO - 
+--------+-------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2  |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+-------+--------+---------+--------+--------+----------+--------+--------+
| 0.9706 | 0.997 | 0.9994 |  0.0556 | 2.1831 | 0.024  |  0.083   | 7.6555 | 0.1639 |
+--------+-------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 19:29:08,784 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_11200.pth.
2022-03-30 19:29:08,784 - depth - INFO - Best abs_rel is 0.0556 at 11200 iter.
2022-03-30 19:29:08,785 - depth - INFO - Iter(val) [82]	a1: 0.9706, a2: 0.9970, a3: 0.9994, abs_rel: 0.055564995855093, rmse: 2.1830971240997314, log_10: 0.023985382169485092, rmse_log: 0.08304854482412338, silog: 7.6555, sq_rel: 0.16390974819660187
2022-03-30 19:29:43,197 - depth - INFO - Iter [11250/38400]	lr: 7.056e-05, eta: 5:46:41, time: 1.764, data_time: 1.083, memory: 25023, decode.loss_depth: 0.0637, loss: 0.0637, grad_norm: 0.7484
2022-03-30 19:30:17,686 - depth - INFO - Iter [11300/38400]	lr: 7.073e-05, eta: 5:45:53, time: 0.690, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0619, loss: 0.0619, grad_norm: 0.7371
2022-03-30 19:30:52,169 - depth - INFO - Iter [11350/38400]	lr: 7.090e-05, eta: 5:45:06, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0627, loss: 0.0627, grad_norm: 0.5602
2022-03-30 19:31:26,086 - depth - INFO - Iter [11400/38400]	lr: 7.106e-05, eta: 5:44:17, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0617, loss: 0.0617, grad_norm: 0.6558
2022-03-30 19:32:00,242 - depth - INFO - Iter [11450/38400]	lr: 7.123e-05, eta: 5:43:30, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0615, loss: 0.0615, grad_norm: 0.6656
2022-03-30 19:32:34,554 - depth - INFO - Iter [11500/38400]	lr: 7.139e-05, eta: 5:42:42, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0633, loss: 0.0633, grad_norm: 0.7516
2022-03-30 19:33:08,616 - depth - INFO - Iter [11550/38400]	lr: 7.155e-05, eta: 5:41:54, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0633, loss: 0.0633, grad_norm: 0.5455
2022-03-30 19:33:51,819 - depth - INFO - Iter [11600/38400]	lr: 7.171e-05, eta: 5:41:28, time: 0.864, data_time: 0.186, memory: 25023, decode.loss_depth: 0.0617, loss: 0.0617, grad_norm: 0.8483
2022-03-30 19:34:26,452 - depth - INFO - Iter [11650/38400]	lr: 7.187e-05, eta: 5:40:41, time: 0.693, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0624, loss: 0.0624, grad_norm: 0.6783
2022-03-30 19:35:00,294 - depth - INFO - Iter [11700/38400]	lr: 7.202e-05, eta: 5:39:53, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0605, loss: 0.0605, grad_norm: 0.6969
2022-03-30 19:35:34,414 - depth - INFO - Iter [11750/38400]	lr: 7.218e-05, eta: 5:39:05, time: 0.682, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0602, loss: 0.0602, grad_norm: 0.5932
2022-03-30 19:36:08,891 - depth - INFO - Iter [11800/38400]	lr: 7.233e-05, eta: 5:38:19, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0624, loss: 0.0624, grad_norm: 0.6936
2022-03-30 19:36:43,294 - depth - INFO - Iter [11850/38400]	lr: 7.248e-05, eta: 5:37:32, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0628, loss: 0.0628, grad_norm: 0.6919
2022-03-30 19:37:17,587 - depth - INFO - Iter [11900/38400]	lr: 7.263e-05, eta: 5:36:46, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0625, loss: 0.0625, grad_norm: 0.6712
2022-03-30 19:37:51,879 - depth - INFO - Iter [11950/38400]	lr: 7.277e-05, eta: 5:35:59, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0632, loss: 0.0632, grad_norm: 0.7720
2022-03-30 19:38:26,242 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 19:38:26,243 - depth - INFO - Iter [12000/38400]	lr: 7.292e-05, eta: 5:35:13, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0625, loss: 0.0625, grad_norm: 0.7597
2022-03-30 19:39:06,518 - depth - INFO - Summary:
2022-03-30 19:39:06,519 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9689 | 0.9966 | 0.9992 |  0.0564 | 2.2059 | 0.0247 |  0.0852  | 7.8173 | 0.1694 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 19:39:06,520 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 19:39:06,520 - depth - INFO - Iter(val) [82]	a1: 0.9689, a2: 0.9966, a3: 0.9992, abs_rel: 0.05643587186932564, rmse: 2.205914258956909, log_10: 0.02473420836031437, rmse_log: 0.08518838882446289, silog: 7.8173, sq_rel: 0.16944335401058197
2022-03-30 19:39:40,597 - depth - INFO - Iter [12050/38400]	lr: 7.306e-05, eta: 5:35:54, time: 1.487, data_time: 0.810, memory: 25023, decode.loss_depth: 0.0617, loss: 0.0617, grad_norm: 0.8471
2022-03-30 19:40:14,552 - depth - INFO - Iter [12100/38400]	lr: 7.320e-05, eta: 5:35:06, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0612, loss: 0.0612, grad_norm: 0.5528
2022-03-30 19:40:48,565 - depth - INFO - Iter [12150/38400]	lr: 7.334e-05, eta: 5:34:19, time: 0.680, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0617, loss: 0.0617, grad_norm: 0.7008
2022-03-30 19:41:22,837 - depth - INFO - Iter [12200/38400]	lr: 7.348e-05, eta: 5:33:32, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0609, loss: 0.0609, grad_norm: 0.7726
2022-03-30 19:41:56,985 - depth - INFO - Iter [12250/38400]	lr: 7.362e-05, eta: 5:32:46, time: 0.683, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0618, loss: 0.0618, grad_norm: 0.6311
2022-03-30 19:42:31,306 - depth - INFO - Iter [12300/38400]	lr: 7.375e-05, eta: 5:31:59, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0604, loss: 0.0604, grad_norm: 0.5116
2022-03-30 19:43:05,346 - depth - INFO - Iter [12350/38400]	lr: 7.388e-05, eta: 5:31:12, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0604, loss: 0.0604, grad_norm: 0.6385
2022-03-30 19:43:39,442 - depth - INFO - Iter [12400/38400]	lr: 7.402e-05, eta: 5:30:26, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0605, loss: 0.0605, grad_norm: 0.5978
2022-03-30 19:44:13,304 - depth - INFO - Iter [12450/38400]	lr: 7.414e-05, eta: 5:29:39, time: 0.677, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0609, loss: 0.0609, grad_norm: 0.8429
2022-03-30 19:44:47,800 - depth - INFO - Iter [12500/38400]	lr: 7.427e-05, eta: 5:28:53, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0616, loss: 0.0616, grad_norm: 1.0375
2022-03-30 19:45:21,653 - depth - INFO - Iter [12550/38400]	lr: 7.440e-05, eta: 5:28:06, time: 0.677, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0601, loss: 0.0601, grad_norm: 0.5992
2022-03-30 19:45:55,947 - depth - INFO - Iter [12600/38400]	lr: 7.452e-05, eta: 5:27:20, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0599, loss: 0.0599, grad_norm: 0.4907
2022-03-30 19:46:29,878 - depth - INFO - Iter [12650/38400]	lr: 7.464e-05, eta: 5:26:34, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0601, loss: 0.0601, grad_norm: 0.6824
2022-03-30 19:47:04,087 - depth - INFO - Iter [12700/38400]	lr: 7.476e-05, eta: 5:25:48, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0599, loss: 0.0599, grad_norm: 0.8111
2022-03-30 19:47:38,258 - depth - INFO - Iter [12750/38400]	lr: 7.488e-05, eta: 5:25:02, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0602, loss: 0.0602, grad_norm: 0.7050
2022-03-30 19:48:12,493 - depth - INFO - Saving checkpoint at 12800 iterations
2022-03-30 19:48:27,175 - depth - INFO - Iter [12800/38400]	lr: 7.500e-05, eta: 5:24:46, time: 0.979, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0595, loss: 0.0595, grad_norm: 0.6239
2022-03-30 19:49:06,664 - depth - INFO - Summary:
2022-03-30 19:49:06,665 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9709 | 0.9968 | 0.9993 |  0.0595 | 2.1969 | 0.0254 |  0.0859  | 7.7665 | 0.1753 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 19:49:06,665 - depth - INFO - Iter(val) [82]	a1: 0.9709, a2: 0.9968, a3: 0.9993, abs_rel: 0.059501081705093384, rmse: 2.1969316005706787, log_10: 0.02536099962890148, rmse_log: 0.08591154217720032, silog: 7.7665, sq_rel: 0.1753094494342804
2022-03-30 19:49:40,808 - depth - INFO - Iter [12850/38400]	lr: 7.483e-05, eta: 5:25:19, time: 1.473, data_time: 0.794, memory: 25023, decode.loss_depth: 0.0593, loss: 0.0593, grad_norm: 0.6094
2022-03-30 19:50:14,911 - depth - INFO - Iter [12900/38400]	lr: 7.465e-05, eta: 5:24:32, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0624, loss: 0.0624, grad_norm: 0.6709
2022-03-30 19:50:49,043 - depth - INFO - Iter [12950/38400]	lr: 7.447e-05, eta: 5:23:46, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0595, loss: 0.0595, grad_norm: 0.6575
2022-03-30 19:51:23,320 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 19:51:23,320 - depth - INFO - Iter [13000/38400]	lr: 7.429e-05, eta: 5:23:01, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0607, loss: 0.0607, grad_norm: 0.6003
2022-03-30 19:52:06,616 - depth - INFO - Iter [13050/38400]	lr: 7.411e-05, eta: 5:22:32, time: 0.866, data_time: 0.183, memory: 25023, decode.loss_depth: 0.0594, loss: 0.0594, grad_norm: 0.5750
2022-03-30 19:52:41,138 - depth - INFO - Iter [13100/38400]	lr: 7.393e-05, eta: 5:21:47, time: 0.690, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0590, loss: 0.0590, grad_norm: 0.6268
2022-03-30 19:53:15,465 - depth - INFO - Iter [13150/38400]	lr: 7.375e-05, eta: 5:21:02, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0587, loss: 0.0587, grad_norm: 0.6740
2022-03-30 19:53:49,515 - depth - INFO - Iter [13200/38400]	lr: 7.357e-05, eta: 5:20:16, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0579, loss: 0.0579, grad_norm: 0.5177
2022-03-30 19:54:23,880 - depth - INFO - Iter [13250/38400]	lr: 7.339e-05, eta: 5:19:30, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0574, loss: 0.0574, grad_norm: 0.5286
2022-03-30 19:54:58,000 - depth - INFO - Iter [13300/38400]	lr: 7.321e-05, eta: 5:18:45, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0587, loss: 0.0587, grad_norm: 0.5624
2022-03-30 19:55:32,331 - depth - INFO - Iter [13350/38400]	lr: 7.303e-05, eta: 5:18:00, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0577, loss: 0.0577, grad_norm: 0.7207
2022-03-30 19:56:06,422 - depth - INFO - Iter [13400/38400]	lr: 7.285e-05, eta: 5:17:14, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0581, loss: 0.0581, grad_norm: 0.6202
2022-03-30 19:56:40,639 - depth - INFO - Iter [13450/38400]	lr: 7.267e-05, eta: 5:16:29, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0566, loss: 0.0566, grad_norm: 0.5427
2022-03-30 19:57:14,899 - depth - INFO - Iter [13500/38400]	lr: 7.248e-05, eta: 5:15:44, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0578, loss: 0.0578, grad_norm: 0.5119
2022-03-30 19:57:49,391 - depth - INFO - Iter [13550/38400]	lr: 7.230e-05, eta: 5:14:59, time: 0.690, data_time: 0.009, memory: 25023, decode.loss_depth: 0.0575, loss: 0.0575, grad_norm: 0.6187
2022-03-30 19:58:23,549 - depth - INFO - Iter [13600/38400]	lr: 7.212e-05, eta: 5:14:14, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0569, loss: 0.0569, grad_norm: 0.5637
2022-03-30 19:59:04,030 - depth - INFO - Summary:
2022-03-30 19:59:04,031 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9719 | 0.9964 | 0.9993 |  0.0596 | 2.2552 | 0.0264 |  0.0868  | 7.5934 | 0.1702 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 19:59:04,032 - depth - INFO - Iter(val) [82]	a1: 0.9719, a2: 0.9964, a3: 0.9993, abs_rel: 0.059564147144556046, rmse: 2.2552194595336914, log_10: 0.02643764764070511, rmse_log: 0.08675340563058853, silog: 7.5934, sq_rel: 0.1702415943145752
2022-03-30 19:59:38,065 - depth - INFO - Iter [13650/38400]	lr: 7.193e-05, eta: 5:14:42, time: 1.490, data_time: 0.816, memory: 25023, decode.loss_depth: 0.0571, loss: 0.0571, grad_norm: 0.6777
2022-03-30 20:00:12,157 - depth - INFO - Iter [13700/38400]	lr: 7.175e-05, eta: 5:13:57, time: 0.682, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0568, loss: 0.0568, grad_norm: 0.5378
2022-03-30 20:00:46,250 - depth - INFO - Iter [13750/38400]	lr: 7.157e-05, eta: 5:13:12, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0566, loss: 0.0566, grad_norm: 0.5432
2022-03-30 20:01:20,680 - depth - INFO - Iter [13800/38400]	lr: 7.138e-05, eta: 5:12:27, time: 0.688, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0576, loss: 0.0576, grad_norm: 0.5907
2022-03-30 20:01:54,759 - depth - INFO - Iter [13850/38400]	lr: 7.120e-05, eta: 5:11:42, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0571, loss: 0.0571, grad_norm: 0.6316
2022-03-30 20:02:28,979 - depth - INFO - Iter [13900/38400]	lr: 7.101e-05, eta: 5:10:57, time: 0.684, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0567, loss: 0.0567, grad_norm: 0.6401
2022-03-30 20:03:02,903 - depth - INFO - Iter [13950/38400]	lr: 7.083e-05, eta: 5:10:11, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0571, loss: 0.0571, grad_norm: 0.5520
2022-03-30 20:03:37,048 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 20:03:37,048 - depth - INFO - Iter [14000/38400]	lr: 7.064e-05, eta: 5:09:26, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0581, loss: 0.0581, grad_norm: 0.6140
2022-03-30 20:04:10,960 - depth - INFO - Iter [14050/38400]	lr: 7.045e-05, eta: 5:08:41, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0572, loss: 0.0572, grad_norm: 0.7522
2022-03-30 20:04:45,033 - depth - INFO - Iter [14100/38400]	lr: 7.027e-05, eta: 5:07:56, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0548, loss: 0.0548, grad_norm: 0.6001
2022-03-30 20:05:19,163 - depth - INFO - Iter [14150/38400]	lr: 7.008e-05, eta: 5:07:12, time: 0.683, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0560, loss: 0.0560, grad_norm: 0.6239
2022-03-30 20:05:53,379 - depth - INFO - Iter [14200/38400]	lr: 6.989e-05, eta: 5:06:27, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0568, loss: 0.0568, grad_norm: 0.5459
2022-03-30 20:06:27,368 - depth - INFO - Iter [14250/38400]	lr: 6.970e-05, eta: 5:05:42, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0580, loss: 0.0580, grad_norm: 0.5619
2022-03-30 20:07:01,582 - depth - INFO - Iter [14300/38400]	lr: 6.952e-05, eta: 5:04:58, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0565, loss: 0.0565, grad_norm: 0.5774
2022-03-30 20:07:35,689 - depth - INFO - Iter [14350/38400]	lr: 6.933e-05, eta: 5:04:14, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0557, loss: 0.0557, grad_norm: 0.5857
2022-03-30 20:08:09,437 - depth - INFO - Saving checkpoint at 14400 iterations
2022-03-30 20:08:23,840 - depth - INFO - Iter [14400/38400]	lr: 6.914e-05, eta: 5:03:53, time: 0.963, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0556, loss: 0.0556, grad_norm: 0.5670
2022-03-30 20:09:03,510 - depth - INFO - Summary:
2022-03-30 20:09:03,510 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9726 | 0.9966 | 0.9992 |  0.0545 | 2.1345 | 0.0237 |  0.082   | 7.5395 | 0.1612 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 20:09:16,150 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_14400.pth.
2022-03-30 20:09:16,150 - depth - INFO - Best abs_rel is 0.0545 at 14400 iter.
2022-03-30 20:09:16,151 - depth - INFO - Iter(val) [82]	a1: 0.9726, a2: 0.9966, a3: 0.9992, abs_rel: 0.05447620525956154, rmse: 2.134493112564087, log_10: 0.02374495379626751, rmse_log: 0.0819525420665741, silog: 7.5395, sq_rel: 0.16122053563594818
2022-03-30 20:09:50,088 - depth - INFO - Iter [14450/38400]	lr: 6.895e-05, eta: 5:04:35, time: 1.725, data_time: 1.052, memory: 25023, decode.loss_depth: 0.0563, loss: 0.0563, grad_norm: 0.5807
2022-03-30 20:10:33,861 - depth - INFO - Iter [14500/38400]	lr: 6.876e-05, eta: 5:04:06, time: 0.876, data_time: 0.189, memory: 25023, decode.loss_depth: 0.0553, loss: 0.0553, grad_norm: 0.4742
2022-03-30 20:11:08,145 - depth - INFO - Iter [14550/38400]	lr: 6.857e-05, eta: 5:03:21, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0562, loss: 0.0562, grad_norm: 0.5435
2022-03-30 20:11:42,819 - depth - INFO - Iter [14600/38400]	lr: 6.838e-05, eta: 5:02:38, time: 0.694, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0552, loss: 0.0552, grad_norm: 0.6206
2022-03-30 20:12:17,106 - depth - INFO - Iter [14650/38400]	lr: 6.819e-05, eta: 5:01:53, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0542, loss: 0.0542, grad_norm: 0.5003
2022-03-30 20:12:51,116 - depth - INFO - Iter [14700/38400]	lr: 6.800e-05, eta: 5:01:08, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0544, loss: 0.0544, grad_norm: 0.6155
2022-03-30 20:13:25,104 - depth - INFO - Iter [14750/38400]	lr: 6.781e-05, eta: 5:00:24, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0550, loss: 0.0550, grad_norm: 0.5769
2022-03-30 20:13:58,973 - depth - INFO - Iter [14800/38400]	lr: 6.762e-05, eta: 4:59:39, time: 0.677, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0544, loss: 0.0544, grad_norm: 0.4845
2022-03-30 20:14:32,905 - depth - INFO - Iter [14850/38400]	lr: 6.742e-05, eta: 4:58:54, time: 0.679, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0546, loss: 0.0546, grad_norm: 0.5926
2022-03-30 20:15:07,348 - depth - INFO - Iter [14900/38400]	lr: 6.723e-05, eta: 4:58:10, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0551, loss: 0.0551, grad_norm: 0.5186
2022-03-30 20:15:41,481 - depth - INFO - Iter [14950/38400]	lr: 6.704e-05, eta: 4:57:26, time: 0.683, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0560, loss: 0.0560, grad_norm: 0.6379
2022-03-30 20:16:15,516 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 20:16:15,517 - depth - INFO - Iter [15000/38400]	lr: 6.685e-05, eta: 4:56:42, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0544, loss: 0.0544, grad_norm: 0.6411
2022-03-30 20:16:49,502 - depth - INFO - Iter [15050/38400]	lr: 6.666e-05, eta: 4:55:57, time: 0.680, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0553, loss: 0.0553, grad_norm: 0.6794
2022-03-30 20:17:23,705 - depth - INFO - Iter [15100/38400]	lr: 6.646e-05, eta: 4:55:13, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0562, loss: 0.0562, grad_norm: 0.6925
2022-03-30 20:17:57,699 - depth - INFO - Iter [15150/38400]	lr: 6.627e-05, eta: 4:54:29, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0544, loss: 0.0544, grad_norm: 0.7938
2022-03-30 20:18:31,910 - depth - INFO - Iter [15200/38400]	lr: 6.608e-05, eta: 4:53:45, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0535, loss: 0.0535, grad_norm: 0.6135
2022-03-30 20:19:11,928 - depth - INFO - Summary:
2022-03-30 20:19:11,929 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9725 | 0.9966 | 0.9992 |  0.0558 | 2.1632 | 0.024  |  0.083   | 7.5911 | 0.1625 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 20:19:11,930 - depth - INFO - Iter(val) [82]	a1: 0.9725, a2: 0.9966, a3: 0.9992, abs_rel: 0.05582860857248306, rmse: 2.163224935531616, log_10: 0.02403036691248417, rmse_log: 0.08295706659555435, silog: 7.5911, sq_rel: 0.16249309480190277
2022-03-30 20:19:45,939 - depth - INFO - Iter [15250/38400]	lr: 6.588e-05, eta: 4:54:02, time: 1.481, data_time: 0.805, memory: 25023, decode.loss_depth: 0.0563, loss: 0.0563, grad_norm: 0.6774
2022-03-30 20:20:20,035 - depth - INFO - Iter [15300/38400]	lr: 6.569e-05, eta: 4:53:18, time: 0.682, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0539, loss: 0.0539, grad_norm: 0.5192
2022-03-30 20:20:54,189 - depth - INFO - Iter [15350/38400]	lr: 6.549e-05, eta: 4:52:34, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0524, loss: 0.0524, grad_norm: 0.5056
2022-03-30 20:21:28,431 - depth - INFO - Iter [15400/38400]	lr: 6.530e-05, eta: 4:51:50, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0546, loss: 0.0546, grad_norm: 0.5275
2022-03-30 20:22:02,698 - depth - INFO - Iter [15450/38400]	lr: 6.510e-05, eta: 4:51:06, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0543, loss: 0.0543, grad_norm: 0.5020
2022-03-30 20:22:36,706 - depth - INFO - Iter [15500/38400]	lr: 6.491e-05, eta: 4:50:22, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0531, loss: 0.0531, grad_norm: 0.5762
2022-03-30 20:23:10,727 - depth - INFO - Iter [15550/38400]	lr: 6.471e-05, eta: 4:49:38, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0536, loss: 0.0536, grad_norm: 0.5713
2022-03-30 20:23:44,888 - depth - INFO - Iter [15600/38400]	lr: 6.452e-05, eta: 4:48:55, time: 0.683, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0539, loss: 0.0539, grad_norm: 0.5666
2022-03-30 20:24:19,173 - depth - INFO - Iter [15650/38400]	lr: 6.432e-05, eta: 4:48:11, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0541, loss: 0.0541, grad_norm: 0.5824
2022-03-30 20:24:53,034 - depth - INFO - Iter [15700/38400]	lr: 6.413e-05, eta: 4:47:27, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0531, loss: 0.0531, grad_norm: 0.4901
2022-03-30 20:25:27,394 - depth - INFO - Iter [15750/38400]	lr: 6.393e-05, eta: 4:46:44, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0535, loss: 0.0535, grad_norm: 0.5750
2022-03-30 20:26:01,491 - depth - INFO - Iter [15800/38400]	lr: 6.373e-05, eta: 4:46:01, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0537, loss: 0.0537, grad_norm: 0.5279
2022-03-30 20:26:35,381 - depth - INFO - Iter [15850/38400]	lr: 6.354e-05, eta: 4:45:17, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0532, loss: 0.0532, grad_norm: 0.5406
2022-03-30 20:27:09,277 - depth - INFO - Iter [15900/38400]	lr: 6.334e-05, eta: 4:44:33, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0536, loss: 0.0536, grad_norm: 0.4787
2022-03-30 20:27:53,010 - depth - INFO - Iter [15950/38400]	lr: 6.314e-05, eta: 4:44:03, time: 0.875, data_time: 0.171, memory: 25023, decode.loss_depth: 0.0531, loss: 0.0531, grad_norm: 0.6167
2022-03-30 20:28:27,463 - depth - INFO - Saving checkpoint at 16000 iterations
2022-03-30 20:28:42,072 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 20:28:42,072 - depth - INFO - Iter [16000/38400]	lr: 6.294e-05, eta: 4:43:41, time: 0.981, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0527, loss: 0.0527, grad_norm: 0.4682
2022-03-30 20:29:21,538 - depth - INFO - Summary:
2022-03-30 20:29:21,552 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
| 0.9703 | 0.9964 | 0.9992 |  0.0544 | 2.2391 | 0.024  |  0.0838  | 7.672 | 0.1661 |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
2022-03-30 20:29:35,683 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_16000.pth.
2022-03-30 20:29:35,683 - depth - INFO - Best abs_rel is 0.0544 at 16000 iter.
2022-03-30 20:29:35,684 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 20:29:35,684 - depth - INFO - Iter(val) [82]	a1: 0.9703, a2: 0.9964, a3: 0.9992, abs_rel: 0.05442065745592117, rmse: 2.2391345500946045, log_10: 0.023996686562895775, rmse_log: 0.08376279473304749, silog: 7.6720, sq_rel: 0.16613630950450897
2022-03-30 20:30:10,045 - depth - INFO - Iter [16050/38400]	lr: 6.275e-05, eta: 4:44:13, time: 1.759, data_time: 1.079, memory: 25023, decode.loss_depth: 0.0525, loss: 0.0525, grad_norm: 0.5390
2022-03-30 20:30:44,356 - depth - INFO - Iter [16100/38400]	lr: 6.255e-05, eta: 4:43:29, time: 0.686, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0527, loss: 0.0527, grad_norm: 0.6800
2022-03-30 20:31:18,493 - depth - INFO - Iter [16150/38400]	lr: 6.235e-05, eta: 4:42:45, time: 0.682, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0524, loss: 0.0524, grad_norm: 0.5368
2022-03-30 20:31:53,334 - depth - INFO - Iter [16200/38400]	lr: 6.215e-05, eta: 4:42:03, time: 0.697, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0515, loss: 0.0515, grad_norm: 0.6196
2022-03-30 20:32:27,326 - depth - INFO - Iter [16250/38400]	lr: 6.195e-05, eta: 4:41:19, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0517, loss: 0.0517, grad_norm: 0.5886
2022-03-30 20:33:01,618 - depth - INFO - Iter [16300/38400]	lr: 6.176e-05, eta: 4:40:36, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0503, loss: 0.0503, grad_norm: 0.4347
2022-03-30 20:33:35,516 - depth - INFO - Iter [16350/38400]	lr: 6.156e-05, eta: 4:39:52, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0518, loss: 0.0518, grad_norm: 0.4183
2022-03-30 20:34:09,672 - depth - INFO - Iter [16400/38400]	lr: 6.136e-05, eta: 4:39:09, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0514, loss: 0.0514, grad_norm: 0.4943
2022-03-30 20:34:44,001 - depth - INFO - Iter [16450/38400]	lr: 6.116e-05, eta: 4:38:26, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0533, loss: 0.0533, grad_norm: 0.6524
2022-03-30 20:35:18,120 - depth - INFO - Iter [16500/38400]	lr: 6.096e-05, eta: 4:37:42, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0517, loss: 0.0517, grad_norm: 0.5699
2022-03-30 20:35:52,302 - depth - INFO - Iter [16550/38400]	lr: 6.076e-05, eta: 4:36:59, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0516, loss: 0.0516, grad_norm: 0.5692
2022-03-30 20:36:26,326 - depth - INFO - Iter [16600/38400]	lr: 6.056e-05, eta: 4:36:16, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0512, loss: 0.0512, grad_norm: 0.4121
2022-03-30 20:37:00,352 - depth - INFO - Iter [16650/38400]	lr: 6.036e-05, eta: 4:35:33, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0502, loss: 0.0502, grad_norm: 0.5145
2022-03-30 20:37:34,980 - depth - INFO - Iter [16700/38400]	lr: 6.016e-05, eta: 4:34:50, time: 0.693, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0516, loss: 0.0516, grad_norm: 0.6349
2022-03-30 20:38:09,231 - depth - INFO - Iter [16750/38400]	lr: 5.996e-05, eta: 4:34:07, time: 0.685, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0512, loss: 0.0512, grad_norm: 0.4906
2022-03-30 20:38:43,294 - depth - INFO - Iter [16800/38400]	lr: 5.976e-05, eta: 4:33:24, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0500, loss: 0.0500, grad_norm: 0.4597
2022-03-30 20:39:23,350 - depth - INFO - Summary:
2022-03-30 20:39:23,351 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9732 | 0.9969 | 0.9992 |  0.0565 | 2.1393 | 0.0243 |  0.0822  | 7.4967 | 0.1611 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 20:39:23,351 - depth - INFO - Iter(val) [82]	a1: 0.9732, a2: 0.9969, a3: 0.9992, abs_rel: 0.05645515024662018, rmse: 2.1392722129821777, log_10: 0.024343449622392654, rmse_log: 0.0822422206401825, silog: 7.4967, sq_rel: 0.16113626956939697
2022-03-30 20:39:57,650 - depth - INFO - Iter [16850/38400]	lr: 5.956e-05, eta: 4:33:33, time: 1.487, data_time: 0.807, memory: 25023, decode.loss_depth: 0.0511, loss: 0.0511, grad_norm: 0.5320
2022-03-30 20:40:31,778 - depth - INFO - Iter [16900/38400]	lr: 5.936e-05, eta: 4:32:50, time: 0.683, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0506, loss: 0.0506, grad_norm: 0.5429
2022-03-30 20:41:05,866 - depth - INFO - Iter [16950/38400]	lr: 5.916e-05, eta: 4:32:07, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0509, loss: 0.0509, grad_norm: 0.5607
2022-03-30 20:41:39,914 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 20:41:39,914 - depth - INFO - Iter [17000/38400]	lr: 5.895e-05, eta: 4:31:24, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0509, loss: 0.0509, grad_norm: 0.4410
2022-03-30 20:42:14,056 - depth - INFO - Iter [17050/38400]	lr: 5.875e-05, eta: 4:30:41, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0503, loss: 0.0503, grad_norm: 0.4748
2022-03-30 20:42:48,231 - depth - INFO - Iter [17100/38400]	lr: 5.855e-05, eta: 4:29:58, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0510, loss: 0.0510, grad_norm: 0.5298
2022-03-30 20:43:22,351 - depth - INFO - Iter [17150/38400]	lr: 5.835e-05, eta: 4:29:15, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0505, loss: 0.0505, grad_norm: 0.4400
2022-03-30 20:43:56,598 - depth - INFO - Iter [17200/38400]	lr: 5.815e-05, eta: 4:28:32, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0505, loss: 0.0505, grad_norm: 0.4934
2022-03-30 20:44:30,711 - depth - INFO - Iter [17250/38400]	lr: 5.795e-05, eta: 4:27:50, time: 0.683, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0512, loss: 0.0512, grad_norm: 0.5036
2022-03-30 20:45:04,948 - depth - INFO - Iter [17300/38400]	lr: 5.774e-05, eta: 4:27:07, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0507, loss: 0.0507, grad_norm: 0.6843
2022-03-30 20:45:38,876 - depth - INFO - Iter [17350/38400]	lr: 5.754e-05, eta: 4:26:24, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0505, loss: 0.0505, grad_norm: 0.4058
2022-03-30 20:46:22,030 - depth - INFO - Iter [17400/38400]	lr: 5.734e-05, eta: 4:25:52, time: 0.863, data_time: 0.179, memory: 25023, decode.loss_depth: 0.0491, loss: 0.0491, grad_norm: 0.4893
2022-03-30 20:46:56,223 - depth - INFO - Iter [17450/38400]	lr: 5.714e-05, eta: 4:25:10, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0499, loss: 0.0499, grad_norm: 0.4823
2022-03-30 20:47:30,295 - depth - INFO - Iter [17500/38400]	lr: 5.694e-05, eta: 4:24:27, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0493, loss: 0.0493, grad_norm: 0.5950
2022-03-30 20:48:04,746 - depth - INFO - Iter [17550/38400]	lr: 5.673e-05, eta: 4:23:45, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0495, loss: 0.0495, grad_norm: 0.5198
2022-03-30 20:48:38,649 - depth - INFO - Saving checkpoint at 17600 iterations
2022-03-30 20:48:53,740 - depth - INFO - Iter [17600/38400]	lr: 5.653e-05, eta: 4:23:20, time: 0.980, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0493, loss: 0.0493, grad_norm: 0.5106
2022-03-30 20:49:33,727 - depth - INFO - Summary:
2022-03-30 20:49:33,728 - depth - INFO - 
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
| 0.9695 | 0.9964 | 0.9993 |  0.0552 | 2.303 | 0.0244 |  0.0846  | 7.6496 | 0.1732 |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
2022-03-30 20:49:33,728 - depth - INFO - Iter(val) [82]	a1: 0.9695, a2: 0.9964, a3: 0.9993, abs_rel: 0.055191297084093094, rmse: 2.3029696941375732, log_10: 0.024386350065469742, rmse_log: 0.08458362519741058, silog: 7.6496, sq_rel: 0.17318373918533325
2022-03-30 20:50:07,736 - depth - INFO - Iter [17650/38400]	lr: 5.633e-05, eta: 4:23:25, time: 1.480, data_time: 0.804, memory: 25023, decode.loss_depth: 0.0489, loss: 0.0489, grad_norm: 0.4843
2022-03-30 20:50:41,668 - depth - INFO - Iter [17700/38400]	lr: 5.612e-05, eta: 4:22:42, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0492, loss: 0.0492, grad_norm: 0.4713
2022-03-30 20:51:16,113 - depth - INFO - Iter [17750/38400]	lr: 5.592e-05, eta: 4:21:59, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0483, loss: 0.0483, grad_norm: 0.4591
2022-03-30 20:51:50,421 - depth - INFO - Iter [17800/38400]	lr: 5.572e-05, eta: 4:21:17, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0494, loss: 0.0494, grad_norm: 0.4954
2022-03-30 20:52:24,669 - depth - INFO - Iter [17850/38400]	lr: 5.552e-05, eta: 4:20:34, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0493, loss: 0.0493, grad_norm: 0.4647
2022-03-30 20:52:58,888 - depth - INFO - Iter [17900/38400]	lr: 5.531e-05, eta: 4:19:52, time: 0.684, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0501, loss: 0.0501, grad_norm: 0.6338
2022-03-30 20:53:33,246 - depth - INFO - Iter [17950/38400]	lr: 5.511e-05, eta: 4:19:10, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0491, loss: 0.0491, grad_norm: 0.5468
2022-03-30 20:54:07,094 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 20:54:07,094 - depth - INFO - Iter [18000/38400]	lr: 5.490e-05, eta: 4:18:27, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0485, loss: 0.0485, grad_norm: 0.4832
2022-03-30 20:54:41,001 - depth - INFO - Iter [18050/38400]	lr: 5.470e-05, eta: 4:17:44, time: 0.678, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0493, loss: 0.0493, grad_norm: 0.4445
2022-03-30 20:55:15,152 - depth - INFO - Iter [18100/38400]	lr: 5.450e-05, eta: 4:17:02, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0485, loss: 0.0485, grad_norm: 0.4374
2022-03-30 20:55:49,328 - depth - INFO - Iter [18150/38400]	lr: 5.429e-05, eta: 4:16:20, time: 0.683, data_time: 0.009, memory: 25023, decode.loss_depth: 0.0488, loss: 0.0488, grad_norm: 0.5441
2022-03-30 20:56:23,219 - depth - INFO - Iter [18200/38400]	lr: 5.409e-05, eta: 4:15:37, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0496, loss: 0.0496, grad_norm: 0.4585
2022-03-30 20:56:56,982 - depth - INFO - Iter [18250/38400]	lr: 5.389e-05, eta: 4:14:55, time: 0.675, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0488, loss: 0.0488, grad_norm: 0.4348
2022-03-30 20:57:30,970 - depth - INFO - Iter [18300/38400]	lr: 5.368e-05, eta: 4:14:12, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0482, loss: 0.0482, grad_norm: 0.5826
2022-03-30 20:58:05,103 - depth - INFO - Iter [18350/38400]	lr: 5.348e-05, eta: 4:13:30, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0491, loss: 0.0491, grad_norm: 0.5112
2022-03-30 20:58:38,996 - depth - INFO - Iter [18400/38400]	lr: 5.327e-05, eta: 4:12:48, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0491, loss: 0.0491, grad_norm: 0.4910
2022-03-30 20:59:19,006 - depth - INFO - Summary:
2022-03-30 20:59:19,007 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9739 | 0.9969 | 0.9992 |  0.0558 | 2.1397 | 0.024  |  0.0812  | 7.3476 | 0.1619 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 20:59:19,008 - depth - INFO - Iter(val) [82]	a1: 0.9739, a2: 0.9969, a3: 0.9992, abs_rel: 0.05579454451799393, rmse: 2.139711618423462, log_10: 0.023961927741765976, rmse_log: 0.08118151128292084, silog: 7.3476, sq_rel: 0.16191144287586212
2022-03-30 20:59:53,392 - depth - INFO - Iter [18450/38400]	lr: 5.307e-05, eta: 4:12:50, time: 1.488, data_time: 0.806, memory: 25023, decode.loss_depth: 0.0489, loss: 0.0489, grad_norm: 0.5486
2022-03-30 21:00:27,341 - depth - INFO - Iter [18500/38400]	lr: 5.287e-05, eta: 4:12:07, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0483, loss: 0.0483, grad_norm: 0.4289
2022-03-30 21:01:01,324 - depth - INFO - Iter [18550/38400]	lr: 5.266e-05, eta: 4:11:25, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0476, loss: 0.0476, grad_norm: 0.3896
2022-03-30 21:01:35,313 - depth - INFO - Iter [18600/38400]	lr: 5.246e-05, eta: 4:10:43, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0488, loss: 0.0488, grad_norm: 0.4984
2022-03-30 21:02:09,474 - depth - INFO - Iter [18650/38400]	lr: 5.225e-05, eta: 4:10:01, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0484, loss: 0.0484, grad_norm: 0.4033
2022-03-30 21:02:43,418 - depth - INFO - Iter [18700/38400]	lr: 5.205e-05, eta: 4:09:18, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0483, loss: 0.0483, grad_norm: 0.3885
2022-03-30 21:03:17,295 - depth - INFO - Iter [18750/38400]	lr: 5.184e-05, eta: 4:08:36, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0485, loss: 0.0485, grad_norm: 0.3912
2022-03-30 21:03:51,211 - depth - INFO - Iter [18800/38400]	lr: 5.164e-05, eta: 4:07:54, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0477, loss: 0.0477, grad_norm: 0.5875
2022-03-30 21:04:34,466 - depth - INFO - Iter [18850/38400]	lr: 5.144e-05, eta: 4:07:21, time: 0.865, data_time: 0.183, memory: 25023, decode.loss_depth: 0.0476, loss: 0.0476, grad_norm: 0.4560
2022-03-30 21:05:08,341 - depth - INFO - Iter [18900/38400]	lr: 5.123e-05, eta: 4:06:39, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0477, loss: 0.0477, grad_norm: 0.4871
2022-03-30 21:05:42,384 - depth - INFO - Iter [18950/38400]	lr: 5.103e-05, eta: 4:05:57, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0477, loss: 0.0477, grad_norm: 0.4437
2022-03-30 21:06:16,749 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 21:06:16,749 - depth - INFO - Iter [19000/38400]	lr: 5.082e-05, eta: 4:05:16, time: 0.687, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0470, loss: 0.0470, grad_norm: 0.5566
2022-03-30 21:06:50,736 - depth - INFO - Iter [19050/38400]	lr: 5.062e-05, eta: 4:04:34, time: 0.680, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0491, loss: 0.0491, grad_norm: 0.5349
2022-03-30 21:07:24,950 - depth - INFO - Iter [19100/38400]	lr: 5.041e-05, eta: 4:03:52, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0473, loss: 0.0473, grad_norm: 0.4299
2022-03-30 21:07:59,176 - depth - INFO - Iter [19150/38400]	lr: 5.021e-05, eta: 4:03:11, time: 0.685, data_time: 0.008, memory: 25023, decode.loss_depth: 0.0484, loss: 0.0484, grad_norm: 0.5966
2022-03-30 21:08:33,164 - depth - INFO - Saving checkpoint at 19200 iterations
2022-03-30 21:08:47,730 - depth - INFO - Iter [19200/38400]	lr: 5.000e-05, eta: 4:02:43, time: 0.971, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0467, loss: 0.0467, grad_norm: 0.4022
2022-03-30 21:09:27,189 - depth - INFO - Summary:
2022-03-30 21:09:27,190 - depth - INFO - 
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
| 0.9725 | 0.9969 | 0.9993 |  0.0537 | 2.116 | 0.0232 |  0.0806  | 7.3626 | 0.1622 |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
2022-03-30 21:09:40,834 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_19200.pth.
2022-03-30 21:09:40,835 - depth - INFO - Best abs_rel is 0.0537 at 19200 iter.
2022-03-30 21:09:40,835 - depth - INFO - Iter(val) [82]	a1: 0.9725, a2: 0.9969, a3: 0.9993, abs_rel: 0.05369195714592934, rmse: 2.1159510612487793, log_10: 0.02317691221833229, rmse_log: 0.0806296095252037, silog: 7.3626, sq_rel: 0.1622193604707718
2022-03-30 21:10:15,283 - depth - INFO - Iter [19250/38400]	lr: 4.980e-05, eta: 4:02:55, time: 1.751, data_time: 1.068, memory: 25023, decode.loss_depth: 0.0471, loss: 0.0471, grad_norm: 0.4294
2022-03-30 21:10:49,537 - depth - INFO - Iter [19300/38400]	lr: 4.960e-05, eta: 4:02:13, time: 0.685, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0484, loss: 0.0484, grad_norm: 0.6328
2022-03-30 21:11:23,630 - depth - INFO - Iter [19350/38400]	lr: 4.939e-05, eta: 4:01:31, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0467, loss: 0.0467, grad_norm: 0.3573
2022-03-30 21:11:57,634 - depth - INFO - Iter [19400/38400]	lr: 4.919e-05, eta: 4:00:49, time: 0.680, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0470, loss: 0.0470, grad_norm: 0.4484
2022-03-30 21:12:31,837 - depth - INFO - Iter [19450/38400]	lr: 4.898e-05, eta: 4:00:07, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0460, loss: 0.0460, grad_norm: 0.4046
2022-03-30 21:13:06,250 - depth - INFO - Iter [19500/38400]	lr: 4.878e-05, eta: 3:59:26, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0470, loss: 0.0470, grad_norm: 0.4791
2022-03-30 21:13:40,491 - depth - INFO - Iter [19550/38400]	lr: 4.857e-05, eta: 3:58:44, time: 0.685, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0466, loss: 0.0466, grad_norm: 0.5394
2022-03-30 21:14:14,529 - depth - INFO - Iter [19600/38400]	lr: 4.837e-05, eta: 3:58:02, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0463, loss: 0.0463, grad_norm: 0.3989
2022-03-30 21:14:48,807 - depth - INFO - Iter [19650/38400]	lr: 4.816e-05, eta: 3:57:21, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0465, loss: 0.0465, grad_norm: 0.4722
2022-03-30 21:15:23,018 - depth - INFO - Iter [19700/38400]	lr: 4.796e-05, eta: 3:56:39, time: 0.684, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0467, loss: 0.0467, grad_norm: 0.3696
2022-03-30 21:15:57,142 - depth - INFO - Iter [19750/38400]	lr: 4.776e-05, eta: 3:55:58, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0461, loss: 0.0461, grad_norm: 0.4216
2022-03-30 21:16:31,095 - depth - INFO - Iter [19800/38400]	lr: 4.755e-05, eta: 3:55:16, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0468, loss: 0.0468, grad_norm: 0.4550
2022-03-30 21:17:05,273 - depth - INFO - Iter [19850/38400]	lr: 4.735e-05, eta: 3:54:34, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0466, loss: 0.0466, grad_norm: 0.4918
2022-03-30 21:17:39,387 - depth - INFO - Iter [19900/38400]	lr: 4.714e-05, eta: 3:53:53, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0455, loss: 0.0455, grad_norm: 0.3345
2022-03-30 21:18:13,162 - depth - INFO - Iter [19950/38400]	lr: 4.694e-05, eta: 3:53:11, time: 0.676, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0463, loss: 0.0463, grad_norm: 0.4431
2022-03-30 21:18:47,542 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 21:18:47,542 - depth - INFO - Iter [20000/38400]	lr: 4.673e-05, eta: 3:52:30, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0454, loss: 0.0454, grad_norm: 0.5207
2022-03-30 21:19:27,607 - depth - INFO - Summary:
2022-03-30 21:19:27,608 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9726 | 0.9969 | 0.9993 |  0.0529 | 2.1506 | 0.0231 |  0.0807  | 7.3804 | 0.1574 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 21:19:40,801 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_20000.pth.
2022-03-30 21:19:40,802 - depth - INFO - Best abs_rel is 0.0529 at 20000 iter.
2022-03-30 21:19:40,803 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 21:19:40,803 - depth - INFO - Iter(val) [82]	a1: 0.9726, a2: 0.9969, a3: 0.9993, abs_rel: 0.052901461720466614, rmse: 2.1505513191223145, log_10: 0.02310517430305481, rmse_log: 0.08073852956295013, silog: 7.3804, sq_rel: 0.1573580503463745
2022-03-30 21:20:14,722 - depth - INFO - Iter [20050/38400]	lr: 4.653e-05, eta: 3:52:37, time: 1.743, data_time: 1.072, memory: 25023, decode.loss_depth: 0.0458, loss: 0.0458, grad_norm: 0.4616
2022-03-30 21:20:48,715 - depth - INFO - Iter [20100/38400]	lr: 4.633e-05, eta: 3:51:55, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0458, loss: 0.0458, grad_norm: 0.4982
2022-03-30 21:21:22,702 - depth - INFO - Iter [20150/38400]	lr: 4.612e-05, eta: 3:51:14, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0468, loss: 0.0468, grad_norm: 0.4088
2022-03-30 21:21:57,017 - depth - INFO - Iter [20200/38400]	lr: 4.592e-05, eta: 3:50:32, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0458, loss: 0.0458, grad_norm: 0.5613
2022-03-30 21:22:31,021 - depth - INFO - Iter [20250/38400]	lr: 4.571e-05, eta: 3:49:51, time: 0.679, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0463, loss: 0.0463, grad_norm: 0.4329
2022-03-30 21:23:14,417 - depth - INFO - Iter [20300/38400]	lr: 4.551e-05, eta: 3:49:18, time: 0.868, data_time: 0.183, memory: 25023, decode.loss_depth: 0.0455, loss: 0.0455, grad_norm: 0.4603
2022-03-30 21:23:48,480 - depth - INFO - Iter [20350/38400]	lr: 4.531e-05, eta: 3:48:36, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0454, loss: 0.0454, grad_norm: 0.4193
2022-03-30 21:24:22,447 - depth - INFO - Iter [20400/38400]	lr: 4.510e-05, eta: 3:47:55, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0455, loss: 0.0455, grad_norm: 0.3776
2022-03-30 21:24:56,808 - depth - INFO - Iter [20450/38400]	lr: 4.490e-05, eta: 3:47:13, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0444, loss: 0.0444, grad_norm: 0.3824
2022-03-30 21:25:31,225 - depth - INFO - Iter [20500/38400]	lr: 4.470e-05, eta: 3:46:32, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0447, loss: 0.0447, grad_norm: 0.3476
2022-03-30 21:26:05,371 - depth - INFO - Iter [20550/38400]	lr: 4.449e-05, eta: 3:45:51, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0463, loss: 0.0463, grad_norm: 0.4432
2022-03-30 21:26:39,366 - depth - INFO - Iter [20600/38400]	lr: 4.429e-05, eta: 3:45:10, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0450, loss: 0.0450, grad_norm: 0.3698
2022-03-30 21:27:13,505 - depth - INFO - Iter [20650/38400]	lr: 4.409e-05, eta: 3:44:28, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0454, loss: 0.0454, grad_norm: 0.3620
2022-03-30 21:27:47,788 - depth - INFO - Iter [20700/38400]	lr: 4.388e-05, eta: 3:43:47, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0450, loss: 0.0450, grad_norm: 0.5134
2022-03-30 21:28:22,020 - depth - INFO - Iter [20750/38400]	lr: 4.368e-05, eta: 3:43:06, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0454, loss: 0.0454, grad_norm: 0.4238
2022-03-30 21:28:55,881 - depth - INFO - Saving checkpoint at 20800 iterations
2022-03-30 21:29:10,932 - depth - INFO - Iter [20800/38400]	lr: 4.348e-05, eta: 3:42:38, time: 0.978, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0456, loss: 0.0456, grad_norm: 0.4328
2022-03-30 21:29:50,697 - depth - INFO - Summary:
2022-03-30 21:29:50,698 - depth - INFO - 
+--------+-------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2  |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+-------+--------+---------+--------+--------+----------+--------+--------+
| 0.9737 | 0.997 | 0.9993 |  0.0541 | 2.1047 | 0.0233 |  0.0801  | 7.2979 |  0.16  |
+--------+-------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 21:29:50,699 - depth - INFO - Iter(val) [82]	a1: 0.9737, a2: 0.9970, a3: 0.9993, abs_rel: 0.054131485521793365, rmse: 2.104706287384033, log_10: 0.023311905562877655, rmse_log: 0.08008462190628052, silog: 7.2979, sq_rel: 0.15996690094470978
2022-03-30 21:30:24,506 - depth - INFO - Iter [20850/38400]	lr: 4.328e-05, eta: 3:42:30, time: 1.471, data_time: 0.800, memory: 25023, decode.loss_depth: 0.0448, loss: 0.0448, grad_norm: 0.4139
2022-03-30 21:30:58,664 - depth - INFO - Iter [20900/38400]	lr: 4.307e-05, eta: 3:41:48, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0457, loss: 0.0457, grad_norm: 0.3983
2022-03-30 21:31:32,451 - depth - INFO - Iter [20950/38400]	lr: 4.287e-05, eta: 3:41:07, time: 0.676, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0449, loss: 0.0449, grad_norm: 0.4366
2022-03-30 21:32:06,486 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 21:32:06,487 - depth - INFO - Iter [21000/38400]	lr: 4.267e-05, eta: 3:40:26, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0448, loss: 0.0448, grad_norm: 0.5468
2022-03-30 21:32:40,845 - depth - INFO - Iter [21050/38400]	lr: 4.247e-05, eta: 3:39:45, time: 0.687, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0450, loss: 0.0450, grad_norm: 0.4682
2022-03-30 21:33:15,294 - depth - INFO - Iter [21100/38400]	lr: 4.226e-05, eta: 3:39:04, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0449, loss: 0.0449, grad_norm: 0.3870
2022-03-30 21:33:49,373 - depth - INFO - Iter [21150/38400]	lr: 4.206e-05, eta: 3:38:23, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0443, loss: 0.0443, grad_norm: 0.4372
2022-03-30 21:34:23,434 - depth - INFO - Iter [21200/38400]	lr: 4.186e-05, eta: 3:37:41, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0444, loss: 0.0444, grad_norm: 0.3834
2022-03-30 21:34:57,367 - depth - INFO - Iter [21250/38400]	lr: 4.166e-05, eta: 3:37:00, time: 0.679, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0449, loss: 0.0449, grad_norm: 0.3739
2022-03-30 21:35:31,487 - depth - INFO - Iter [21300/38400]	lr: 4.146e-05, eta: 3:36:19, time: 0.682, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0437, loss: 0.0437, grad_norm: 0.3583
2022-03-30 21:36:05,755 - depth - INFO - Iter [21350/38400]	lr: 4.125e-05, eta: 3:35:38, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0451, loss: 0.0451, grad_norm: 0.5458
2022-03-30 21:36:39,849 - depth - INFO - Iter [21400/38400]	lr: 4.105e-05, eta: 3:34:57, time: 0.682, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0444, loss: 0.0444, grad_norm: 0.4564
2022-03-30 21:37:14,292 - depth - INFO - Iter [21450/38400]	lr: 4.085e-05, eta: 3:34:16, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0446, loss: 0.0446, grad_norm: 0.5054
2022-03-30 21:37:48,229 - depth - INFO - Iter [21500/38400]	lr: 4.065e-05, eta: 3:33:35, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0440, loss: 0.0440, grad_norm: 0.5271
2022-03-30 21:38:22,212 - depth - INFO - Iter [21550/38400]	lr: 4.045e-05, eta: 3:32:54, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0444, loss: 0.0444, grad_norm: 0.5156
2022-03-30 21:38:56,025 - depth - INFO - Iter [21600/38400]	lr: 4.025e-05, eta: 3:32:13, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0438, loss: 0.0438, grad_norm: 0.3861
2022-03-30 21:39:36,074 - depth - INFO - Summary:
2022-03-30 21:39:36,075 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
| 0.9725 | 0.9968 | 0.9993 |  0.0535 | 2.1645 | 0.0233 |  0.0806  | 7.389 | 0.1589 |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
2022-03-30 21:39:36,076 - depth - INFO - Iter(val) [82]	a1: 0.9725, a2: 0.9968, a3: 0.9993, abs_rel: 0.05348595976829529, rmse: 2.164461374282837, log_10: 0.02326393686234951, rmse_log: 0.0806482583284378, silog: 7.3890, sq_rel: 0.15886521339416504
2022-03-30 21:40:10,206 - depth - INFO - Iter [21650/38400]	lr: 4.005e-05, eta: 3:32:03, time: 1.483, data_time: 0.806, memory: 25023, decode.loss_depth: 0.0442, loss: 0.0442, grad_norm: 0.3592
2022-03-30 21:40:44,604 - depth - INFO - Iter [21700/38400]	lr: 3.985e-05, eta: 3:31:23, time: 0.688, data_time: 0.010, memory: 25023, decode.loss_depth: 0.0443, loss: 0.0443, grad_norm: 0.4239
2022-03-30 21:41:28,169 - depth - INFO - Iter [21750/38400]	lr: 3.965e-05, eta: 3:30:49, time: 0.871, data_time: 0.192, memory: 25023, decode.loss_depth: 0.0439, loss: 0.0439, grad_norm: 0.4645
2022-03-30 21:42:02,371 - depth - INFO - Iter [21800/38400]	lr: 3.945e-05, eta: 3:30:08, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0441, loss: 0.0441, grad_norm: 0.3643
2022-03-30 21:42:36,321 - depth - INFO - Iter [21850/38400]	lr: 3.925e-05, eta: 3:29:27, time: 0.679, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0435, loss: 0.0435, grad_norm: 0.3815
2022-03-30 21:43:10,528 - depth - INFO - Iter [21900/38400]	lr: 3.905e-05, eta: 3:28:46, time: 0.684, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0433, loss: 0.0433, grad_norm: 0.3757
2022-03-30 21:43:44,871 - depth - INFO - Iter [21950/38400]	lr: 3.885e-05, eta: 3:28:06, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0435, loss: 0.0435, grad_norm: 0.3641
2022-03-30 21:44:18,696 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 21:44:18,697 - depth - INFO - Iter [22000/38400]	lr: 3.865e-05, eta: 3:27:25, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0430, loss: 0.0430, grad_norm: 0.3184
2022-03-30 21:44:52,857 - depth - INFO - Iter [22050/38400]	lr: 3.845e-05, eta: 3:26:44, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0435, loss: 0.0435, grad_norm: 0.3670
2022-03-30 21:45:26,662 - depth - INFO - Iter [22100/38400]	lr: 3.825e-05, eta: 3:26:03, time: 0.676, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0440, loss: 0.0440, grad_norm: 0.4686
2022-03-30 21:46:00,560 - depth - INFO - Iter [22150/38400]	lr: 3.805e-05, eta: 3:25:22, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0439, loss: 0.0439, grad_norm: 0.4521
2022-03-30 21:46:34,752 - depth - INFO - Iter [22200/38400]	lr: 3.785e-05, eta: 3:24:41, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0433, loss: 0.0433, grad_norm: 0.3824
2022-03-30 21:47:08,611 - depth - INFO - Iter [22250/38400]	lr: 3.766e-05, eta: 3:24:01, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0433, loss: 0.0433, grad_norm: 0.4000
2022-03-30 21:47:42,456 - depth - INFO - Iter [22300/38400]	lr: 3.746e-05, eta: 3:23:20, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0433, loss: 0.0433, grad_norm: 0.3844
2022-03-30 21:48:16,522 - depth - INFO - Iter [22350/38400]	lr: 3.726e-05, eta: 3:22:39, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0431, loss: 0.0431, grad_norm: 0.4000
2022-03-30 21:48:50,280 - depth - INFO - Saving checkpoint at 22400 iterations
2022-03-30 21:49:05,452 - depth - INFO - Iter [22400/38400]	lr: 3.706e-05, eta: 3:22:09, time: 0.979, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0439, loss: 0.0439, grad_norm: 0.4121
2022-03-30 21:49:45,520 - depth - INFO - Summary:
2022-03-30 21:49:45,520 - depth - INFO - 
+--------+-------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2  |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+-------+--------+---------+--------+--------+----------+--------+--------+
| 0.9741 | 0.997 | 0.9993 |  0.0521 | 2.1197 | 0.0227 |  0.0795  | 7.2941 | 0.1547 |
+--------+-------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 21:49:59,680 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_22400.pth.
2022-03-30 21:49:59,680 - depth - INFO - Best abs_rel is 0.0521 at 22400 iter.
2022-03-30 21:49:59,681 - depth - INFO - Iter(val) [82]	a1: 0.9741, a2: 0.9970, a3: 0.9993, abs_rel: 0.052138932049274445, rmse: 2.1196858882904053, log_10: 0.0226946659386158, rmse_log: 0.07946588099002838, silog: 7.2941, sq_rel: 0.1546775996685028
2022-03-30 21:50:33,851 - depth - INFO - Iter [22450/38400]	lr: 3.687e-05, eta: 3:22:07, time: 1.768, data_time: 1.092, memory: 25023, decode.loss_depth: 0.0427, loss: 0.0427, grad_norm: 0.3927
2022-03-30 21:51:07,772 - depth - INFO - Iter [22500/38400]	lr: 3.667e-05, eta: 3:21:26, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0431, loss: 0.0431, grad_norm: 0.3502
2022-03-30 21:51:41,950 - depth - INFO - Iter [22550/38400]	lr: 3.647e-05, eta: 3:20:45, time: 0.684, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0435, loss: 0.0435, grad_norm: 0.4130
2022-03-30 21:52:16,076 - depth - INFO - Iter [22600/38400]	lr: 3.627e-05, eta: 3:20:05, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0431, loss: 0.0431, grad_norm: 0.4909
2022-03-30 21:52:50,228 - depth - INFO - Iter [22650/38400]	lr: 3.608e-05, eta: 3:19:24, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0440, loss: 0.0440, grad_norm: 0.5194
2022-03-30 21:53:24,093 - depth - INFO - Iter [22700/38400]	lr: 3.588e-05, eta: 3:18:43, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0430, loss: 0.0430, grad_norm: 0.4204
2022-03-30 21:53:57,899 - depth - INFO - Iter [22750/38400]	lr: 3.569e-05, eta: 3:18:02, time: 0.676, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0430, loss: 0.0430, grad_norm: 0.4758
2022-03-30 21:54:31,858 - depth - INFO - Iter [22800/38400]	lr: 3.549e-05, eta: 3:17:22, time: 0.679, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0433, loss: 0.0433, grad_norm: 0.4099
2022-03-30 21:55:06,065 - depth - INFO - Iter [22850/38400]	lr: 3.529e-05, eta: 3:16:41, time: 0.684, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0419, loss: 0.0419, grad_norm: 0.3385
2022-03-30 21:55:40,219 - depth - INFO - Iter [22900/38400]	lr: 3.510e-05, eta: 3:16:01, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0421, loss: 0.0421, grad_norm: 0.3523
2022-03-30 21:56:14,369 - depth - INFO - Iter [22950/38400]	lr: 3.490e-05, eta: 3:15:20, time: 0.683, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0424, loss: 0.0424, grad_norm: 0.4177
2022-03-30 21:56:48,485 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 21:56:48,486 - depth - INFO - Iter [23000/38400]	lr: 3.471e-05, eta: 3:14:40, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0423, loss: 0.0423, grad_norm: 0.3430
2022-03-30 21:57:22,413 - depth - INFO - Iter [23050/38400]	lr: 3.451e-05, eta: 3:13:59, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0421, loss: 0.0421, grad_norm: 0.4747
2022-03-30 21:57:56,866 - depth - INFO - Iter [23100/38400]	lr: 3.432e-05, eta: 3:13:19, time: 0.688, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0426, loss: 0.0426, grad_norm: 0.4656
2022-03-30 21:58:30,690 - depth - INFO - Iter [23150/38400]	lr: 3.413e-05, eta: 3:12:38, time: 0.678, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0426, loss: 0.0426, grad_norm: 0.3775
2022-03-30 21:59:14,650 - depth - INFO - Iter [23200/38400]	lr: 3.393e-05, eta: 3:12:04, time: 0.879, data_time: 0.183, memory: 25023, decode.loss_depth: 0.0423, loss: 0.0423, grad_norm: 0.4069
2022-03-30 21:59:54,814 - depth - INFO - Summary:
2022-03-30 21:59:54,816 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9723 | 0.9967 | 0.9992 |  0.0525 | 2.1828 | 0.023  |  0.0807  | 7.3865 | 0.1604 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 21:59:54,816 - depth - INFO - Iter(val) [82]	a1: 0.9723, a2: 0.9967, a3: 0.9992, abs_rel: 0.0524529293179512, rmse: 2.182844638824463, log_10: 0.02301783300936222, rmse_log: 0.08065690845251083, silog: 7.3865, sq_rel: 0.1604153960943222
2022-03-30 22:00:29,325 - depth - INFO - Iter [23250/38400]	lr: 3.374e-05, eta: 3:11:50, time: 1.494, data_time: 0.809, memory: 25023, decode.loss_depth: 0.0426, loss: 0.0426, grad_norm: 0.3679
2022-03-30 22:01:03,394 - depth - INFO - Iter [23300/38400]	lr: 3.355e-05, eta: 3:11:10, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0432, loss: 0.0432, grad_norm: 0.4333
2022-03-30 22:01:37,946 - depth - INFO - Iter [23350/38400]	lr: 3.335e-05, eta: 3:10:30, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0414, loss: 0.0414, grad_norm: 0.3143
2022-03-30 22:02:12,257 - depth - INFO - Iter [23400/38400]	lr: 3.316e-05, eta: 3:09:49, time: 0.686, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0409, loss: 0.0409, grad_norm: 0.3640
2022-03-30 22:02:46,626 - depth - INFO - Iter [23450/38400]	lr: 3.297e-05, eta: 3:09:09, time: 0.687, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0418, loss: 0.0418, grad_norm: 0.3945
2022-03-30 22:03:21,332 - depth - INFO - Iter [23500/38400]	lr: 3.277e-05, eta: 3:08:29, time: 0.694, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0421, loss: 0.0421, grad_norm: 0.4764
2022-03-30 22:03:55,427 - depth - INFO - Iter [23550/38400]	lr: 3.258e-05, eta: 3:07:49, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0410, loss: 0.0410, grad_norm: 0.3222
2022-03-30 22:04:29,680 - depth - INFO - Iter [23600/38400]	lr: 3.239e-05, eta: 3:07:08, time: 0.685, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0421, loss: 0.0421, grad_norm: 0.3568
2022-03-30 22:05:03,694 - depth - INFO - Iter [23650/38400]	lr: 3.220e-05, eta: 3:06:28, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0410, loss: 0.0410, grad_norm: 0.3626
2022-03-30 22:05:37,651 - depth - INFO - Iter [23700/38400]	lr: 3.201e-05, eta: 3:05:48, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0420, loss: 0.0420, grad_norm: 0.4802
2022-03-30 22:06:11,773 - depth - INFO - Iter [23750/38400]	lr: 3.182e-05, eta: 3:05:07, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0416, loss: 0.0416, grad_norm: 0.3448
2022-03-30 22:06:45,822 - depth - INFO - Iter [23800/38400]	lr: 3.163e-05, eta: 3:04:27, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0425, loss: 0.0425, grad_norm: 0.3496
2022-03-30 22:07:19,895 - depth - INFO - Iter [23850/38400]	lr: 3.144e-05, eta: 3:03:47, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0410, loss: 0.0410, grad_norm: 0.4076
2022-03-30 22:07:54,309 - depth - INFO - Iter [23900/38400]	lr: 3.125e-05, eta: 3:03:07, time: 0.688, data_time: 0.011, memory: 25023, decode.loss_depth: 0.0419, loss: 0.0419, grad_norm: 0.3984
2022-03-30 22:08:28,362 - depth - INFO - Iter [23950/38400]	lr: 3.106e-05, eta: 3:02:27, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0419, loss: 0.0419, grad_norm: 0.3885
2022-03-30 22:09:02,189 - depth - INFO - Saving checkpoint at 24000 iterations
2022-03-30 22:09:17,046 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 22:09:17,047 - depth - INFO - Iter [24000/38400]	lr: 3.087e-05, eta: 3:01:55, time: 0.974, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0407, loss: 0.0407, grad_norm: 0.3418
2022-03-30 22:09:56,946 - depth - INFO - Summary:
2022-03-30 22:09:56,947 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9729 | 0.9968 | 0.9993 |  0.0535 | 2.1537 | 0.0232 |  0.0806  | 7.3711 | 0.1587 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 22:09:56,948 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 22:09:56,948 - depth - INFO - Iter(val) [82]	a1: 0.9729, a2: 0.9968, a3: 0.9993, abs_rel: 0.053468067198991776, rmse: 2.15366792678833, log_10: 0.02318168617784977, rmse_log: 0.08056487143039703, silog: 7.3711, sq_rel: 0.15866366028785706
2022-03-30 22:10:31,029 - depth - INFO - Iter [24050/38400]	lr: 3.068e-05, eta: 3:01:39, time: 1.479, data_time: 0.806, memory: 25023, decode.loss_depth: 0.0416, loss: 0.0416, grad_norm: 0.4135
2022-03-30 22:11:04,872 - depth - INFO - Iter [24100/38400]	lr: 3.049e-05, eta: 3:00:59, time: 0.677, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0415, loss: 0.0415, grad_norm: 0.3633
2022-03-30 22:11:39,017 - depth - INFO - Iter [24150/38400]	lr: 3.030e-05, eta: 3:00:18, time: 0.683, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0416, loss: 0.0416, grad_norm: 0.3751
2022-03-30 22:12:13,220 - depth - INFO - Iter [24200/38400]	lr: 3.012e-05, eta: 2:59:38, time: 0.684, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0411, loss: 0.0411, grad_norm: 0.3653
2022-03-30 22:12:47,024 - depth - INFO - Iter [24250/38400]	lr: 2.993e-05, eta: 2:58:58, time: 0.676, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0419, loss: 0.0419, grad_norm: 0.3468
2022-03-30 22:13:21,017 - depth - INFO - Iter [24300/38400]	lr: 2.974e-05, eta: 2:58:18, time: 0.680, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0416, loss: 0.0416, grad_norm: 0.3528
2022-03-30 22:13:55,142 - depth - INFO - Iter [24350/38400]	lr: 2.955e-05, eta: 2:57:37, time: 0.682, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0410, loss: 0.0410, grad_norm: 0.3424
2022-03-30 22:14:29,216 - depth - INFO - Iter [24400/38400]	lr: 2.937e-05, eta: 2:56:57, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0413, loss: 0.0413, grad_norm: 0.3455
2022-03-30 22:15:03,223 - depth - INFO - Iter [24450/38400]	lr: 2.918e-05, eta: 2:56:17, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0407, loss: 0.0407, grad_norm: 0.3572
2022-03-30 22:15:37,350 - depth - INFO - Iter [24500/38400]	lr: 2.900e-05, eta: 2:55:37, time: 0.683, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0405, loss: 0.0405, grad_norm: 0.4692
2022-03-30 22:16:11,291 - depth - INFO - Iter [24550/38400]	lr: 2.881e-05, eta: 2:54:57, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0413, loss: 0.0413, grad_norm: 0.4649
2022-03-30 22:16:54,237 - depth - INFO - Iter [24600/38400]	lr: 2.863e-05, eta: 2:54:22, time: 0.859, data_time: 0.183, memory: 25023, decode.loss_depth: 0.0413, loss: 0.0413, grad_norm: 0.3417
2022-03-30 22:17:28,578 - depth - INFO - Iter [24650/38400]	lr: 2.844e-05, eta: 2:53:42, time: 0.687, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0412, loss: 0.0412, grad_norm: 0.3387
2022-03-30 22:18:03,060 - depth - INFO - Iter [24700/38400]	lr: 2.826e-05, eta: 2:53:02, time: 0.689, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0404, loss: 0.0404, grad_norm: 0.3492
2022-03-30 22:18:37,068 - depth - INFO - Iter [24750/38400]	lr: 2.807e-05, eta: 2:52:22, time: 0.680, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0409, loss: 0.0409, grad_norm: 0.4363
2022-03-30 22:19:11,257 - depth - INFO - Iter [24800/38400]	lr: 2.789e-05, eta: 2:51:42, time: 0.684, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0406, loss: 0.0406, grad_norm: 0.3882
2022-03-30 22:19:51,469 - depth - INFO - Summary:
2022-03-30 22:19:51,471 - depth - INFO - 
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1  |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.974 | 0.9968 | 0.9993 |  0.0522 | 2.1464 | 0.0228 |  0.0796  | 7.3036 | 0.1559 |
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 22:19:51,473 - depth - INFO - Iter(val) [82]	a1: 0.9740, a2: 0.9968, a3: 0.9993, abs_rel: 0.05215458944439888, rmse: 2.146365165710449, log_10: 0.02278602123260498, rmse_log: 0.07955677807331085, silog: 7.3036, sq_rel: 0.1558685004711151
2022-03-30 22:20:25,609 - depth - INFO - Iter [24850/38400]	lr: 2.771e-05, eta: 2:51:24, time: 1.487, data_time: 0.810, memory: 25023, decode.loss_depth: 0.0408, loss: 0.0408, grad_norm: 0.3712
2022-03-30 22:20:59,669 - depth - INFO - Iter [24900/38400]	lr: 2.752e-05, eta: 2:50:44, time: 0.681, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0409, loss: 0.0409, grad_norm: 0.2926
2022-03-30 22:21:33,529 - depth - INFO - Iter [24950/38400]	lr: 2.734e-05, eta: 2:50:04, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0411, loss: 0.0411, grad_norm: 0.3282
2022-03-30 22:22:07,526 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 22:22:07,526 - depth - INFO - Iter [25000/38400]	lr: 2.716e-05, eta: 2:49:24, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0407, loss: 0.0407, grad_norm: 0.3705
2022-03-30 22:22:41,435 - depth - INFO - Iter [25050/38400]	lr: 2.698e-05, eta: 2:48:44, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0408, loss: 0.0408, grad_norm: 0.3601
2022-03-30 22:23:15,326 - depth - INFO - Iter [25100/38400]	lr: 2.680e-05, eta: 2:48:04, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0411, loss: 0.0411, grad_norm: 0.3275
2022-03-30 22:23:49,041 - depth - INFO - Iter [25150/38400]	lr: 2.661e-05, eta: 2:47:24, time: 0.674, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0404, loss: 0.0404, grad_norm: 0.4473
2022-03-30 22:24:23,029 - depth - INFO - Iter [25200/38400]	lr: 2.643e-05, eta: 2:46:44, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0396, loss: 0.0396, grad_norm: 0.3441
2022-03-30 22:24:56,930 - depth - INFO - Iter [25250/38400]	lr: 2.625e-05, eta: 2:46:04, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0400, loss: 0.0400, grad_norm: 0.2997
2022-03-30 22:25:30,898 - depth - INFO - Iter [25300/38400]	lr: 2.607e-05, eta: 2:45:24, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0404, loss: 0.0404, grad_norm: 0.2866
2022-03-30 22:26:04,754 - depth - INFO - Iter [25350/38400]	lr: 2.589e-05, eta: 2:44:44, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0406, loss: 0.0406, grad_norm: 0.3346
2022-03-30 22:26:38,695 - depth - INFO - Iter [25400/38400]	lr: 2.572e-05, eta: 2:44:04, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0401, loss: 0.0401, grad_norm: 0.3254
2022-03-30 22:27:12,755 - depth - INFO - Iter [25450/38400]	lr: 2.554e-05, eta: 2:43:24, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0406, loss: 0.0406, grad_norm: 0.3213
2022-03-30 22:27:46,684 - depth - INFO - Iter [25500/38400]	lr: 2.536e-05, eta: 2:42:44, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0392, loss: 0.0392, grad_norm: 0.3399
2022-03-30 22:28:20,731 - depth - INFO - Iter [25550/38400]	lr: 2.518e-05, eta: 2:42:05, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0402, loss: 0.0402, grad_norm: 0.3128
2022-03-30 22:28:54,811 - depth - INFO - Saving checkpoint at 25600 iterations
2022-03-30 22:29:09,457 - depth - INFO - Iter [25600/38400]	lr: 2.500e-05, eta: 2:41:32, time: 0.975, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0401, loss: 0.0401, grad_norm: 0.4404
2022-03-30 22:29:48,915 - depth - INFO - Summary:
2022-03-30 22:29:48,916 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9734 | 0.9967 | 0.9992 |  0.0529 | 2.1666 | 0.0234 |  0.0809  | 7.2964 | 0.1595 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 22:29:48,917 - depth - INFO - Iter(val) [82]	a1: 0.9734, a2: 0.9967, a3: 0.9992, abs_rel: 0.05294642969965935, rmse: 2.1665635108947754, log_10: 0.02335333451628685, rmse_log: 0.08090073615312576, silog: 7.2964, sq_rel: 0.15952716767787933
2022-03-30 22:30:23,115 - depth - INFO - Iter [25650/38400]	lr: 2.483e-05, eta: 2:41:12, time: 1.473, data_time: 0.794, memory: 25023, decode.loss_depth: 0.0406, loss: 0.0406, grad_norm: 0.3183
2022-03-30 22:30:57,126 - depth - INFO - Iter [25700/38400]	lr: 2.465e-05, eta: 2:40:32, time: 0.680, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0399, loss: 0.0399, grad_norm: 0.3745
2022-03-30 22:31:31,032 - depth - INFO - Iter [25750/38400]	lr: 2.447e-05, eta: 2:39:52, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0404, loss: 0.0404, grad_norm: 0.3544
2022-03-30 22:32:04,977 - depth - INFO - Iter [25800/38400]	lr: 2.430e-05, eta: 2:39:13, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0394, loss: 0.0394, grad_norm: 0.3437
2022-03-30 22:32:38,785 - depth - INFO - Iter [25850/38400]	lr: 2.412e-05, eta: 2:38:33, time: 0.676, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0407, loss: 0.0407, grad_norm: 0.3228
2022-03-30 22:33:12,742 - depth - INFO - Iter [25900/38400]	lr: 2.395e-05, eta: 2:37:53, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0405, loss: 0.0405, grad_norm: 0.2921
2022-03-30 22:33:46,874 - depth - INFO - Iter [25950/38400]	lr: 2.377e-05, eta: 2:37:13, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0397, loss: 0.0397, grad_norm: 0.3178
2022-03-30 22:34:21,248 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 22:34:21,248 - depth - INFO - Iter [26000/38400]	lr: 2.360e-05, eta: 2:36:34, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0401, loss: 0.0401, grad_norm: 0.2770
2022-03-30 22:35:04,481 - depth - INFO - Iter [26050/38400]	lr: 2.343e-05, eta: 2:35:58, time: 0.865, data_time: 0.179, memory: 25023, decode.loss_depth: 0.0402, loss: 0.0402, grad_norm: 0.3006
2022-03-30 22:35:38,625 - depth - INFO - Iter [26100/38400]	lr: 2.325e-05, eta: 2:35:19, time: 0.683, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0397, loss: 0.0397, grad_norm: 0.2848
2022-03-30 22:36:12,724 - depth - INFO - Iter [26150/38400]	lr: 2.308e-05, eta: 2:34:39, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0394, loss: 0.0394, grad_norm: 0.3774
2022-03-30 22:36:46,664 - depth - INFO - Iter [26200/38400]	lr: 2.291e-05, eta: 2:33:59, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0395, loss: 0.0395, grad_norm: 0.3848
2022-03-30 22:37:20,999 - depth - INFO - Iter [26250/38400]	lr: 2.274e-05, eta: 2:33:20, time: 0.687, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0399, loss: 0.0399, grad_norm: 0.3811
2022-03-30 22:37:55,275 - depth - INFO - Iter [26300/38400]	lr: 2.257e-05, eta: 2:32:40, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0391, loss: 0.0391, grad_norm: 0.3142
2022-03-30 22:38:29,629 - depth - INFO - Iter [26350/38400]	lr: 2.240e-05, eta: 2:32:01, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0399, loss: 0.0399, grad_norm: 0.2943
2022-03-30 22:39:03,695 - depth - INFO - Iter [26400/38400]	lr: 2.222e-05, eta: 2:31:21, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0389, loss: 0.0389, grad_norm: 0.4059
2022-03-30 22:39:45,033 - depth - INFO - Summary:
2022-03-30 22:39:45,034 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9741 | 0.9968 | 0.9993 |  0.0516 | 2.1321 | 0.0225 |  0.079   | 7.2315 | 0.1549 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 22:39:58,919 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_26400.pth.
2022-03-30 22:39:58,920 - depth - INFO - Best abs_rel is 0.0516 at 26400 iter.
2022-03-30 22:39:58,920 - depth - INFO - Iter(val) [82]	a1: 0.9741, a2: 0.9968, a3: 0.9993, abs_rel: 0.0515875443816185, rmse: 2.13208270072937, log_10: 0.02249082922935486, rmse_log: 0.0790235698223114, silog: 7.2315, sq_rel: 0.1549205482006073
2022-03-30 22:40:33,143 - depth - INFO - Iter [26450/38400]	lr: 2.206e-05, eta: 2:31:07, time: 1.789, data_time: 1.111, memory: 25023, decode.loss_depth: 0.0395, loss: 0.0395, grad_norm: 0.3877
2022-03-30 22:41:07,014 - depth - INFO - Iter [26500/38400]	lr: 2.189e-05, eta: 2:30:27, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0394, loss: 0.0394, grad_norm: 0.3461
2022-03-30 22:41:41,381 - depth - INFO - Iter [26550/38400]	lr: 2.172e-05, eta: 2:29:47, time: 0.687, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0395, loss: 0.0395, grad_norm: 0.3434
2022-03-30 22:42:15,574 - depth - INFO - Iter [26600/38400]	lr: 2.155e-05, eta: 2:29:08, time: 0.684, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0396, loss: 0.0396, grad_norm: 0.4447
2022-03-30 22:42:50,041 - depth - INFO - Iter [26650/38400]	lr: 2.138e-05, eta: 2:28:28, time: 0.689, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0393, loss: 0.0393, grad_norm: 0.3346
2022-03-30 22:43:24,456 - depth - INFO - Iter [26700/38400]	lr: 2.121e-05, eta: 2:27:49, time: 0.688, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0393, loss: 0.0393, grad_norm: 0.3255
2022-03-30 22:43:59,007 - depth - INFO - Iter [26750/38400]	lr: 2.105e-05, eta: 2:27:10, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0393, loss: 0.0393, grad_norm: 0.3260
2022-03-30 22:44:33,141 - depth - INFO - Iter [26800/38400]	lr: 2.088e-05, eta: 2:26:30, time: 0.683, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0388, loss: 0.0388, grad_norm: 0.3980
2022-03-30 22:45:07,284 - depth - INFO - Iter [26850/38400]	lr: 2.071e-05, eta: 2:25:51, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0391, loss: 0.0391, grad_norm: 0.3325
2022-03-30 22:45:41,952 - depth - INFO - Iter [26900/38400]	lr: 2.055e-05, eta: 2:25:11, time: 0.693, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0389, loss: 0.0389, grad_norm: 0.3982
2022-03-30 22:46:16,116 - depth - INFO - Iter [26950/38400]	lr: 2.038e-05, eta: 2:24:32, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0392, loss: 0.0392, grad_norm: 0.3664
2022-03-30 22:46:50,426 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 22:46:50,426 - depth - INFO - Iter [27000/38400]	lr: 2.022e-05, eta: 2:23:52, time: 0.686, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0389, loss: 0.0389, grad_norm: 0.3326
2022-03-30 22:47:24,756 - depth - INFO - Iter [27050/38400]	lr: 2.005e-05, eta: 2:23:13, time: 0.687, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0391, loss: 0.0391, grad_norm: 0.2997
2022-03-30 22:47:58,823 - depth - INFO - Iter [27100/38400]	lr: 1.989e-05, eta: 2:22:34, time: 0.681, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0392, loss: 0.0392, grad_norm: 0.3196
2022-03-30 22:48:33,141 - depth - INFO - Iter [27150/38400]	lr: 1.973e-05, eta: 2:21:54, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0386, loss: 0.0386, grad_norm: 0.3440
2022-03-30 22:49:07,235 - depth - INFO - Saving checkpoint at 27200 iterations
2022-03-30 22:49:22,027 - depth - INFO - Iter [27200/38400]	lr: 1.957e-05, eta: 2:21:21, time: 0.978, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0390, loss: 0.0390, grad_norm: 0.3580
2022-03-30 22:50:01,310 - depth - INFO - Summary:
2022-03-30 22:50:01,311 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9742 | 0.9969 | 0.9993 |  0.0519 | 2.1213 | 0.0226 |  0.0788  | 7.1988 | 0.154  |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 22:50:01,312 - depth - INFO - Iter(val) [82]	a1: 0.9742, a2: 0.9969, a3: 0.9993, abs_rel: 0.05185413360595703, rmse: 2.121260643005371, log_10: 0.02255135588347912, rmse_log: 0.07875929027795792, silog: 7.1988, sq_rel: 0.15400809049606323
2022-03-30 22:50:35,635 - depth - INFO - Iter [27250/38400]	lr: 1.940e-05, eta: 2:20:58, time: 1.472, data_time: 0.790, memory: 25023, decode.loss_depth: 0.0393, loss: 0.0393, grad_norm: 0.3368
2022-03-30 22:51:09,664 - depth - INFO - Iter [27300/38400]	lr: 1.924e-05, eta: 2:20:18, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0386, loss: 0.0386, grad_norm: 0.3303
2022-03-30 22:51:44,209 - depth - INFO - Iter [27350/38400]	lr: 1.908e-05, eta: 2:19:39, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0388, loss: 0.0388, grad_norm: 0.3601
2022-03-30 22:52:18,384 - depth - INFO - Iter [27400/38400]	lr: 1.892e-05, eta: 2:19:00, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0388, loss: 0.0388, grad_norm: 0.2953
2022-03-30 22:52:52,681 - depth - INFO - Iter [27450/38400]	lr: 1.876e-05, eta: 2:18:20, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0392, loss: 0.0392, grad_norm: 0.2844
2022-03-30 22:53:36,102 - depth - INFO - Iter [27500/38400]	lr: 1.860e-05, eta: 2:17:45, time: 0.868, data_time: 0.178, memory: 25023, decode.loss_depth: 0.0383, loss: 0.0383, grad_norm: 0.2503
2022-03-30 22:54:10,356 - depth - INFO - Iter [27550/38400]	lr: 1.844e-05, eta: 2:17:05, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0388, loss: 0.0388, grad_norm: 0.2589
2022-03-30 22:54:44,497 - depth - INFO - Iter [27600/38400]	lr: 1.828e-05, eta: 2:16:26, time: 0.682, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0381, loss: 0.0381, grad_norm: 0.3338
2022-03-30 22:55:18,929 - depth - INFO - Iter [27650/38400]	lr: 1.813e-05, eta: 2:15:47, time: 0.689, data_time: 0.011, memory: 25023, decode.loss_depth: 0.0385, loss: 0.0385, grad_norm: 0.3355
2022-03-30 22:55:53,251 - depth - INFO - Iter [27700/38400]	lr: 1.797e-05, eta: 2:15:07, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0383, loss: 0.0383, grad_norm: 0.3557
2022-03-30 22:56:27,351 - depth - INFO - Iter [27750/38400]	lr: 1.781e-05, eta: 2:14:28, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0392, loss: 0.0392, grad_norm: 0.3049
2022-03-30 22:57:01,273 - depth - INFO - Iter [27800/38400]	lr: 1.766e-05, eta: 2:13:49, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0388, loss: 0.0388, grad_norm: 0.2997
2022-03-30 22:57:35,519 - depth - INFO - Iter [27850/38400]	lr: 1.750e-05, eta: 2:13:09, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0383, loss: 0.0383, grad_norm: 0.2922
2022-03-30 22:58:09,702 - depth - INFO - Iter [27900/38400]	lr: 1.734e-05, eta: 2:12:30, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0390, loss: 0.0390, grad_norm: 0.2837
2022-03-30 22:58:43,885 - depth - INFO - Iter [27950/38400]	lr: 1.719e-05, eta: 2:11:51, time: 0.683, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0385, loss: 0.0385, grad_norm: 0.3571
2022-03-30 22:59:18,052 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 22:59:18,052 - depth - INFO - Iter [28000/38400]	lr: 1.704e-05, eta: 2:11:12, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0382, loss: 0.0382, grad_norm: 0.3517
2022-03-30 22:59:58,060 - depth - INFO - Summary:
2022-03-30 22:59:58,061 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9741 | 0.9969 | 0.9993 |  0.0516 | 2.1166 | 0.0225 |  0.0789  | 7.2196 | 0.1545 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 22:59:58,062 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 22:59:58,063 - depth - INFO - Iter(val) [82]	a1: 0.9741, a2: 0.9969, a3: 0.9993, abs_rel: 0.051622260361909866, rmse: 2.116595983505249, log_10: 0.022507319226861, rmse_log: 0.07888682186603546, silog: 7.2196, sq_rel: 0.15445096790790558
2022-03-30 23:00:31,887 - depth - INFO - Iter [28050/38400]	lr: 1.688e-05, eta: 2:10:47, time: 1.477, data_time: 0.805, memory: 25023, decode.loss_depth: 0.0378, loss: 0.0378, grad_norm: 0.2944
2022-03-30 23:01:06,164 - depth - INFO - Iter [28100/38400]	lr: 1.673e-05, eta: 2:10:08, time: 0.685, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0382, loss: 0.0382, grad_norm: 0.3321
2022-03-30 23:01:40,716 - depth - INFO - Iter [28150/38400]	lr: 1.658e-05, eta: 2:09:29, time: 0.691, data_time: 0.012, memory: 25023, decode.loss_depth: 0.0385, loss: 0.0385, grad_norm: 0.2822
2022-03-30 23:02:14,567 - depth - INFO - Iter [28200/38400]	lr: 1.643e-05, eta: 2:08:49, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0385, loss: 0.0385, grad_norm: 0.2985
2022-03-30 23:02:48,552 - depth - INFO - Iter [28250/38400]	lr: 1.627e-05, eta: 2:08:10, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0382, loss: 0.0382, grad_norm: 0.2909
2022-03-30 23:03:22,545 - depth - INFO - Iter [28300/38400]	lr: 1.612e-05, eta: 2:07:31, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0387, loss: 0.0387, grad_norm: 0.2668
2022-03-30 23:03:56,833 - depth - INFO - Iter [28350/38400]	lr: 1.597e-05, eta: 2:06:52, time: 0.686, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0385, loss: 0.0385, grad_norm: 0.3341
2022-03-30 23:04:30,796 - depth - INFO - Iter [28400/38400]	lr: 1.582e-05, eta: 2:06:12, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0383, loss: 0.0383, grad_norm: 0.2690
2022-03-30 23:05:04,985 - depth - INFO - Iter [28450/38400]	lr: 1.567e-05, eta: 2:05:33, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0380, loss: 0.0380, grad_norm: 0.3091
2022-03-30 23:05:38,988 - depth - INFO - Iter [28500/38400]	lr: 1.553e-05, eta: 2:04:54, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0375, loss: 0.0375, grad_norm: 0.2920
2022-03-30 23:06:12,758 - depth - INFO - Iter [28550/38400]	lr: 1.538e-05, eta: 2:04:15, time: 0.675, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0375, loss: 0.0375, grad_norm: 0.2872
2022-03-30 23:06:47,067 - depth - INFO - Iter [28600/38400]	lr: 1.523e-05, eta: 2:03:36, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0370, loss: 0.0370, grad_norm: 0.3497
2022-03-30 23:07:21,028 - depth - INFO - Iter [28650/38400]	lr: 1.508e-05, eta: 2:02:56, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0378, loss: 0.0378, grad_norm: 0.3473
2022-03-30 23:07:54,996 - depth - INFO - Iter [28700/38400]	lr: 1.494e-05, eta: 2:02:17, time: 0.679, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0373, loss: 0.0373, grad_norm: 0.3554
2022-03-30 23:08:29,430 - depth - INFO - Iter [28750/38400]	lr: 1.479e-05, eta: 2:01:38, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0377, loss: 0.0377, grad_norm: 0.3183
2022-03-30 23:09:03,647 - depth - INFO - Saving checkpoint at 28800 iterations
2022-03-30 23:09:18,127 - depth - INFO - Iter [28800/38400]	lr: 1.465e-05, eta: 2:01:04, time: 0.974, data_time: 0.008, memory: 25023, decode.loss_depth: 0.0378, loss: 0.0378, grad_norm: 0.3019
2022-03-30 23:09:57,549 - depth - INFO - Summary:
2022-03-30 23:09:57,550 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9741 | 0.9968 | 0.9992 |  0.0516 | 2.1419 | 0.0227 |  0.0796  | 7.2532 | 0.1561 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 23:09:57,551 - depth - INFO - Iter(val) [82]	a1: 0.9741, a2: 0.9968, a3: 0.9992, abs_rel: 0.05162152647972107, rmse: 2.141930103302002, log_10: 0.02269619330763817, rmse_log: 0.0795527994632721, silog: 7.2532, sq_rel: 0.15614119172096252
2022-03-30 23:10:31,678 - depth - INFO - Iter [28850/38400]	lr: 1.450e-05, eta: 2:00:38, time: 1.471, data_time: 0.793, memory: 25023, decode.loss_depth: 0.0383, loss: 0.0383, grad_norm: 0.2882
2022-03-30 23:11:05,912 - depth - INFO - Iter [28900/38400]	lr: 1.436e-05, eta: 1:59:59, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0380, loss: 0.0380, grad_norm: 0.3312
2022-03-30 23:11:48,888 - depth - INFO - Iter [28950/38400]	lr: 1.422e-05, eta: 1:59:23, time: 0.859, data_time: 0.185, memory: 25023, decode.loss_depth: 0.0384, loss: 0.0384, grad_norm: 0.3173
2022-03-30 23:12:22,930 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 23:12:22,930 - depth - INFO - Iter [29000/38400]	lr: 1.407e-05, eta: 1:58:44, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0378, loss: 0.0378, grad_norm: 0.2515
2022-03-30 23:12:56,967 - depth - INFO - Iter [29050/38400]	lr: 1.393e-05, eta: 1:58:05, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0372, loss: 0.0372, grad_norm: 0.3119
2022-03-30 23:13:31,422 - depth - INFO - Iter [29100/38400]	lr: 1.379e-05, eta: 1:57:26, time: 0.689, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0377, loss: 0.0377, grad_norm: 0.2491
2022-03-30 23:14:05,586 - depth - INFO - Iter [29150/38400]	lr: 1.365e-05, eta: 1:56:47, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0375, loss: 0.0375, grad_norm: 0.2936
2022-03-30 23:14:39,821 - depth - INFO - Iter [29200/38400]	lr: 1.351e-05, eta: 1:56:07, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0377, loss: 0.0377, grad_norm: 0.2841
2022-03-30 23:15:13,811 - depth - INFO - Iter [29250/38400]	lr: 1.337e-05, eta: 1:55:28, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0383, loss: 0.0383, grad_norm: 0.2468
2022-03-30 23:15:47,772 - depth - INFO - Iter [29300/38400]	lr: 1.323e-05, eta: 1:54:49, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0376, loss: 0.0376, grad_norm: 0.2773
2022-03-30 23:16:21,958 - depth - INFO - Iter [29350/38400]	lr: 1.309e-05, eta: 1:54:10, time: 0.683, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0378, loss: 0.0378, grad_norm: 0.3013
2022-03-30 23:16:56,152 - depth - INFO - Iter [29400/38400]	lr: 1.296e-05, eta: 1:53:31, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0374, loss: 0.0374, grad_norm: 0.3369
2022-03-30 23:17:30,275 - depth - INFO - Iter [29450/38400]	lr: 1.282e-05, eta: 1:52:52, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0376, loss: 0.0376, grad_norm: 0.3111
2022-03-30 23:18:04,543 - depth - INFO - Iter [29500/38400]	lr: 1.268e-05, eta: 1:52:13, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0370, loss: 0.0370, grad_norm: 0.2622
2022-03-30 23:18:38,369 - depth - INFO - Iter [29550/38400]	lr: 1.255e-05, eta: 1:51:34, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0378, loss: 0.0378, grad_norm: 0.2698
2022-03-30 23:19:12,595 - depth - INFO - Iter [29600/38400]	lr: 1.241e-05, eta: 1:50:56, time: 0.684, data_time: 0.009, memory: 25023, decode.loss_depth: 0.0370, loss: 0.0370, grad_norm: 0.3116
2022-03-30 23:19:52,734 - depth - INFO - Summary:
2022-03-30 23:19:52,734 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9745 | 0.9968 | 0.9993 |  0.0517 | 2.1445 | 0.0228 |  0.0795  | 7.2171 | 0.1554 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 23:19:52,735 - depth - INFO - Iter(val) [82]	a1: 0.9745, a2: 0.9968, a3: 0.9993, abs_rel: 0.05174742266535759, rmse: 2.1445207595825195, log_10: 0.02275094762444496, rmse_log: 0.07949995249509811, silog: 7.2171, sq_rel: 0.15542395412921906
2022-03-30 23:20:26,758 - depth - INFO - Iter [29650/38400]	lr: 1.228e-05, eta: 1:50:28, time: 1.483, data_time: 0.808, memory: 25023, decode.loss_depth: 0.0372, loss: 0.0372, grad_norm: 0.2903
2022-03-30 23:21:01,047 - depth - INFO - Iter [29700/38400]	lr: 1.214e-05, eta: 1:49:50, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0378, loss: 0.0378, grad_norm: 0.2613
2022-03-30 23:21:35,213 - depth - INFO - Iter [29750/38400]	lr: 1.201e-05, eta: 1:49:11, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0373, loss: 0.0373, grad_norm: 0.3021
2022-03-30 23:22:09,270 - depth - INFO - Iter [29800/38400]	lr: 1.188e-05, eta: 1:48:32, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0368, loss: 0.0368, grad_norm: 0.3098
2022-03-30 23:22:43,635 - depth - INFO - Iter [29850/38400]	lr: 1.174e-05, eta: 1:47:53, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0370, loss: 0.0370, grad_norm: 0.2798
2022-03-30 23:23:17,956 - depth - INFO - Iter [29900/38400]	lr: 1.161e-05, eta: 1:47:14, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0371, loss: 0.0371, grad_norm: 0.3172
2022-03-30 23:23:51,915 - depth - INFO - Iter [29950/38400]	lr: 1.148e-05, eta: 1:46:35, time: 0.679, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0373, loss: 0.0373, grad_norm: 0.2733
2022-03-30 23:24:25,788 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 23:24:25,788 - depth - INFO - Iter [30000/38400]	lr: 1.135e-05, eta: 1:45:56, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0374, loss: 0.0374, grad_norm: 0.2609
2022-03-30 23:24:59,906 - depth - INFO - Iter [30050/38400]	lr: 1.122e-05, eta: 1:45:17, time: 0.682, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0370, loss: 0.0370, grad_norm: 0.2988
2022-03-30 23:25:34,041 - depth - INFO - Iter [30100/38400]	lr: 1.109e-05, eta: 1:44:38, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0374, loss: 0.0374, grad_norm: 0.2989
2022-03-30 23:26:08,374 - depth - INFO - Iter [30150/38400]	lr: 1.097e-05, eta: 1:43:59, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0369, loss: 0.0369, grad_norm: 0.2722
2022-03-30 23:26:42,805 - depth - INFO - Iter [30200/38400]	lr: 1.084e-05, eta: 1:43:21, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0382, loss: 0.0382, grad_norm: 0.2983
2022-03-30 23:27:16,987 - depth - INFO - Iter [30250/38400]	lr: 1.071e-05, eta: 1:42:42, time: 0.684, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0373, loss: 0.0373, grad_norm: 0.2540
2022-03-30 23:27:51,429 - depth - INFO - Iter [30300/38400]	lr: 1.059e-05, eta: 1:42:03, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0374, loss: 0.0374, grad_norm: 0.2529
2022-03-30 23:28:25,677 - depth - INFO - Iter [30350/38400]	lr: 1.046e-05, eta: 1:41:25, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0370, loss: 0.0370, grad_norm: 0.2595
2022-03-30 23:29:09,208 - depth - INFO - Saving checkpoint at 30400 iterations
2022-03-30 23:29:23,925 - depth - INFO - Iter [30400/38400]	lr: 1.033e-05, eta: 1:40:52, time: 1.165, data_time: 0.175, memory: 25023, decode.loss_depth: 0.0373, loss: 0.0373, grad_norm: 0.2717
2022-03-30 23:30:03,808 - depth - INFO - Summary:
2022-03-30 23:30:03,809 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9743 | 0.9969 | 0.9993 |  0.0517 | 2.1518 | 0.0226 |  0.0791  | 7.2326 | 0.1551 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 23:30:03,810 - depth - INFO - Iter(val) [82]	a1: 0.9743, a2: 0.9969, a3: 0.9993, abs_rel: 0.05172959342598915, rmse: 2.151843309402466, log_10: 0.02262861281633377, rmse_log: 0.07914579659700394, silog: 7.2326, sq_rel: 0.1551043838262558
2022-03-30 23:30:38,162 - depth - INFO - Iter [30450/38400]	lr: 1.021e-05, eta: 1:40:24, time: 1.485, data_time: 0.803, memory: 25023, decode.loss_depth: 0.0366, loss: 0.0366, grad_norm: 0.2518
2022-03-30 23:31:12,304 - depth - INFO - Iter [30500/38400]	lr: 1.009e-05, eta: 1:39:45, time: 0.683, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0374, loss: 0.0374, grad_norm: 0.2869
2022-03-30 23:31:46,280 - depth - INFO - Iter [30550/38400]	lr: 9.964e-06, eta: 1:39:06, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0372, loss: 0.0372, grad_norm: 0.2576
2022-03-30 23:32:20,499 - depth - INFO - Iter [30600/38400]	lr: 9.842e-06, eta: 1:38:27, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 0.2374
2022-03-30 23:32:54,807 - depth - INFO - Iter [30650/38400]	lr: 9.721e-06, eta: 1:37:48, time: 0.686, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0367, loss: 0.0367, grad_norm: 0.2435
2022-03-30 23:33:28,622 - depth - INFO - Iter [30700/38400]	lr: 9.600e-06, eta: 1:37:10, time: 0.676, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0366, loss: 0.0366, grad_norm: 0.2453
2022-03-30 23:34:03,240 - depth - INFO - Iter [30750/38400]	lr: 9.480e-06, eta: 1:36:31, time: 0.692, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0365, loss: 0.0365, grad_norm: 0.2254
2022-03-30 23:34:37,442 - depth - INFO - Iter [30800/38400]	lr: 9.360e-06, eta: 1:35:52, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0368, loss: 0.0368, grad_norm: 0.2743
2022-03-30 23:35:11,398 - depth - INFO - Iter [30850/38400]	lr: 9.241e-06, eta: 1:35:13, time: 0.679, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0367, loss: 0.0367, grad_norm: 0.2596
2022-03-30 23:35:45,365 - depth - INFO - Iter [30900/38400]	lr: 9.123e-06, eta: 1:34:35, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0373, loss: 0.0373, grad_norm: 0.2663
2022-03-30 23:36:19,532 - depth - INFO - Iter [30950/38400]	lr: 9.006e-06, eta: 1:33:56, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0369, loss: 0.0369, grad_norm: 0.2499
2022-03-30 23:36:53,614 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 23:36:53,615 - depth - INFO - Iter [31000/38400]	lr: 8.889e-06, eta: 1:33:17, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0371, loss: 0.0371, grad_norm: 0.3046
2022-03-30 23:37:27,705 - depth - INFO - Iter [31050/38400]	lr: 8.773e-06, eta: 1:32:38, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0371, loss: 0.0371, grad_norm: 0.3812
2022-03-30 23:38:01,862 - depth - INFO - Iter [31100/38400]	lr: 8.657e-06, eta: 1:32:00, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0368, loss: 0.0368, grad_norm: 0.2839
2022-03-30 23:38:35,909 - depth - INFO - Iter [31150/38400]	lr: 8.543e-06, eta: 1:31:21, time: 0.681, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0375, loss: 0.0375, grad_norm: 0.2684
2022-03-30 23:39:09,986 - depth - INFO - Iter [31200/38400]	lr: 8.429e-06, eta: 1:30:42, time: 0.681, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0365, loss: 0.0365, grad_norm: 0.2671
2022-03-30 23:39:50,233 - depth - INFO - Summary:
2022-03-30 23:39:50,233 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9745 | 0.9968 | 0.9993 |  0.0516 | 2.1467 | 0.0226 |  0.0791  | 7.2307 | 0.1553 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 23:40:04,858 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_31200.pth.
2022-03-30 23:40:04,858 - depth - INFO - Best abs_rel is 0.0516 at 31200 iter.
2022-03-30 23:40:04,859 - depth - INFO - Iter(val) [82]	a1: 0.9745, a2: 0.9968, a3: 0.9993, abs_rel: 0.051580656319856644, rmse: 2.1466641426086426, log_10: 0.022572562098503113, rmse_log: 0.07913292944431305, silog: 7.2307, sq_rel: 0.1553059220314026
2022-03-30 23:40:39,091 - depth - INFO - Iter [31250/38400]	lr: 8.315e-06, eta: 1:30:16, time: 1.782, data_time: 1.105, memory: 25023, decode.loss_depth: 0.0362, loss: 0.0362, grad_norm: 0.2536
2022-03-30 23:41:13,164 - depth - INFO - Iter [31300/38400]	lr: 8.203e-06, eta: 1:29:38, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0362, loss: 0.0362, grad_norm: 0.2358
2022-03-30 23:41:47,190 - depth - INFO - Iter [31350/38400]	lr: 8.091e-06, eta: 1:28:59, time: 0.681, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0372, loss: 0.0372, grad_norm: 0.2547
2022-03-30 23:42:21,652 - depth - INFO - Iter [31400/38400]	lr: 7.980e-06, eta: 1:28:20, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0367, loss: 0.0367, grad_norm: 0.2703
2022-03-30 23:42:55,702 - depth - INFO - Iter [31450/38400]	lr: 7.869e-06, eta: 1:27:42, time: 0.681, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0366, loss: 0.0366, grad_norm: 0.2543
2022-03-30 23:43:29,834 - depth - INFO - Iter [31500/38400]	lr: 7.760e-06, eta: 1:27:03, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0366, loss: 0.0366, grad_norm: 0.2864
2022-03-30 23:44:03,683 - depth - INFO - Iter [31550/38400]	lr: 7.650e-06, eta: 1:26:24, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0369, loss: 0.0369, grad_norm: 0.2629
2022-03-30 23:44:37,767 - depth - INFO - Iter [31600/38400]	lr: 7.542e-06, eta: 1:25:45, time: 0.682, data_time: 0.008, memory: 25023, decode.loss_depth: 0.0370, loss: 0.0370, grad_norm: 0.2866
2022-03-30 23:45:11,614 - depth - INFO - Iter [31650/38400]	lr: 7.434e-06, eta: 1:25:07, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0362, loss: 0.0362, grad_norm: 0.2619
2022-03-30 23:45:45,818 - depth - INFO - Iter [31700/38400]	lr: 7.327e-06, eta: 1:24:28, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 0.2810
2022-03-30 23:46:19,905 - depth - INFO - Iter [31750/38400]	lr: 7.221e-06, eta: 1:23:50, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0362, loss: 0.0362, grad_norm: 0.2624
2022-03-30 23:46:54,330 - depth - INFO - Iter [31800/38400]	lr: 7.116e-06, eta: 1:23:11, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.2654
2022-03-30 23:47:37,875 - depth - INFO - Iter [31850/38400]	lr: 7.011e-06, eta: 1:22:34, time: 0.871, data_time: 0.186, memory: 25023, decode.loss_depth: 0.0364, loss: 0.0364, grad_norm: 0.2529
2022-03-30 23:48:12,049 - depth - INFO - Iter [31900/38400]	lr: 6.907e-06, eta: 1:21:56, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0365, loss: 0.0365, grad_norm: 0.2317
2022-03-30 23:48:46,711 - depth - INFO - Iter [31950/38400]	lr: 6.803e-06, eta: 1:21:17, time: 0.693, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0365, loss: 0.0365, grad_norm: 0.2545
2022-03-30 23:49:20,835 - depth - INFO - Saving checkpoint at 32000 iterations
2022-03-30 23:49:35,791 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 23:49:35,791 - depth - INFO - Iter [32000/38400]	lr: 6.701e-06, eta: 1:20:42, time: 0.982, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0366, loss: 0.0366, grad_norm: 0.2485
2022-03-30 23:50:15,812 - depth - INFO - Summary:
2022-03-30 23:50:15,813 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9743 | 0.9968 | 0.9993 |  0.0516 | 2.1297 | 0.0226 |  0.0791  | 7.2117 | 0.1548 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-30 23:50:15,814 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-30 23:50:15,814 - depth - INFO - Iter(val) [82]	a1: 0.9743, a2: 0.9968, a3: 0.9993, abs_rel: 0.05161182954907417, rmse: 2.1296653747558594, log_10: 0.022584041580557823, rmse_log: 0.07905033230781555, silog: 7.2117, sq_rel: 0.1548125445842743
2022-03-30 23:50:49,918 - depth - INFO - Iter [32050/38400]	lr: 6.599e-06, eta: 1:20:11, time: 1.482, data_time: 0.806, memory: 25023, decode.loss_depth: 0.0360, loss: 0.0360, grad_norm: 0.2550
2022-03-30 23:51:24,374 - depth - INFO - Iter [32100/38400]	lr: 6.498e-06, eta: 1:19:33, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0369, loss: 0.0369, grad_norm: 0.2492
2022-03-30 23:51:58,593 - depth - INFO - Iter [32150/38400]	lr: 6.397e-06, eta: 1:18:54, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0362, loss: 0.0362, grad_norm: 0.2258
2022-03-30 23:52:32,778 - depth - INFO - Iter [32200/38400]	lr: 6.297e-06, eta: 1:18:15, time: 0.684, data_time: 0.008, memory: 25023, decode.loss_depth: 0.0363, loss: 0.0363, grad_norm: 0.2470
2022-03-30 23:53:06,871 - depth - INFO - Iter [32250/38400]	lr: 6.198e-06, eta: 1:17:37, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0364, loss: 0.0364, grad_norm: 0.2485
2022-03-30 23:53:41,153 - depth - INFO - Iter [32300/38400]	lr: 6.100e-06, eta: 1:16:58, time: 0.686, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.2191
2022-03-30 23:54:15,015 - depth - INFO - Iter [32350/38400]	lr: 6.003e-06, eta: 1:16:20, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0365, loss: 0.0365, grad_norm: 0.2224
2022-03-30 23:54:49,209 - depth - INFO - Iter [32400/38400]	lr: 5.906e-06, eta: 1:15:41, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0363, loss: 0.0363, grad_norm: 0.2427
2022-03-30 23:55:23,334 - depth - INFO - Iter [32450/38400]	lr: 5.810e-06, eta: 1:15:03, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0362, loss: 0.0362, grad_norm: 0.2652
2022-03-30 23:55:57,270 - depth - INFO - Iter [32500/38400]	lr: 5.714e-06, eta: 1:14:24, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 0.2860
2022-03-30 23:56:31,287 - depth - INFO - Iter [32550/38400]	lr: 5.620e-06, eta: 1:13:46, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.2551
2022-03-30 23:57:05,127 - depth - INFO - Iter [32600/38400]	lr: 5.526e-06, eta: 1:13:07, time: 0.677, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0360, loss: 0.0360, grad_norm: 0.2212
2022-03-30 23:57:39,266 - depth - INFO - Iter [32650/38400]	lr: 5.433e-06, eta: 1:12:29, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0365, loss: 0.0365, grad_norm: 0.2236
2022-03-30 23:58:13,532 - depth - INFO - Iter [32700/38400]	lr: 5.341e-06, eta: 1:11:50, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0367, loss: 0.0367, grad_norm: 0.2305
2022-03-30 23:58:47,455 - depth - INFO - Iter [32750/38400]	lr: 5.249e-06, eta: 1:11:12, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0364, loss: 0.0364, grad_norm: 0.2393
2022-03-30 23:59:21,467 - depth - INFO - Iter [32800/38400]	lr: 5.158e-06, eta: 1:10:33, time: 0.680, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.2867
2022-03-31 00:00:01,746 - depth - INFO - Summary:
2022-03-31 00:00:01,746 - depth - INFO - 
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1  |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.974 | 0.9968 | 0.9993 |  0.0519 | 2.1442 | 0.0228 |  0.0796  | 7.2359 | 0.1564 |
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-31 00:00:01,747 - depth - INFO - Iter(val) [82]	a1: 0.9740, a2: 0.9968, a3: 0.9993, abs_rel: 0.05193701386451721, rmse: 2.1441874504089355, log_10: 0.022779876366257668, rmse_log: 0.07956266403198242, silog: 7.2359, sq_rel: 0.15643350780010223
2022-03-31 00:00:35,879 - depth - INFO - Iter [32850/38400]	lr: 5.068e-06, eta: 1:10:02, time: 1.488, data_time: 0.810, memory: 25023, decode.loss_depth: 0.0360, loss: 0.0360, grad_norm: 0.2600
2022-03-31 00:01:10,279 - depth - INFO - Iter [32900/38400]	lr: 4.979e-06, eta: 1:09:23, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 0.2298
2022-03-31 00:01:44,755 - depth - INFO - Iter [32950/38400]	lr: 4.890e-06, eta: 1:08:45, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0362, loss: 0.0362, grad_norm: 0.2393
2022-03-31 00:02:18,847 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-31 00:02:18,847 - depth - INFO - Iter [33000/38400]	lr: 4.802e-06, eta: 1:08:06, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0371, loss: 0.0371, grad_norm: 0.2192
2022-03-31 00:02:53,055 - depth - INFO - Iter [33050/38400]	lr: 4.715e-06, eta: 1:07:28, time: 0.684, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0360, loss: 0.0360, grad_norm: 0.2660
2022-03-31 00:03:27,499 - depth - INFO - Iter [33100/38400]	lr: 4.629e-06, eta: 1:06:49, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0360, loss: 0.0360, grad_norm: 0.2117
2022-03-31 00:04:01,660 - depth - INFO - Iter [33150/38400]	lr: 4.543e-06, eta: 1:06:11, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0364, loss: 0.0364, grad_norm: 0.2160
2022-03-31 00:04:36,181 - depth - INFO - Iter [33200/38400]	lr: 4.458e-06, eta: 1:05:33, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0365, loss: 0.0365, grad_norm: 0.2129
2022-03-31 00:05:10,345 - depth - INFO - Iter [33250/38400]	lr: 4.374e-06, eta: 1:04:54, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.2265
2022-03-31 00:05:53,559 - depth - INFO - Iter [33300/38400]	lr: 4.291e-06, eta: 1:04:17, time: 0.864, data_time: 0.176, memory: 25023, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 0.2139
2022-03-31 00:06:27,812 - depth - INFO - Iter [33350/38400]	lr: 4.209e-06, eta: 1:03:39, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0360, loss: 0.0360, grad_norm: 0.2415
2022-03-31 00:07:02,355 - depth - INFO - Iter [33400/38400]	lr: 4.127e-06, eta: 1:03:01, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.2321
2022-03-31 00:07:36,479 - depth - INFO - Iter [33450/38400]	lr: 4.046e-06, eta: 1:02:22, time: 0.683, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.2145
2022-03-31 00:08:10,831 - depth - INFO - Iter [33500/38400]	lr: 3.966e-06, eta: 1:01:44, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 0.2291
2022-03-31 00:08:45,160 - depth - INFO - Iter [33550/38400]	lr: 3.886e-06, eta: 1:01:06, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.2152
2022-03-31 00:09:19,332 - depth - INFO - Saving checkpoint at 33600 iterations
2022-03-31 00:09:34,315 - depth - INFO - Iter [33600/38400]	lr: 3.808e-06, eta: 1:00:30, time: 0.983, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.1960
2022-03-31 00:10:14,307 - depth - INFO - Summary:
2022-03-31 00:10:14,308 - depth - INFO - 
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
| 0.9742 | 0.9968 | 0.9993 |  0.0519 | 2.146 | 0.0227 |  0.0794  | 7.2582 | 0.1563 |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
2022-03-31 00:10:14,308 - depth - INFO - Iter(val) [82]	a1: 0.9742, a2: 0.9968, a3: 0.9993, abs_rel: 0.051900021731853485, rmse: 2.146000862121582, log_10: 0.02270248346030712, rmse_log: 0.07943879812955856, silog: 7.2582, sq_rel: 0.15632933378219604
2022-03-31 00:10:48,601 - depth - INFO - Iter [33650/38400]	lr: 3.730e-06, eta: 0:59:57, time: 1.485, data_time: 0.804, memory: 25023, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 0.2303
2022-03-31 00:11:22,835 - depth - INFO - Iter [33700/38400]	lr: 3.653e-06, eta: 0:59:18, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.2175
2022-03-31 00:11:57,040 - depth - INFO - Iter [33750/38400]	lr: 3.576e-06, eta: 0:58:40, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0363, loss: 0.0363, grad_norm: 0.2315
2022-03-31 00:12:31,417 - depth - INFO - Iter [33800/38400]	lr: 3.501e-06, eta: 0:58:02, time: 0.688, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 0.2281
2022-03-31 00:13:05,726 - depth - INFO - Iter [33850/38400]	lr: 3.426e-06, eta: 0:57:23, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0363, loss: 0.0363, grad_norm: 0.2287
2022-03-31 00:13:39,889 - depth - INFO - Iter [33900/38400]	lr: 3.352e-06, eta: 0:56:45, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.2208
2022-03-31 00:14:14,397 - depth - INFO - Iter [33950/38400]	lr: 3.279e-06, eta: 0:56:07, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.2143
2022-03-31 00:14:48,903 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-31 00:14:48,903 - depth - INFO - Iter [34000/38400]	lr: 3.206e-06, eta: 0:55:29, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.2047
2022-03-31 00:15:23,136 - depth - INFO - Iter [34050/38400]	lr: 3.134e-06, eta: 0:54:50, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0360, loss: 0.0360, grad_norm: 0.2248
2022-03-31 00:15:57,371 - depth - INFO - Iter [34100/38400]	lr: 3.064e-06, eta: 0:54:12, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 0.2159
2022-03-31 00:16:31,838 - depth - INFO - Iter [34150/38400]	lr: 2.993e-06, eta: 0:53:34, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0360, loss: 0.0360, grad_norm: 0.2440
2022-03-31 00:17:06,402 - depth - INFO - Iter [34200/38400]	lr: 2.924e-06, eta: 0:52:56, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0363, loss: 0.0363, grad_norm: 0.2380
2022-03-31 00:17:40,763 - depth - INFO - Iter [34250/38400]	lr: 2.856e-06, eta: 0:52:17, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 0.2206
2022-03-31 00:18:15,161 - depth - INFO - Iter [34300/38400]	lr: 2.788e-06, eta: 0:51:39, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 0.1988
2022-03-31 00:18:49,417 - depth - INFO - Iter [34350/38400]	lr: 2.721e-06, eta: 0:51:01, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.2218
2022-03-31 00:19:23,746 - depth - INFO - Iter [34400/38400]	lr: 2.655e-06, eta: 0:50:23, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 0.2491
2022-03-31 00:20:03,758 - depth - INFO - Summary:
2022-03-31 00:20:03,759 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9744 | 0.9968 | 0.9993 |  0.0516 | 2.1255 | 0.0225 |  0.0789  | 7.1993 | 0.1543 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-31 00:20:17,206 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_34400.pth.
2022-03-31 00:20:17,207 - depth - INFO - Best abs_rel is 0.0516 at 34400 iter.
2022-03-31 00:20:17,207 - depth - INFO - Iter(val) [82]	a1: 0.9744, a2: 0.9968, a3: 0.9993, abs_rel: 0.05157040059566498, rmse: 2.125504732131958, log_10: 0.022545291110873222, rmse_log: 0.0788687914609909, silog: 7.1993, sq_rel: 0.15430161356925964
2022-03-31 00:20:51,739 - depth - INFO - Iter [34450/38400]	lr: 2.589e-06, eta: 0:49:51, time: 1.760, data_time: 1.076, memory: 25023, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.2187
2022-03-31 00:21:26,366 - depth - INFO - Iter [34500/38400]	lr: 2.525e-06, eta: 0:49:12, time: 0.693, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 0.2127
2022-03-31 00:22:00,902 - depth - INFO - Iter [34550/38400]	lr: 2.461e-06, eta: 0:48:34, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.2287
2022-03-31 00:22:35,308 - depth - INFO - Iter [34600/38400]	lr: 2.398e-06, eta: 0:47:56, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0360, loss: 0.0360, grad_norm: 0.2452
2022-03-31 00:23:09,883 - depth - INFO - Iter [34650/38400]	lr: 2.336e-06, eta: 0:47:18, time: 0.692, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 0.2149
2022-03-31 00:23:44,311 - depth - INFO - Iter [34700/38400]	lr: 2.275e-06, eta: 0:46:40, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 0.2010
2022-03-31 00:24:28,582 - depth - INFO - Iter [34750/38400]	lr: 2.214e-06, eta: 0:46:02, time: 0.886, data_time: 0.187, memory: 25023, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.1961
2022-03-31 00:25:02,816 - depth - INFO - Iter [34800/38400]	lr: 2.154e-06, eta: 0:45:24, time: 0.685, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.2112
2022-03-31 00:25:37,134 - depth - INFO - Iter [34850/38400]	lr: 2.095e-06, eta: 0:44:46, time: 0.686, data_time: 0.004, memory: 25023, decode.loss_depth: 0.0360, loss: 0.0360, grad_norm: 0.2277
2022-03-31 00:26:11,105 - depth - INFO - Iter [34900/38400]	lr: 2.037e-06, eta: 0:44:08, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.2200
2022-03-31 00:26:45,236 - depth - INFO - Iter [34950/38400]	lr: 1.980e-06, eta: 0:43:30, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.2009
2022-03-31 00:27:19,355 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-31 00:27:19,355 - depth - INFO - Iter [35000/38400]	lr: 1.923e-06, eta: 0:42:51, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.1943
2022-03-31 00:27:53,395 - depth - INFO - Iter [35050/38400]	lr: 1.867e-06, eta: 0:42:13, time: 0.681, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.2288
2022-03-31 00:28:27,833 - depth - INFO - Iter [35100/38400]	lr: 1.812e-06, eta: 0:41:35, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.2172
2022-03-31 00:29:01,897 - depth - INFO - Iter [35150/38400]	lr: 1.758e-06, eta: 0:40:57, time: 0.681, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0363, loss: 0.0363, grad_norm: 0.2127
2022-03-31 00:29:36,214 - depth - INFO - Saving checkpoint at 35200 iterations
2022-03-31 00:29:51,436 - depth - INFO - Iter [35200/38400]	lr: 1.705e-06, eta: 0:40:20, time: 0.991, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 0.2061
2022-03-31 00:30:31,129 - depth - INFO - Summary:
2022-03-31 00:30:31,130 - depth - INFO - 
+--------+--------+--------+---------+------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel | rmse | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+------+--------+----------+--------+--------+
| 0.9744 | 0.9969 | 0.9993 |  0.0517 | 2.13 | 0.0225 |  0.0789  | 7.2201 | 0.1551 |
+--------+--------+--------+---------+------+--------+----------+--------+--------+
2022-03-31 00:30:31,130 - depth - INFO - Iter(val) [82]	a1: 0.9744, a2: 0.9969, a3: 0.9993, abs_rel: 0.05167574808001518, rmse: 2.1300466060638428, log_10: 0.022541528567671776, rmse_log: 0.07894980907440186, silog: 7.2201, sq_rel: 0.15506131947040558
2022-03-31 00:31:05,523 - depth - INFO - Iter [35250/38400]	lr: 1.652e-06, eta: 0:39:46, time: 1.481, data_time: 0.798, memory: 25023, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.2230
2022-03-31 00:31:39,761 - depth - INFO - Iter [35300/38400]	lr: 1.600e-06, eta: 0:39:07, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.1972
2022-03-31 00:32:13,993 - depth - INFO - Iter [35350/38400]	lr: 1.550e-06, eta: 0:38:29, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 0.2014
2022-03-31 00:32:48,072 - depth - INFO - Iter [35400/38400]	lr: 1.499e-06, eta: 0:37:51, time: 0.681, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.2228
2022-03-31 00:33:22,511 - depth - INFO - Iter [35450/38400]	lr: 1.450e-06, eta: 0:37:13, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.2024
2022-03-31 00:33:56,892 - depth - INFO - Iter [35500/38400]	lr: 1.402e-06, eta: 0:36:35, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 0.2273
2022-03-31 00:34:31,179 - depth - INFO - Iter [35550/38400]	lr: 1.354e-06, eta: 0:35:57, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.2128
2022-03-31 00:35:05,494 - depth - INFO - Iter [35600/38400]	lr: 1.307e-06, eta: 0:35:18, time: 0.686, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.2188
2022-03-31 00:35:39,980 - depth - INFO - Iter [35650/38400]	lr: 1.261e-06, eta: 0:34:40, time: 0.690, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.2207
2022-03-31 00:36:14,496 - depth - INFO - Iter [35700/38400]	lr: 1.216e-06, eta: 0:34:02, time: 0.690, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.1818
2022-03-31 00:36:48,844 - depth - INFO - Iter [35750/38400]	lr: 1.171e-06, eta: 0:33:24, time: 0.687, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.1868
2022-03-31 00:37:23,384 - depth - INFO - Iter [35800/38400]	lr: 1.128e-06, eta: 0:32:46, time: 0.691, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 0.1925
2022-03-31 00:37:57,980 - depth - INFO - Iter [35850/38400]	lr: 1.085e-06, eta: 0:32:08, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.2138
2022-03-31 00:38:32,767 - depth - INFO - Iter [35900/38400]	lr: 1.043e-06, eta: 0:31:30, time: 0.696, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 0.1943
2022-03-31 00:39:07,228 - depth - INFO - Iter [35950/38400]	lr: 1.002e-06, eta: 0:30:52, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.2071
2022-03-31 00:39:41,487 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-31 00:39:41,487 - depth - INFO - Iter [36000/38400]	lr: 9.615e-07, eta: 0:30:14, time: 0.686, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.2211
2022-03-31 00:40:22,137 - depth - INFO - Summary:
2022-03-31 00:40:22,138 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9743 | 0.9969 | 0.9993 |  0.0516 | 2.1378 | 0.0226 |  0.079   | 7.2149 | 0.1556 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-31 00:40:22,138 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-31 00:40:22,139 - depth - INFO - Iter(val) [82]	a1: 0.9743, a2: 0.9969, a3: 0.9993, abs_rel: 0.05161430314183235, rmse: 2.137812614440918, log_10: 0.022562505677342415, rmse_log: 0.07904277741909027, silog: 7.2149, sq_rel: 0.15560781955718994
2022-03-31 00:40:56,739 - depth - INFO - Iter [36050/38400]	lr: 9.220e-07, eta: 0:29:39, time: 1.505, data_time: 0.818, memory: 25023, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.1875
2022-03-31 00:41:30,738 - depth - INFO - Iter [36100/38400]	lr: 8.833e-07, eta: 0:29:01, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.1925
2022-03-31 00:42:04,874 - depth - INFO - Iter [36150/38400]	lr: 8.455e-07, eta: 0:28:22, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.2050
2022-03-31 00:42:48,839 - depth - INFO - Iter [36200/38400]	lr: 8.084e-07, eta: 0:27:45, time: 0.879, data_time: 0.192, memory: 25023, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.2055
2022-03-31 00:43:23,584 - depth - INFO - Iter [36250/38400]	lr: 7.722e-07, eta: 0:27:07, time: 0.695, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.2311
2022-03-31 00:43:57,812 - depth - INFO - Iter [36300/38400]	lr: 7.368e-07, eta: 0:26:29, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.1944
2022-03-31 00:44:32,166 - depth - INFO - Iter [36350/38400]	lr: 7.022e-07, eta: 0:25:51, time: 0.687, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.2030
2022-03-31 00:45:06,450 - depth - INFO - Iter [36400/38400]	lr: 6.685e-07, eta: 0:25:13, time: 0.686, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 0.1930
2022-03-31 00:45:40,803 - depth - INFO - Iter [36450/38400]	lr: 6.356e-07, eta: 0:24:35, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0350, loss: 0.0350, grad_norm: 0.1962
2022-03-31 00:46:14,913 - depth - INFO - Iter [36500/38400]	lr: 6.035e-07, eta: 0:23:57, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.1878
2022-03-31 00:46:49,081 - depth - INFO - Iter [36550/38400]	lr: 5.722e-07, eta: 0:23:19, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.2071
2022-03-31 00:47:23,559 - depth - INFO - Iter [36600/38400]	lr: 5.418e-07, eta: 0:22:41, time: 0.689, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.1979
2022-03-31 00:47:57,902 - depth - INFO - Iter [36650/38400]	lr: 5.122e-07, eta: 0:22:03, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.2019
2022-03-31 00:48:31,854 - depth - INFO - Iter [36700/38400]	lr: 4.834e-07, eta: 0:21:25, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.1916
2022-03-31 00:49:05,861 - depth - INFO - Iter [36750/38400]	lr: 4.554e-07, eta: 0:20:47, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.1864
2022-03-31 00:49:40,644 - depth - INFO - Saving checkpoint at 36800 iterations
2022-03-31 00:49:56,321 - depth - INFO - Iter [36800/38400]	lr: 4.283e-07, eta: 0:20:10, time: 1.009, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.1924
2022-03-31 00:50:35,791 - depth - INFO - Summary:
2022-03-31 00:50:35,792 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9743 | 0.9968 | 0.9993 |  0.0517 | 2.1348 | 0.0226 |  0.0791  | 7.2129 | 0.1555 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-31 00:50:35,793 - depth - INFO - Iter(val) [82]	a1: 0.9743, a2: 0.9968, a3: 0.9993, abs_rel: 0.051687996834516525, rmse: 2.134833812713623, log_10: 0.02260359562933445, rmse_log: 0.07909364253282547, silog: 7.2129, sq_rel: 0.15545602142810822
2022-03-31 00:51:10,054 - depth - INFO - Iter [36850/38400]	lr: 4.020e-07, eta: 0:19:33, time: 1.474, data_time: 0.796, memory: 25023, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.2211
2022-03-31 00:51:44,200 - depth - INFO - Iter [36900/38400]	lr: 3.765e-07, eta: 0:18:55, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.2122
2022-03-31 00:52:18,736 - depth - INFO - Iter [36950/38400]	lr: 3.519e-07, eta: 0:18:17, time: 0.693, data_time: 0.008, memory: 25023, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 0.2015
2022-03-31 00:52:52,892 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-31 00:52:52,893 - depth - INFO - Iter [37000/38400]	lr: 3.281e-07, eta: 0:17:39, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 0.2056
2022-03-31 00:53:27,121 - depth - INFO - Iter [37050/38400]	lr: 3.051e-07, eta: 0:17:01, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.1959
2022-03-31 00:54:01,612 - depth - INFO - Iter [37100/38400]	lr: 2.830e-07, eta: 0:16:23, time: 0.690, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0360, loss: 0.0360, grad_norm: 0.1877
2022-03-31 00:54:36,196 - depth - INFO - Iter [37150/38400]	lr: 2.616e-07, eta: 0:15:45, time: 0.692, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.1935
2022-03-31 00:55:10,516 - depth - INFO - Iter [37200/38400]	lr: 2.412e-07, eta: 0:15:07, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.1931
2022-03-31 00:55:44,579 - depth - INFO - Iter [37250/38400]	lr: 2.215e-07, eta: 0:14:30, time: 0.682, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.2090
2022-03-31 00:56:19,012 - depth - INFO - Iter [37300/38400]	lr: 2.027e-07, eta: 0:13:52, time: 0.689, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.1920
2022-03-31 00:56:53,145 - depth - INFO - Iter [37350/38400]	lr: 1.847e-07, eta: 0:13:14, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0360, loss: 0.0360, grad_norm: 0.1982
2022-03-31 00:57:27,474 - depth - INFO - Iter [37400/38400]	lr: 1.676e-07, eta: 0:12:36, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 0.1901
2022-03-31 00:58:01,579 - depth - INFO - Iter [37450/38400]	lr: 1.513e-07, eta: 0:11:58, time: 0.682, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.2176
2022-03-31 00:58:35,812 - depth - INFO - Iter [37500/38400]	lr: 1.358e-07, eta: 0:11:20, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 0.1941
2022-03-31 00:59:10,139 - depth - INFO - Iter [37550/38400]	lr: 1.211e-07, eta: 0:10:42, time: 0.687, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.1984
2022-03-31 00:59:44,401 - depth - INFO - Iter [37600/38400]	lr: 1.073e-07, eta: 0:10:04, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.2079
2022-03-31 01:00:24,406 - depth - INFO - Summary:
2022-03-31 01:00:24,407 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9743 | 0.9968 | 0.9993 |  0.0517 | 2.1383 | 0.0226 |  0.0791  | 7.2172 | 0.1556 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-31 01:00:24,408 - depth - INFO - Iter(val) [82]	a1: 0.9743, a2: 0.9968, a3: 0.9993, abs_rel: 0.05165672302246094, rmse: 2.1382720470428467, log_10: 0.022594520822167397, rmse_log: 0.07911236584186554, silog: 7.2172, sq_rel: 0.15564709901809692
2022-03-31 01:01:07,708 - depth - INFO - Iter [37650/38400]	lr: 9.435e-08, eta: 0:09:27, time: 1.666, data_time: 0.978, memory: 25023, decode.loss_depth: 0.0349, loss: 0.0349, grad_norm: 0.1857
2022-03-31 01:01:42,058 - depth - INFO - Iter [37700/38400]	lr: 8.221e-08, eta: 0:08:49, time: 0.687, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.1968
2022-03-31 01:02:16,300 - depth - INFO - Iter [37750/38400]	lr: 7.090e-08, eta: 0:08:11, time: 0.685, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.1832
2022-03-31 01:02:50,452 - depth - INFO - Iter [37800/38400]	lr: 6.043e-08, eta: 0:07:34, time: 0.683, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.2057
2022-03-31 01:03:24,887 - depth - INFO - Iter [37850/38400]	lr: 5.079e-08, eta: 0:06:56, time: 0.688, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.1951
2022-03-31 01:03:58,923 - depth - INFO - Iter [37900/38400]	lr: 4.200e-08, eta: 0:06:18, time: 0.681, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.1895
2022-03-31 01:04:33,504 - depth - INFO - Iter [37950/38400]	lr: 3.403e-08, eta: 0:05:40, time: 0.691, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 0.1953
2022-03-31 01:05:07,507 - depth - INFO - Exp name: depthformer_swinl_22k_w7_kitti.py
2022-03-31 01:05:07,508 - depth - INFO - Iter [38000/38400]	lr: 2.691e-08, eta: 0:05:02, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.1943
2022-03-31 01:05:41,507 - depth - INFO - Iter [38050/38400]	lr: 2.061e-08, eta: 0:04:24, time: 0.680, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 0.1824
2022-03-31 01:06:15,415 - depth - INFO - Iter [38100/38400]	lr: 1.516e-08, eta: 0:03:46, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.2110
2022-03-31 01:06:49,365 - depth - INFO - Iter [38150/38400]	lr: 1.054e-08, eta: 0:03:09, time: 0.679, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.2004
2022-03-31 01:07:23,576 - depth - INFO - Iter [38200/38400]	lr: 6.761e-09, eta: 0:02:31, time: 0.684, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 0.1974
2022-03-31 01:07:57,591 - depth - INFO - Iter [38250/38400]	lr: 3.816e-09, eta: 0:01:53, time: 0.680, data_time: 0.007, memory: 25023, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 0.2081
2022-03-31 01:08:31,863 - depth - INFO - Iter [38300/38400]	lr: 1.708e-09, eta: 0:01:15, time: 0.686, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.2040
2022-03-31 01:09:05,764 - depth - INFO - Iter [38350/38400]	lr: 4.362e-10, eta: 0:00:37, time: 0.678, data_time: 0.005, memory: 25023, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.1939
2022-03-31 01:09:39,808 - depth - INFO - Saving checkpoint at 38400 iterations
2022-03-31 01:09:54,653 - depth - INFO - Iter [38400/38400]	lr: 1.167e-12, eta: 0:00:00, time: 0.978, data_time: 0.006, memory: 25023, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.1892
2022-03-31 01:10:34,469 - depth - INFO - Summary:
2022-03-31 01:10:34,469 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9743 | 0.9968 | 0.9993 |  0.0516 | 2.1328 | 0.0226 |  0.0791  | 7.2099 | 0.1555 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-31 01:10:34,470 - depth - INFO - Iter(val) [82]	a1: 0.9743, a2: 0.9968, a3: 0.9993, abs_rel: 0.05164171755313873, rmse: 2.132796049118042, log_10: 0.022586993873119354, rmse_log: 0.07906818389892578, silog: 7.2099, sq_rel: 0.15545181930065155
