2022-03-28 09:01:56,406 - depth - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.11 (default, Jul 27 2021, 14:32:16) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: Tesla V100-SXM2-32GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.1, V10.1.243
GCC: gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
PyTorch: 1.6.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.3
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.7.0
OpenCV: 4.5.5
MMCV: 1.3.13
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.1
Depth: 0.0.0+5253be7
------------------------------------------------------------

2022-03-28 09:01:56,407 - depth - INFO - Distributed training: True
2022-03-28 09:01:56,627 - depth - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
backbone_norm_cfg = dict(type='LN', requires_grad=True)
conv_stem_norm_cfg = dict(type='BN', requires_grad=True)
model = dict(
    type='DepthEncoderDecoder',
    pretrained='nfs/checkpoints/swin_large_patch4_window7_224_22k.pth',
    backbone=dict(
        type='DepthFormerSwin',
        pretrain_img_size=224,
        embed_dims=192,
        patch_size=4,
        window_size=7,
        mlp_ratio=4,
        depths=[2, 2, 18, 2],
        num_heads=[6, 12, 24, 48],
        strides=(4, 2, 2, 2),
        out_indices=(0, 1, 2, 3),
        qkv_bias=True,
        qk_scale=None,
        patch_norm=True,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.3,
        use_abs_pos_embed=False,
        act_cfg=dict(type='GELU'),
        norm_cfg=dict(type='LN', requires_grad=True),
        pretrain_style='official',
        conv_norm_cfg=dict(type='BN', requires_grad=True),
        depth=50,
        num_stages=0),
    decode_head=dict(
        type='DenseDepthHead',
        in_channels=[64, 192, 384, 768, 1536],
        up_sample_channels=[64, 192, 384, 768, 1536],
        channels=64,
        align_corners=True,
        loss_decode=dict(type='SigLoss', valid_mask=True, loss_weight=1.0),
        act_cfg=dict(type='LeakyReLU', inplace=True),
        min_depth=0.001,
        max_depth=10),
    train_cfg=dict(),
    test_cfg=dict(mode='whole'),
    neck=dict(
        type='HAHIHeteroNeck',
        positional_encoding=dict(type='SinePositionalEncoding', num_feats=256),
        in_channels=[64, 192, 384, 768, 1536],
        out_channels=[64, 192, 384, 768, 1536],
        embedding_dim=512,
        scales=[1, 1, 1, 1, 1]))
dataset_type = 'NYUDataset'
data_root = 'data/nyu/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (416, 544)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='DepthLoadAnnotations'),
    dict(type='NYUCrop', depth=True),
    dict(type='RandomRotate', prob=0.5, degree=2.5),
    dict(type='RandomFlip', prob=0.5),
    dict(type='RandomCrop', crop_size=(416, 544)),
    dict(
        type='ColorAug',
        prob=0.5,
        gamma_range=[0.9, 1.1],
        brightness_range=[0.75, 1.25],
        color_range=[0.9, 1.1]),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'depth_gt'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(480, 640),
        flip=True,
        flip_direction='horizontal',
        transforms=[
            dict(type='RandomFlip', direction='horizontal'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='NYUDataset',
        data_root='data/nyu/',
        depth_scale=1000,
        split='nyu_train.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='DepthLoadAnnotations'),
            dict(type='NYUCrop', depth=True),
            dict(type='RandomRotate', prob=0.5, degree=2.5),
            dict(type='RandomFlip', prob=0.5),
            dict(type='RandomCrop', crop_size=(416, 544)),
            dict(
                type='ColorAug',
                prob=0.5,
                gamma_range=[0.9, 1.1],
                brightness_range=[0.75, 1.25],
                color_range=[0.9, 1.1]),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'depth_gt'])
        ],
        garg_crop=False,
        eigen_crop=True,
        min_depth=0.001,
        max_depth=10),
    val=dict(
        type='NYUDataset',
        data_root='data/nyu/',
        depth_scale=1000,
        split='nyu_test.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(480, 640),
                flip=True,
                flip_direction='horizontal',
                transforms=[
                    dict(type='RandomFlip', direction='horizontal'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        garg_crop=False,
        eigen_crop=True,
        min_depth=0.001,
        max_depth=10),
    test=dict(
        type='NYUDataset',
        data_root='data/nyu/',
        depth_scale=1000,
        split='nyu_test.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(480, 640),
                flip=True,
                flip_direction='horizontal',
                transforms=[
                    dict(type='RandomFlip', direction='horizontal'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        garg_crop=False,
        eigen_crop=True,
        min_depth=0.001,
        max_depth=10))
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook', by_epoch=False),
        dict(type='TensorboardLoggerHook')
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
max_lr = 0.0001
optimizer = dict(
    type='AdamW',
    lr=0.0001,
    betas=(0.9, 0.999),
    weight_decay=0.01,
    paramwise_cfg=dict(
        custom_keys=dict(
            absolute_pos_embed=dict(decay_mult=0.0),
            relative_position_bias_table=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0))))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=12800,
    warmup_ratio=0.001,
    min_lr_ratio=1e-08,
    by_epoch=False)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
runner = dict(type='IterBasedRunner', max_iters=38400)
checkpoint_config = dict(by_epoch=False, max_keep_ckpts=2, interval=1600)
evaluation = dict(
    by_epoch=False,
    start=0,
    interval=800,
    pre_eval=True,
    rule='less',
    save_best='abs_rel',
    greater_keys=('a1', 'a2', 'a3'),
    less_keys=('abs_rel', 'rmse'))
work_dir = 'nfs/saves/depthformer/depthformer_swinl_22k_nyu_repro'
gpu_ids = range(0, 1)

2022-03-28 09:01:58,839 - depth - INFO - Use load_from_local loader
Name of parameter - Initialization information

backbone.patch_embed.projection.weight - torch.Size([192, 3, 4, 4]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.patch_embed.projection.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.patch_embed.norm.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.patch_embed.norm.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.norm1.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.norm1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.attn.w_msa.qkv.bias - torch.Size([576]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.attn.w_msa.proj.weight - torch.Size([192, 192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.attn.w_msa.proj.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.norm2.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.norm2.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.ffn.layers.0.0.weight - torch.Size([768, 192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.ffn.layers.0.0.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.ffn.layers.1.weight - torch.Size([192, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.0.ffn.layers.1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.norm1.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.norm1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.attn.w_msa.qkv.bias - torch.Size([576]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.attn.w_msa.proj.weight - torch.Size([192, 192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.attn.w_msa.proj.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.norm2.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.norm2.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.ffn.layers.0.0.weight - torch.Size([768, 192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.ffn.layers.0.0.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.ffn.layers.1.weight - torch.Size([192, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.blocks.1.ffn.layers.1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.downsample.norm.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.downsample.norm.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.0.downsample.reduction.weight - torch.Size([384, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.0.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.blocks.1.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.downsample.norm.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.downsample.norm.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.1.downsample.reduction.weight - torch.Size([768, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.0.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.1.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.2.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.3.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.4.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.5.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.6.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.7.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.8.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.9.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.10.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.11.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.12.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.13.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.14.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.15.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.16.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.blocks.17.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.downsample.norm.weight - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.downsample.norm.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.2.downsample.reduction.weight - torch.Size([1536, 3072]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.norm1.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.norm1.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 48]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.attn.w_msa.qkv.weight - torch.Size([4608, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.attn.w_msa.qkv.bias - torch.Size([4608]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.attn.w_msa.proj.weight - torch.Size([1536, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.attn.w_msa.proj.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.norm2.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.norm2.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.ffn.layers.0.0.weight - torch.Size([6144, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.ffn.layers.0.0.bias - torch.Size([6144]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.ffn.layers.1.weight - torch.Size([1536, 6144]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.0.ffn.layers.1.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.norm1.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.norm1.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 48]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.attn.w_msa.qkv.weight - torch.Size([4608, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.attn.w_msa.qkv.bias - torch.Size([4608]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.attn.w_msa.proj.weight - torch.Size([1536, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.attn.w_msa.proj.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.norm2.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.norm2.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.ffn.layers.0.0.weight - torch.Size([6144, 1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.ffn.layers.0.0.bias - torch.Size([6144]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.ffn.layers.1.weight - torch.Size([1536, 6144]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.stages.3.blocks.1.ffn.layers.1.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in DepthFormerSwin  

backbone.norm0.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm0.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm3.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm3.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_depth.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_depth.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.0.conv.weight - torch.Size([1536, 1536, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.0.conv.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.1.convA.conv.weight - torch.Size([768, 2304, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.1.convA.conv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.1.convB.conv.weight - torch.Size([768, 768, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.1.convB.conv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.2.convA.conv.weight - torch.Size([384, 1152, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.2.convA.conv.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.2.convB.conv.weight - torch.Size([384, 384, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.2.convB.conv.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.3.convA.conv.weight - torch.Size([192, 576, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.3.convA.conv.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.3.convB.conv.weight - torch.Size([192, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.3.convB.conv.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.4.convA.conv.weight - torch.Size([64, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.4.convA.conv.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.4.convB.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_list.4.convB.conv.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.level_embed - torch.Size([4, 512]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.lateral_convs.0.conv.weight - torch.Size([64, 64, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.lateral_convs.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.1.conv.weight - torch.Size([192, 192, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.lateral_convs.1.bn.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.1.bn.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.2.conv.weight - torch.Size([384, 384, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.lateral_convs.2.bn.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.2.bn.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.3.conv.weight - torch.Size([768, 768, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.lateral_convs.3.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.3.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.4.conv.weight - torch.Size([1536, 1536, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.lateral_convs.4.bn.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.lateral_convs.4.bn.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_proj.0.conv.weight - torch.Size([512, 192, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.trans_proj.0.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_proj.0.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_proj.1.conv.weight - torch.Size([512, 384, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.trans_proj.1.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_proj.1.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_proj.2.conv.weight - torch.Size([512, 768, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.trans_proj.2.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_proj.2.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_proj.3.conv.weight - torch.Size([512, 1536, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.trans_proj.3.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_proj.3.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_fusion.0.conv.weight - torch.Size([192, 704, 3, 3]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.trans_fusion.0.bn.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_fusion.0.bn.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_fusion.1.conv.weight - torch.Size([384, 896, 3, 3]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.trans_fusion.1.bn.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_fusion.1.bn.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_fusion.2.conv.weight - torch.Size([768, 1280, 3, 3]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.trans_fusion.2.bn.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_fusion.2.bn.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_fusion.3.conv.weight - torch.Size([1536, 2048, 3, 3]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.trans_fusion.3.bn.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.trans_fusion.3.bn.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.conv_proj.0.conv.weight - torch.Size([512, 64, 1, 1]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.conv_proj.0.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.conv_proj.0.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.conv_fusion.0.conv.weight - torch.Size([64, 576, 3, 3]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.conv_fusion.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.conv_fusion.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.reference_points.weight - torch.Size([2, 512]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.multi_att.sampling_offsets.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.multi_att.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.multi_att.attention_weights.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.multi_att.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.multi_att.value_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.multi_att.value_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.multi_att.output_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.multi_att.output_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.self_attn.sampling_offsets.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.self_attn.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.self_attn.attention_weights.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.self_attn.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.self_attn.value_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.self_attn.value_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

neck.self_attn.output_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in HAHIHeteroNeck  

neck.self_attn.output_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  
2022-03-28 09:02:01,846 - depth - INFO - DepthEncoderDecoder(
  (backbone): DepthFormerSwin(
    (patch_embed): PatchEmbed(
      (projection): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    )
    (drop_after_pos): Dropout(p=0.0, inplace=False)
    (stages): ModuleList(
      (0): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=192, out_features=768, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=768, out_features=192, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=192, out_features=768, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=768, out_features=192, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (downsample): PatchMerging(
          (sampler): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=768, out_features=384, bias=False)
        )
      )
      (1): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=384, out_features=1536, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=1536, out_features=384, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=384, out_features=1536, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=1536, out_features=384, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (downsample): PatchMerging(
          (sampler): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
        )
      )
      (2): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (3): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (4): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (5): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (6): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (7): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (8): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (9): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (10): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (11): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (12): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (13): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (14): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (15): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (16): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (17): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (downsample): PatchMerging(
          (sampler): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)
          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=3072, out_features=1536, bias=False)
        )
      )
      (3): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=1536, out_features=4608, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1536, out_features=1536, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1536, out_features=6144, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=6144, out_features=1536, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=1536, out_features=4608, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1536, out_features=1536, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1536, out_features=6144, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=6144, out_features=1536, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
      )
    )
    (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (_conv_stem_relu): ReLU(inplace=True)
    (_conv_stem_maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (decode_head): DenseDepthHead(
    align_corners=True
    (loss_decode): SigLoss()
    (conv_depth): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu): ReLU()
    (sigmoid): Sigmoid()
    (conv_list): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): UpSample(
        (convA): ConvModule(
          (conv): Conv2d(2304, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (convB): ConvModule(
          (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (2): UpSample(
        (convA): ConvModule(
          (conv): Conv2d(1152, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (convB): ConvModule(
          (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (3): UpSample(
        (convA): ConvModule(
          (conv): Conv2d(576, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (convB): ConvModule(
          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (4): UpSample(
        (convA): ConvModule(
          (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (convB): ConvModule(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
  )
  (neck): HAHIHeteroNeck(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (3): ConvModule(
        (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (4): ConvModule(
        (conv): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (trans_proj): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (3): ConvModule(
        (conv): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (trans_fusion): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(704, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(896, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(1280, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (3): ConvModule(
        (conv): Conv2d(2048, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (conv_proj): Sequential(
      (0): ConvModule(
        (conv): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (conv_fusion): Sequential(
      (0): ConvModule(
        (conv): Conv2d(576, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (trans_positional_encoding): SinePositionalEncoding(num_feats=256, temperature=10000, normalize=False, scale=6.283185307179586, eps=1e-06)
    (conv_positional_encoding): SinePositionalEncoding(num_feats=256, temperature=10000, normalize=False, scale=6.283185307179586, eps=1e-06)
    (reference_points): Linear(in_features=512, out_features=2, bias=True)
    (multi_att): MultiScaleDeformableAttention(
      (dropout): Dropout(p=0.1, inplace=False)
      (sampling_offsets): Linear(in_features=512, out_features=512, bias=True)
      (attention_weights): Linear(in_features=512, out_features=256, bias=True)
      (value_proj): Linear(in_features=512, out_features=512, bias=True)
      (output_proj): Linear(in_features=512, out_features=512, bias=True)
    )
    (self_attn): MultiScaleDeformableAttention(
      (dropout): Dropout(p=0.1, inplace=False)
      (sampling_offsets): Linear(in_features=512, out_features=512, bias=True)
      (attention_weights): Linear(in_features=512, out_features=256, bias=True)
      (value_proj): Linear(in_features=512, out_features=512, bias=True)
      (output_proj): Linear(in_features=512, out_features=512, bias=True)
    )
  )
)
2022-03-28 09:02:01,997 - depth - INFO - Loaded 24231 images. Totally 0 invalid pairs are filtered
2022-03-28 09:02:07,846 - depth - INFO - Loaded 654 images. Totally 0 invalid pairs are filtered
2022-03-28 09:02:07,846 - depth - INFO - Start running, host: root@mono3d-da-2-master-0, work_dir: /nfs/lizhenyu2/codes/Monocular-Depth-Estimation-Toolbox/nfs/saves/depthformer/depthformer_swinl_22k_nyu_repro
2022-03-28 09:02:07,847 - depth - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2022-03-28 09:02:07,847 - depth - INFO - workflow: [('train', 1)], max: 38400 iters
2022-03-28 09:02:52,517 - depth - INFO - Iter [50/38400]	lr: 4.824e-07, eta: 7:49:00, time: 0.734, data_time: 0.007, memory: 24409, decode.loss_depth: 0.7066, loss: 0.7066, grad_norm: 39.9235
2022-03-28 09:03:24,690 - depth - INFO - Iter [100/38400]	lr: 8.726e-07, eta: 7:19:24, time: 0.643, data_time: 0.004, memory: 24409, decode.loss_depth: 0.5142, loss: 0.5142, grad_norm: 5.8809
2022-03-28 09:03:57,326 - depth - INFO - Iter [150/38400]	lr: 1.263e-06, eta: 7:11:21, time: 0.653, data_time: 0.006, memory: 24409, decode.loss_depth: 0.4489, loss: 0.4489, grad_norm: 4.6621
2022-03-28 09:04:29,636 - depth - INFO - Iter [200/38400]	lr: 1.653e-06, eta: 7:05:54, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.4037, loss: 0.4037, grad_norm: 4.1781
2022-03-28 09:05:01,940 - depth - INFO - Iter [250/38400]	lr: 2.043e-06, eta: 7:02:26, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.3666, loss: 0.3666, grad_norm: 4.1897
2022-03-28 09:05:34,172 - depth - INFO - Iter [300/38400]	lr: 2.433e-06, eta: 6:59:50, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.3371, loss: 0.3371, grad_norm: 3.8061
2022-03-28 09:06:06,350 - depth - INFO - Iter [350/38400]	lr: 2.823e-06, eta: 6:57:41, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.3187, loss: 0.3187, grad_norm: 3.6144
2022-03-28 09:06:38,622 - depth - INFO - Iter [400/38400]	lr: 3.213e-06, eta: 6:56:05, time: 0.645, data_time: 0.004, memory: 24409, decode.loss_depth: 0.2927, loss: 0.2927, grad_norm: 3.6046
2022-03-28 09:07:10,805 - depth - INFO - Iter [450/38400]	lr: 3.603e-06, eta: 6:54:36, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.2832, loss: 0.2832, grad_norm: 3.3872
2022-03-28 09:07:42,986 - depth - INFO - Iter [500/38400]	lr: 3.993e-06, eta: 6:53:16, time: 0.643, data_time: 0.005, memory: 24409, decode.loss_depth: 0.2757, loss: 0.2757, grad_norm: 3.4595
2022-03-28 09:08:15,370 - depth - INFO - Iter [550/38400]	lr: 4.383e-06, eta: 6:52:22, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.2656, loss: 0.2656, grad_norm: 3.2122
2022-03-28 09:08:47,596 - depth - INFO - Iter [600/38400]	lr: 4.772e-06, eta: 6:51:19, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.2536, loss: 0.2536, grad_norm: 3.1981
2022-03-28 09:09:19,705 - depth - INFO - Iter [650/38400]	lr: 5.162e-06, eta: 6:50:16, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.2491, loss: 0.2491, grad_norm: 3.3580
2022-03-28 09:09:51,756 - depth - INFO - Iter [700/38400]	lr: 5.551e-06, eta: 6:49:13, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.2382, loss: 0.2382, grad_norm: 3.9780
2022-03-28 09:10:24,207 - depth - INFO - Iter [750/38400]	lr: 5.940e-06, eta: 6:48:34, time: 0.649, data_time: 0.005, memory: 24409, decode.loss_depth: 0.2295, loss: 0.2295, grad_norm: 3.5889
2022-03-28 09:10:56,588 - depth - INFO - Iter [800/38400]	lr: 6.329e-06, eta: 6:47:55, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.2297, loss: 0.2297, grad_norm: 3.6201
2022-03-28 09:11:30,875 - depth - INFO - Summary:
2022-03-28 09:11:30,876 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.7206 | 0.9498 | 0.9913 |  0.1892 | 0.5833 | 0.074  |  0.2124  | 17.216 | 0.1489 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 09:11:41,327 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_800.pth.
2022-03-28 09:11:41,328 - depth - INFO - Best abs_rel is 0.1892 at 800 iter.
2022-03-28 09:11:41,329 - depth - INFO - Iter(val) [82]	a1: 0.7206, a2: 0.9498, a3: 0.9913, abs_rel: 0.18922019004821777, rmse: 0.5833306908607483, log_10: 0.07404902577400208, rmse_log: 0.21239231526851654, silog: 17.2160, sq_rel: 0.14893531799316406
2022-03-28 09:12:13,515 - depth - INFO - Iter [850/38400]	lr: 6.718e-06, eta: 7:20:02, time: 1.538, data_time: 0.900, memory: 24409, decode.loss_depth: 0.2222, loss: 0.2222, grad_norm: 3.6791
2022-03-28 09:12:45,752 - depth - INFO - Iter [900/38400]	lr: 7.107e-06, eta: 7:17:25, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.2173, loss: 0.2173, grad_norm: 3.4921
2022-03-28 09:13:17,908 - depth - INFO - Iter [950/38400]	lr: 7.495e-06, eta: 7:14:59, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.2166, loss: 0.2166, grad_norm: 3.7309
2022-03-28 09:13:50,028 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 09:13:50,029 - depth - INFO - Iter [1000/38400]	lr: 7.884e-06, eta: 7:12:43, time: 0.643, data_time: 0.005, memory: 24409, decode.loss_depth: 0.2109, loss: 0.2109, grad_norm: 3.3529
2022-03-28 09:14:22,260 - depth - INFO - Iter [1050/38400]	lr: 8.272e-06, eta: 7:10:39, time: 0.644, data_time: 0.004, memory: 24409, decode.loss_depth: 0.2088, loss: 0.2088, grad_norm: 3.3408
2022-03-28 09:14:54,587 - depth - INFO - Iter [1100/38400]	lr: 8.660e-06, eta: 7:08:47, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.2046, loss: 0.2046, grad_norm: 3.2852
2022-03-28 09:15:26,549 - depth - INFO - Iter [1150/38400]	lr: 9.048e-06, eta: 7:06:52, time: 0.640, data_time: 0.005, memory: 24409, decode.loss_depth: 0.2030, loss: 0.2030, grad_norm: 3.7332
2022-03-28 09:15:58,645 - depth - INFO - Iter [1200/38400]	lr: 9.435e-06, eta: 7:05:06, time: 0.642, data_time: 0.004, memory: 24409, decode.loss_depth: 0.1965, loss: 0.1965, grad_norm: 3.9008
2022-03-28 09:16:31,117 - depth - INFO - Iter [1250/38400]	lr: 9.822e-06, eta: 7:03:38, time: 0.649, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1965, loss: 0.1965, grad_norm: 3.6482
2022-03-28 09:17:03,157 - depth - INFO - Iter [1300/38400]	lr: 1.021e-05, eta: 7:02:02, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1919, loss: 0.1919, grad_norm: 3.9513
2022-03-28 09:17:35,390 - depth - INFO - Iter [1350/38400]	lr: 1.060e-05, eta: 7:00:36, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1934, loss: 0.1934, grad_norm: 3.3359
2022-03-28 09:18:07,707 - depth - INFO - Iter [1400/38400]	lr: 1.098e-05, eta: 6:59:16, time: 0.647, data_time: 0.004, memory: 24409, decode.loss_depth: 0.1886, loss: 0.1886, grad_norm: 3.6112
2022-03-28 09:18:39,921 - depth - INFO - Iter [1450/38400]	lr: 1.137e-05, eta: 6:57:57, time: 0.644, data_time: 0.004, memory: 24409, decode.loss_depth: 0.1862, loss: 0.1862, grad_norm: 4.0407
2022-03-28 09:19:12,177 - depth - INFO - Iter [1500/38400]	lr: 1.175e-05, eta: 6:56:41, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1811, loss: 0.1811, grad_norm: 4.2475
2022-03-28 09:19:53,947 - depth - INFO - Iter [1550/38400]	lr: 1.214e-05, eta: 6:59:15, time: 0.835, data_time: 0.195, memory: 24409, decode.loss_depth: 0.1781, loss: 0.1781, grad_norm: 3.9361
2022-03-28 09:20:26,182 - depth - INFO - Saving checkpoint at 1600 iterations
2022-03-28 09:20:38,656 - depth - INFO - Iter [1600/38400]	lr: 1.253e-05, eta: 7:02:44, time: 0.895, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1767, loss: 0.1767, grad_norm: 2.8369
2022-03-28 09:21:11,166 - depth - INFO - Summary:
2022-03-28 09:21:11,167 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8228 | 0.9782 | 0.9967 |  0.1377 | 0.4661 | 0.0581 |  0.1687  | 14.2587 | 0.0857 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-03-28 09:21:23,289 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_1600.pth.
2022-03-28 09:21:23,290 - depth - INFO - Best abs_rel is 0.1377 at 1600 iter.
2022-03-28 09:21:23,290 - depth - INFO - Iter(val) [82]	a1: 0.8228, a2: 0.9782, a3: 0.9967, abs_rel: 0.1377364993095398, rmse: 0.4660695493221283, log_10: 0.058122336864471436, rmse_log: 0.16873423755168915, silog: 14.2587, sq_rel: 0.08574536442756653
2022-03-28 09:21:55,457 - depth - INFO - Iter [1650/38400]	lr: 1.291e-05, eta: 7:17:53, time: 1.536, data_time: 0.898, memory: 24409, decode.loss_depth: 0.1727, loss: 0.1727, grad_norm: 3.7600
2022-03-28 09:22:27,807 - depth - INFO - Iter [1700/38400]	lr: 1.330e-05, eta: 7:16:03, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1687, loss: 0.1687, grad_norm: 3.8466
2022-03-28 09:23:00,065 - depth - INFO - Iter [1750/38400]	lr: 1.368e-05, eta: 7:14:17, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1747, loss: 0.1747, grad_norm: 3.7085
2022-03-28 09:23:32,296 - depth - INFO - Iter [1800/38400]	lr: 1.406e-05, eta: 7:12:34, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1680, loss: 0.1680, grad_norm: 3.7759
2022-03-28 09:24:04,510 - depth - INFO - Iter [1850/38400]	lr: 1.445e-05, eta: 7:10:54, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1707, loss: 0.1707, grad_norm: 3.6533
2022-03-28 09:24:36,803 - depth - INFO - Iter [1900/38400]	lr: 1.483e-05, eta: 7:09:20, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1654, loss: 0.1654, grad_norm: 3.8040
2022-03-28 09:25:09,172 - depth - INFO - Iter [1950/38400]	lr: 1.521e-05, eta: 7:07:50, time: 0.647, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1569, loss: 0.1569, grad_norm: 3.5529
2022-03-28 09:25:41,436 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 09:25:41,436 - depth - INFO - Iter [2000/38400]	lr: 1.560e-05, eta: 7:06:21, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1627, loss: 0.1627, grad_norm: 3.2030
2022-03-28 09:26:13,905 - depth - INFO - Iter [2050/38400]	lr: 1.598e-05, eta: 7:04:58, time: 0.649, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1597, loss: 0.1597, grad_norm: 3.7642
2022-03-28 09:26:46,242 - depth - INFO - Iter [2100/38400]	lr: 1.636e-05, eta: 7:03:36, time: 0.647, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1561, loss: 0.1561, grad_norm: 3.3360
2022-03-28 09:27:18,505 - depth - INFO - Iter [2150/38400]	lr: 1.674e-05, eta: 7:02:15, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1527, loss: 0.1527, grad_norm: 3.5589
2022-03-28 09:27:51,042 - depth - INFO - Iter [2200/38400]	lr: 1.712e-05, eta: 7:01:00, time: 0.651, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1522, loss: 0.1522, grad_norm: 3.1365
2022-03-28 09:28:23,479 - depth - INFO - Iter [2250/38400]	lr: 1.750e-05, eta: 6:59:46, time: 0.649, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1518, loss: 0.1518, grad_norm: 3.1859
2022-03-28 09:28:55,656 - depth - INFO - Iter [2300/38400]	lr: 1.788e-05, eta: 6:58:29, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1494, loss: 0.1494, grad_norm: 3.0262
2022-03-28 09:29:28,086 - depth - INFO - Iter [2350/38400]	lr: 1.826e-05, eta: 6:57:19, time: 0.649, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1472, loss: 0.1472, grad_norm: 3.2792
2022-03-28 09:30:00,111 - depth - INFO - Iter [2400/38400]	lr: 1.864e-05, eta: 6:56:03, time: 0.640, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1474, loss: 0.1474, grad_norm: 3.2655
2022-03-28 09:30:32,769 - depth - INFO - Summary:
2022-03-28 09:30:32,771 - depth - INFO - 
+--------+--------+--------+---------+-------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+-------+--------+----------+---------+--------+
| 0.8118 | 0.9779 | 0.9967 |  0.1515 | 0.521 | 0.0603 |  0.1721  | 12.9859 | 0.1105 |
+--------+--------+--------+---------+-------+--------+----------+---------+--------+
2022-03-28 09:30:32,772 - depth - INFO - Iter(val) [82]	a1: 0.8118, a2: 0.9779, a3: 0.9967, abs_rel: 0.15150843560695648, rmse: 0.520954966545105, log_10: 0.0602974072098732, rmse_log: 0.17210644483566284, silog: 12.9859, sq_rel: 0.1104608103632927
2022-03-28 09:31:05,213 - depth - INFO - Iter [2450/38400]	lr: 1.902e-05, eta: 7:02:55, time: 1.302, data_time: 0.659, memory: 24409, decode.loss_depth: 0.1448, loss: 0.1448, grad_norm: 3.6825
2022-03-28 09:31:37,547 - depth - INFO - Iter [2500/38400]	lr: 1.940e-05, eta: 7:01:37, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1456, loss: 0.1456, grad_norm: 3.2276
2022-03-28 09:32:09,819 - depth - INFO - Iter [2550/38400]	lr: 1.978e-05, eta: 7:00:20, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1433, loss: 0.1433, grad_norm: 3.0271
2022-03-28 09:32:42,137 - depth - INFO - Iter [2600/38400]	lr: 2.015e-05, eta: 6:59:06, time: 0.647, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1374, loss: 0.1374, grad_norm: 3.8776
2022-03-28 09:33:14,228 - depth - INFO - Iter [2650/38400]	lr: 2.053e-05, eta: 6:57:50, time: 0.642, data_time: 0.004, memory: 24409, decode.loss_depth: 0.1414, loss: 0.1414, grad_norm: 3.2877
2022-03-28 09:33:46,487 - depth - INFO - Iter [2700/38400]	lr: 2.091e-05, eta: 6:56:38, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1394, loss: 0.1394, grad_norm: 3.3629
2022-03-28 09:34:18,642 - depth - INFO - Iter [2750/38400]	lr: 2.128e-05, eta: 6:55:26, time: 0.643, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1352, loss: 0.1352, grad_norm: 2.9641
2022-03-28 09:34:51,187 - depth - INFO - Iter [2800/38400]	lr: 2.166e-05, eta: 6:54:20, time: 0.651, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1329, loss: 0.1329, grad_norm: 3.0219
2022-03-28 09:35:23,430 - depth - INFO - Iter [2850/38400]	lr: 2.203e-05, eta: 6:53:12, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1331, loss: 0.1331, grad_norm: 2.9007
2022-03-28 09:35:55,662 - depth - INFO - Iter [2900/38400]	lr: 2.241e-05, eta: 6:52:04, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1354, loss: 0.1354, grad_norm: 3.0003
2022-03-28 09:36:27,852 - depth - INFO - Iter [2950/38400]	lr: 2.278e-05, eta: 6:50:58, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1311, loss: 0.1311, grad_norm: 3.3188
2022-03-28 09:37:00,030 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 09:37:00,031 - depth - INFO - Iter [3000/38400]	lr: 2.315e-05, eta: 6:49:53, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1314, loss: 0.1314, grad_norm: 3.3407
2022-03-28 09:37:42,494 - depth - INFO - Iter [3050/38400]	lr: 2.353e-05, eta: 6:50:47, time: 0.849, data_time: 0.207, memory: 24409, decode.loss_depth: 0.1276, loss: 0.1276, grad_norm: 2.8407
2022-03-28 09:38:14,942 - depth - INFO - Iter [3100/38400]	lr: 2.390e-05, eta: 6:49:45, time: 0.650, data_time: 0.007, memory: 24409, decode.loss_depth: 0.1300, loss: 0.1300, grad_norm: 2.7122
2022-03-28 09:38:47,354 - depth - INFO - Iter [3150/38400]	lr: 2.427e-05, eta: 6:48:43, time: 0.648, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1279, loss: 0.1279, grad_norm: 3.1247
2022-03-28 09:39:19,667 - depth - INFO - Saving checkpoint at 3200 iterations
2022-03-28 09:39:32,153 - depth - INFO - Iter [3200/38400]	lr: 2.464e-05, eta: 6:49:59, time: 0.896, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1245, loss: 0.1245, grad_norm: 3.2850
2022-03-28 09:40:04,358 - depth - INFO - Summary:
2022-03-28 09:40:04,359 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8521 | 0.9839 | 0.9975 |  0.1345 | 0.4606 | 0.0551 |  0.157   | 12.3063 | 0.0837 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-03-28 09:40:15,956 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_3200.pth.
2022-03-28 09:40:15,957 - depth - INFO - Best abs_rel is 0.1345 at 3200 iter.
2022-03-28 09:40:15,957 - depth - INFO - Iter(val) [82]	a1: 0.8521, a2: 0.9839, a3: 0.9975, abs_rel: 0.13451872766017914, rmse: 0.4606108069419861, log_10: 0.055098000913858414, rmse_log: 0.15700238943099976, silog: 12.3063, sq_rel: 0.08374162763357162
2022-03-28 09:40:48,204 - depth - INFO - Iter [3250/38400]	lr: 2.501e-05, eta: 6:56:48, time: 1.521, data_time: 0.881, memory: 24409, decode.loss_depth: 0.1240, loss: 0.1240, grad_norm: 2.6707
2022-03-28 09:41:20,577 - depth - INFO - Iter [3300/38400]	lr: 2.538e-05, eta: 6:55:39, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1292, loss: 0.1292, grad_norm: 3.5067
2022-03-28 09:41:52,783 - depth - INFO - Iter [3350/38400]	lr: 2.575e-05, eta: 6:54:28, time: 0.644, data_time: 0.007, memory: 24409, decode.loss_depth: 0.1232, loss: 0.1232, grad_norm: 2.7559
2022-03-28 09:42:25,106 - depth - INFO - Iter [3400/38400]	lr: 2.612e-05, eta: 6:53:21, time: 0.647, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1237, loss: 0.1237, grad_norm: 2.6357
2022-03-28 09:42:57,305 - depth - INFO - Iter [3450/38400]	lr: 2.648e-05, eta: 6:52:12, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1230, loss: 0.1230, grad_norm: 2.9718
2022-03-28 09:43:29,671 - depth - INFO - Iter [3500/38400]	lr: 2.685e-05, eta: 6:51:07, time: 0.648, data_time: 0.007, memory: 24409, decode.loss_depth: 0.1228, loss: 0.1228, grad_norm: 3.1164
2022-03-28 09:44:01,932 - depth - INFO - Iter [3550/38400]	lr: 2.722e-05, eta: 6:50:01, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1188, loss: 0.1188, grad_norm: 2.3682
2022-03-28 09:44:34,301 - depth - INFO - Iter [3600/38400]	lr: 2.758e-05, eta: 6:48:58, time: 0.647, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1194, loss: 0.1194, grad_norm: 2.5025
2022-03-28 09:45:06,687 - depth - INFO - Iter [3650/38400]	lr: 2.795e-05, eta: 6:47:55, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1161, loss: 0.1161, grad_norm: 2.8417
2022-03-28 09:45:39,002 - depth - INFO - Iter [3700/38400]	lr: 2.831e-05, eta: 6:46:52, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1186, loss: 0.1186, grad_norm: 2.4780
2022-03-28 09:46:11,159 - depth - INFO - Iter [3750/38400]	lr: 2.867e-05, eta: 6:45:50, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1137, loss: 0.1137, grad_norm: 2.8159
2022-03-28 09:46:43,534 - depth - INFO - Iter [3800/38400]	lr: 2.904e-05, eta: 6:44:49, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1181, loss: 0.1181, grad_norm: 2.2588
2022-03-28 09:47:15,579 - depth - INFO - Iter [3850/38400]	lr: 2.940e-05, eta: 6:43:47, time: 0.641, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1156, loss: 0.1156, grad_norm: 2.5572
2022-03-28 09:47:47,995 - depth - INFO - Iter [3900/38400]	lr: 2.976e-05, eta: 6:42:48, time: 0.648, data_time: 0.007, memory: 24409, decode.loss_depth: 0.1125, loss: 0.1125, grad_norm: 2.0157
2022-03-28 09:48:20,204 - depth - INFO - Iter [3950/38400]	lr: 3.012e-05, eta: 6:41:49, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1150, loss: 0.1150, grad_norm: 2.8085
2022-03-28 09:48:52,663 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 09:48:52,663 - depth - INFO - Iter [4000/38400]	lr: 3.048e-05, eta: 6:40:52, time: 0.649, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1138, loss: 0.1138, grad_norm: 3.1319
2022-03-28 09:49:25,927 - depth - INFO - Summary:
2022-03-28 09:49:25,928 - depth - INFO - 
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1  |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.862 | 0.9843 | 0.9977 |  0.1317 | 0.4372 | 0.0534 |  0.1528  | 11.7468 | 0.0784 |
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-03-28 09:49:36,934 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_4000.pth.
2022-03-28 09:49:36,934 - depth - INFO - Best abs_rel is 0.1317 at 4000 iter.
2022-03-28 09:49:36,935 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 09:49:36,935 - depth - INFO - Iter(val) [82]	a1: 0.8620, a2: 0.9843, a3: 0.9977, abs_rel: 0.1317119598388672, rmse: 0.43722882866859436, log_10: 0.0534282885491848, rmse_log: 0.152770534157753, silog: 11.7468, sq_rel: 0.07840733230113983
2022-03-28 09:50:09,246 - depth - INFO - Iter [4050/38400]	lr: 3.084e-05, eta: 6:46:10, time: 1.531, data_time: 0.892, memory: 24409, decode.loss_depth: 0.1100, loss: 0.1100, grad_norm: 2.2225
2022-03-28 09:50:41,556 - depth - INFO - Iter [4100/38400]	lr: 3.120e-05, eta: 6:45:08, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1118, loss: 0.1118, grad_norm: 2.9515
2022-03-28 09:51:13,752 - depth - INFO - Iter [4150/38400]	lr: 3.155e-05, eta: 6:44:06, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1115, loss: 0.1115, grad_norm: 2.3302
2022-03-28 09:51:46,014 - depth - INFO - Iter [4200/38400]	lr: 3.191e-05, eta: 6:43:05, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1123, loss: 0.1123, grad_norm: 2.3277
2022-03-28 09:52:18,367 - depth - INFO - Iter [4250/38400]	lr: 3.227e-05, eta: 6:42:05, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1100, loss: 0.1100, grad_norm: 1.8228
2022-03-28 09:52:50,687 - depth - INFO - Iter [4300/38400]	lr: 3.262e-05, eta: 6:41:06, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1093, loss: 0.1093, grad_norm: 2.0010
2022-03-28 09:53:23,008 - depth - INFO - Iter [4350/38400]	lr: 3.298e-05, eta: 6:40:08, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1077, loss: 0.1077, grad_norm: 2.8477
2022-03-28 09:53:55,160 - depth - INFO - Iter [4400/38400]	lr: 3.333e-05, eta: 6:39:08, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1091, loss: 0.1091, grad_norm: 2.5128
2022-03-28 09:54:27,262 - depth - INFO - Iter [4450/38400]	lr: 3.368e-05, eta: 6:38:09, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1106, loss: 0.1106, grad_norm: 2.8606
2022-03-28 09:54:59,553 - depth - INFO - Iter [4500/38400]	lr: 3.403e-05, eta: 6:37:12, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1081, loss: 0.1081, grad_norm: 2.4314
2022-03-28 09:55:41,954 - depth - INFO - Iter [4550/38400]	lr: 3.438e-05, eta: 6:37:31, time: 0.849, data_time: 0.183, memory: 24409, decode.loss_depth: 0.1038, loss: 0.1038, grad_norm: 2.5245
2022-03-28 09:56:14,166 - depth - INFO - Iter [4600/38400]	lr: 3.473e-05, eta: 6:36:34, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1044, loss: 0.1044, grad_norm: 3.3411
2022-03-28 09:56:46,767 - depth - INFO - Iter [4650/38400]	lr: 3.508e-05, eta: 6:35:40, time: 0.652, data_time: 0.007, memory: 24409, decode.loss_depth: 0.1079, loss: 0.1079, grad_norm: 2.1718
2022-03-28 09:57:19,162 - depth - INFO - Iter [4700/38400]	lr: 3.543e-05, eta: 6:34:45, time: 0.648, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1040, loss: 0.1040, grad_norm: 2.3938
2022-03-28 09:57:51,342 - depth - INFO - Iter [4750/38400]	lr: 3.578e-05, eta: 6:33:48, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1065, loss: 0.1065, grad_norm: 2.7883
2022-03-28 09:58:23,504 - depth - INFO - Saving checkpoint at 4800 iterations
2022-03-28 09:58:36,569 - depth - INFO - Iter [4800/38400]	lr: 3.613e-05, eta: 6:34:24, time: 0.905, data_time: 0.007, memory: 24409, decode.loss_depth: 0.1046, loss: 0.1046, grad_norm: 2.1721
2022-03-28 09:59:09,402 - depth - INFO - Summary:
2022-03-28 09:59:09,403 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8851 | 0.9862 | 0.9977 |  0.1162 | 0.3922 | 0.0482 |  0.1406  | 11.3593 | 0.0627 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-03-28 09:59:22,383 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_4800.pth.
2022-03-28 09:59:22,384 - depth - INFO - Best abs_rel is 0.1162 at 4800 iter.
2022-03-28 09:59:22,384 - depth - INFO - Iter(val) [82]	a1: 0.8851, a2: 0.9862, a3: 0.9977, abs_rel: 0.11620823293924332, rmse: 0.3921833038330078, log_10: 0.04818841814994812, rmse_log: 0.14061659574508667, silog: 11.3593, sq_rel: 0.062705397605896
2022-03-28 09:59:54,687 - depth - INFO - Iter [4850/38400]	lr: 3.647e-05, eta: 6:38:46, time: 1.562, data_time: 0.921, memory: 24409, decode.loss_depth: 0.1027, loss: 0.1027, grad_norm: 2.3304
2022-03-28 10:00:27,062 - depth - INFO - Iter [4900/38400]	lr: 3.682e-05, eta: 6:37:48, time: 0.648, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1013, loss: 0.1013, grad_norm: 2.4090
2022-03-28 10:00:59,507 - depth - INFO - Iter [4950/38400]	lr: 3.716e-05, eta: 6:36:51, time: 0.649, data_time: 0.005, memory: 24409, decode.loss_depth: 0.1014, loss: 0.1014, grad_norm: 1.9841
2022-03-28 10:01:31,904 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 10:01:31,904 - depth - INFO - Iter [5000/38400]	lr: 3.750e-05, eta: 6:35:54, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1000, loss: 0.1000, grad_norm: 2.1525
2022-03-28 10:02:04,007 - depth - INFO - Iter [5050/38400]	lr: 3.784e-05, eta: 6:34:55, time: 0.642, data_time: 0.007, memory: 24409, decode.loss_depth: 0.1001, loss: 0.1001, grad_norm: 2.6471
2022-03-28 10:02:36,614 - depth - INFO - Iter [5100/38400]	lr: 3.819e-05, eta: 6:34:01, time: 0.653, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1021, loss: 0.1021, grad_norm: 2.2525
2022-03-28 10:03:08,969 - depth - INFO - Iter [5150/38400]	lr: 3.853e-05, eta: 6:33:05, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0998, loss: 0.0998, grad_norm: 2.0228
2022-03-28 10:03:41,278 - depth - INFO - Iter [5200/38400]	lr: 3.886e-05, eta: 6:32:09, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1055, loss: 0.1055, grad_norm: 2.7465
2022-03-28 10:04:13,507 - depth - INFO - Iter [5250/38400]	lr: 3.920e-05, eta: 6:31:14, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1021, loss: 0.1021, grad_norm: 2.6400
2022-03-28 10:04:45,863 - depth - INFO - Iter [5300/38400]	lr: 3.954e-05, eta: 6:30:19, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1021, loss: 0.1021, grad_norm: 2.2111
2022-03-28 10:05:17,899 - depth - INFO - Iter [5350/38400]	lr: 3.988e-05, eta: 6:29:23, time: 0.640, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0991, loss: 0.0991, grad_norm: 1.9825
2022-03-28 10:05:50,352 - depth - INFO - Iter [5400/38400]	lr: 4.021e-05, eta: 6:28:30, time: 0.649, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0986, loss: 0.0986, grad_norm: 2.0608
2022-03-28 10:06:22,597 - depth - INFO - Iter [5450/38400]	lr: 4.054e-05, eta: 6:27:36, time: 0.645, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0956, loss: 0.0956, grad_norm: 2.5367
2022-03-28 10:06:54,980 - depth - INFO - Iter [5500/38400]	lr: 4.088e-05, eta: 6:26:43, time: 0.648, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0974, loss: 0.0974, grad_norm: 1.8712
2022-03-28 10:07:27,281 - depth - INFO - Iter [5550/38400]	lr: 4.121e-05, eta: 6:25:51, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0982, loss: 0.0982, grad_norm: 2.5259
2022-03-28 10:07:59,577 - depth - INFO - Iter [5600/38400]	lr: 4.154e-05, eta: 6:24:58, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0964, loss: 0.0964, grad_norm: 2.0098
2022-03-28 10:08:31,713 - depth - INFO - Summary:
2022-03-28 10:08:31,714 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
| 0.8878 | 0.9881 | 0.998 |  0.1159 | 0.3931 | 0.0478 |  0.139   | 10.9166 | 0.0628 |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
2022-03-28 10:08:43,254 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_5600.pth.
2022-03-28 10:08:43,254 - depth - INFO - Best abs_rel is 0.1159 at 5600 iter.
2022-03-28 10:08:43,255 - depth - INFO - Iter(val) [82]	a1: 0.8878, a2: 0.9881, a3: 0.9980, abs_rel: 0.11588417738676071, rmse: 0.3931218385696411, log_10: 0.04782263562083244, rmse_log: 0.13899587094783783, silog: 10.9166, sq_rel: 0.06278110295534134
2022-03-28 10:09:15,407 - depth - INFO - Iter [5650/38400]	lr: 4.187e-05, eta: 6:28:18, time: 1.517, data_time: 0.879, memory: 24409, decode.loss_depth: 0.0953, loss: 0.0953, grad_norm: 2.6243
2022-03-28 10:09:47,537 - depth - INFO - Iter [5700/38400]	lr: 4.220e-05, eta: 6:27:23, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0964, loss: 0.0964, grad_norm: 2.5536
2022-03-28 10:10:19,899 - depth - INFO - Iter [5750/38400]	lr: 4.253e-05, eta: 6:26:29, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.1004, loss: 0.1004, grad_norm: 2.6600
2022-03-28 10:10:52,155 - depth - INFO - Iter [5800/38400]	lr: 4.285e-05, eta: 6:25:36, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0983, loss: 0.0983, grad_norm: 2.1501
2022-03-28 10:11:24,341 - depth - INFO - Iter [5850/38400]	lr: 4.318e-05, eta: 6:24:42, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0991, loss: 0.0991, grad_norm: 2.0826
2022-03-28 10:11:56,319 - depth - INFO - Iter [5900/38400]	lr: 4.350e-05, eta: 6:23:47, time: 0.640, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0962, loss: 0.0962, grad_norm: 2.8255
2022-03-28 10:12:28,343 - depth - INFO - Iter [5950/38400]	lr: 4.383e-05, eta: 6:22:53, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0948, loss: 0.0948, grad_norm: 1.8109
2022-03-28 10:13:00,484 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 10:13:00,484 - depth - INFO - Iter [6000/38400]	lr: 4.415e-05, eta: 6:22:00, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0943, loss: 0.0943, grad_norm: 1.8812
2022-03-28 10:13:32,611 - depth - INFO - Iter [6050/38400]	lr: 4.447e-05, eta: 6:21:08, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0946, loss: 0.0946, grad_norm: 2.1467
2022-03-28 10:14:14,975 - depth - INFO - Iter [6100/38400]	lr: 4.479e-05, eta: 6:21:09, time: 0.848, data_time: 0.203, memory: 24409, decode.loss_depth: 0.0916, loss: 0.0916, grad_norm: 2.2753
2022-03-28 10:14:47,061 - depth - INFO - Iter [6150/38400]	lr: 4.511e-05, eta: 6:20:17, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0919, loss: 0.0919, grad_norm: 3.4645
2022-03-28 10:15:19,203 - depth - INFO - Iter [6200/38400]	lr: 4.543e-05, eta: 6:19:24, time: 0.643, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0903, loss: 0.0903, grad_norm: 2.1996
2022-03-28 10:15:51,410 - depth - INFO - Iter [6250/38400]	lr: 4.575e-05, eta: 6:18:33, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0916, loss: 0.0916, grad_norm: 1.8502
2022-03-28 10:16:23,598 - depth - INFO - Iter [6300/38400]	lr: 4.606e-05, eta: 6:17:41, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0893, loss: 0.0893, grad_norm: 1.8475
2022-03-28 10:16:56,093 - depth - INFO - Iter [6350/38400]	lr: 4.638e-05, eta: 6:16:52, time: 0.650, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0893, loss: 0.0893, grad_norm: 1.9826
2022-03-28 10:17:28,327 - depth - INFO - Saving checkpoint at 6400 iterations
2022-03-28 10:17:41,319 - depth - INFO - Iter [6400/38400]	lr: 4.669e-05, eta: 6:17:07, time: 0.905, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0893, loss: 0.0893, grad_norm: 2.4441
2022-03-28 10:18:14,308 - depth - INFO - Summary:
2022-03-28 10:18:14,343 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8548 | 0.9837 | 0.9976 |  0.1362 | 0.4199 | 0.0543 |  0.153   | 10.7713 | 0.0751 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-03-28 10:18:14,344 - depth - INFO - Iter(val) [82]	a1: 0.8548, a2: 0.9837, a3: 0.9976, abs_rel: 0.1362306773662567, rmse: 0.41993653774261475, log_10: 0.05425722151994705, rmse_log: 0.15300564467906952, silog: 10.7713, sq_rel: 0.07508348673582077
2022-03-28 10:18:46,365 - depth - INFO - Iter [6450/38400]	lr: 4.700e-05, eta: 6:18:58, time: 1.301, data_time: 0.666, memory: 24409, decode.loss_depth: 0.0901, loss: 0.0901, grad_norm: 3.1461
2022-03-28 10:19:18,890 - depth - INFO - Iter [6500/38400]	lr: 4.731e-05, eta: 6:18:08, time: 0.650, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0888, loss: 0.0888, grad_norm: 2.2214
2022-03-28 10:19:51,099 - depth - INFO - Iter [6550/38400]	lr: 4.762e-05, eta: 6:17:16, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0896, loss: 0.0896, grad_norm: 1.9007
2022-03-28 10:20:23,506 - depth - INFO - Iter [6600/38400]	lr: 4.793e-05, eta: 6:16:25, time: 0.648, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0905, loss: 0.0905, grad_norm: 2.2546
2022-03-28 10:20:55,916 - depth - INFO - Iter [6650/38400]	lr: 4.824e-05, eta: 6:15:35, time: 0.649, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0893, loss: 0.0893, grad_norm: 1.5966
2022-03-28 10:21:28,010 - depth - INFO - Iter [6700/38400]	lr: 4.855e-05, eta: 6:14:43, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0890, loss: 0.0890, grad_norm: 2.5225
2022-03-28 10:22:00,002 - depth - INFO - Iter [6750/38400]	lr: 4.885e-05, eta: 6:13:52, time: 0.639, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0876, loss: 0.0876, grad_norm: 2.2181
2022-03-28 10:22:32,208 - depth - INFO - Iter [6800/38400]	lr: 4.916e-05, eta: 6:13:01, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0845, loss: 0.0845, grad_norm: 2.2909
2022-03-28 10:23:04,396 - depth - INFO - Iter [6850/38400]	lr: 4.946e-05, eta: 6:12:11, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0900, loss: 0.0900, grad_norm: 2.1968
2022-03-28 10:23:36,561 - depth - INFO - Iter [6900/38400]	lr: 4.976e-05, eta: 6:11:21, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0897, loss: 0.0897, grad_norm: 3.1237
2022-03-28 10:24:08,663 - depth - INFO - Iter [6950/38400]	lr: 5.006e-05, eta: 6:10:30, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0889, loss: 0.0889, grad_norm: 2.8571
2022-03-28 10:24:40,750 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 10:24:40,750 - depth - INFO - Iter [7000/38400]	lr: 5.036e-05, eta: 6:09:40, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0919, loss: 0.0919, grad_norm: 2.6315
2022-03-28 10:25:12,916 - depth - INFO - Iter [7050/38400]	lr: 5.066e-05, eta: 6:08:51, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0914, loss: 0.0914, grad_norm: 2.1062
2022-03-28 10:25:44,955 - depth - INFO - Iter [7100/38400]	lr: 5.095e-05, eta: 6:08:01, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0876, loss: 0.0876, grad_norm: 1.9874
2022-03-28 10:26:17,064 - depth - INFO - Iter [7150/38400]	lr: 5.125e-05, eta: 6:07:12, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0888, loss: 0.0888, grad_norm: 2.1306
2022-03-28 10:26:49,275 - depth - INFO - Iter [7200/38400]	lr: 5.154e-05, eta: 6:06:24, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0874, loss: 0.0874, grad_norm: 3.2014
2022-03-28 10:27:22,049 - depth - INFO - Summary:
2022-03-28 10:27:22,051 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
| 0.8756 | 0.9863 | 0.998 |  0.1259 | 0.3918 | 0.0509 |  0.1446  | 10.6428 | 0.0643 |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
2022-03-28 10:27:22,052 - depth - INFO - Iter(val) [82]	a1: 0.8756, a2: 0.9863, a3: 0.9980, abs_rel: 0.1258784532546997, rmse: 0.39178261160850525, log_10: 0.050883691757917404, rmse_log: 0.14461344480514526, silog: 10.6428, sq_rel: 0.06426748633384705
2022-03-28 10:27:54,276 - depth - INFO - Iter [7250/38400]	lr: 5.184e-05, eta: 6:07:57, time: 1.300, data_time: 0.662, memory: 24409, decode.loss_depth: 0.0913, loss: 0.0913, grad_norm: 2.4122
2022-03-28 10:28:26,378 - depth - INFO - Iter [7300/38400]	lr: 5.213e-05, eta: 6:07:07, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0890, loss: 0.0890, grad_norm: 2.2937
2022-03-28 10:28:58,596 - depth - INFO - Iter [7350/38400]	lr: 5.242e-05, eta: 6:06:18, time: 0.644, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0862, loss: 0.0862, grad_norm: 2.4926
2022-03-28 10:29:30,814 - depth - INFO - Iter [7400/38400]	lr: 5.271e-05, eta: 6:05:29, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0870, loss: 0.0870, grad_norm: 1.6772
2022-03-28 10:30:02,933 - depth - INFO - Iter [7450/38400]	lr: 5.300e-05, eta: 6:04:41, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0860, loss: 0.0860, grad_norm: 2.5822
2022-03-28 10:30:35,282 - depth - INFO - Iter [7500/38400]	lr: 5.328e-05, eta: 6:03:53, time: 0.647, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0882, loss: 0.0882, grad_norm: 2.6549
2022-03-28 10:31:07,582 - depth - INFO - Iter [7550/38400]	lr: 5.357e-05, eta: 6:03:05, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0848, loss: 0.0848, grad_norm: 2.3605
2022-03-28 10:31:49,942 - depth - INFO - Iter [7600/38400]	lr: 5.385e-05, eta: 6:02:58, time: 0.848, data_time: 0.164, memory: 24409, decode.loss_depth: 0.0865, loss: 0.0865, grad_norm: 2.4061
2022-03-28 10:32:22,245 - depth - INFO - Iter [7650/38400]	lr: 5.413e-05, eta: 6:02:11, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0805, loss: 0.0805, grad_norm: 1.8139
2022-03-28 10:32:54,347 - depth - INFO - Iter [7700/38400]	lr: 5.441e-05, eta: 6:01:23, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0829, loss: 0.0829, grad_norm: 2.3041
2022-03-28 10:33:26,316 - depth - INFO - Iter [7750/38400]	lr: 5.469e-05, eta: 6:00:34, time: 0.639, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0835, loss: 0.0835, grad_norm: 2.5908
2022-03-28 10:33:58,325 - depth - INFO - Iter [7800/38400]	lr: 5.497e-05, eta: 5:59:46, time: 0.640, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0821, loss: 0.0821, grad_norm: 2.3265
2022-03-28 10:34:30,424 - depth - INFO - Iter [7850/38400]	lr: 5.525e-05, eta: 5:58:58, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0825, loss: 0.0825, grad_norm: 1.8140
2022-03-28 10:35:02,609 - depth - INFO - Iter [7900/38400]	lr: 5.552e-05, eta: 5:58:11, time: 0.643, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0835, loss: 0.0835, grad_norm: 2.6480
2022-03-28 10:35:34,674 - depth - INFO - Iter [7950/38400]	lr: 5.580e-05, eta: 5:57:24, time: 0.641, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0830, loss: 0.0830, grad_norm: 1.7646
2022-03-28 10:36:06,883 - depth - INFO - Saving checkpoint at 8000 iterations
2022-03-28 10:36:19,886 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 10:36:19,887 - depth - INFO - Iter [8000/38400]	lr: 5.607e-05, eta: 5:57:27, time: 0.905, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0806, loss: 0.0806, grad_norm: 2.2376
2022-03-28 10:36:51,918 - depth - INFO - Summary:
2022-03-28 10:36:51,919 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
| 0.8978 | 0.9885 | 0.9984 |  0.1091 | 0.3607 | 0.0454 |  0.132   | 10.45 | 0.0541 |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
2022-03-28 10:37:03,585 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_8000.pth.
2022-03-28 10:37:03,586 - depth - INFO - Best abs_rel is 0.1091 at 8000 iter.
2022-03-28 10:37:03,586 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 10:37:03,587 - depth - INFO - Iter(val) [82]	a1: 0.8978, a2: 0.9885, a3: 0.9984, abs_rel: 0.1091127097606659, rmse: 0.36067262291908264, log_10: 0.045390475541353226, rmse_log: 0.13202272355556488, silog: 10.4500, sq_rel: 0.054135747253894806
2022-03-28 10:37:35,691 - depth - INFO - Iter [8050/38400]	lr: 5.634e-05, eta: 5:59:24, time: 1.515, data_time: 0.880, memory: 24409, decode.loss_depth: 0.0800, loss: 0.0800, grad_norm: 2.0922
2022-03-28 10:38:08,005 - depth - INFO - Iter [8100/38400]	lr: 5.661e-05, eta: 5:58:36, time: 0.647, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0821, loss: 0.0821, grad_norm: 2.2167
2022-03-28 10:38:40,151 - depth - INFO - Iter [8150/38400]	lr: 5.688e-05, eta: 5:57:48, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0827, loss: 0.0827, grad_norm: 2.4067
2022-03-28 10:39:12,163 - depth - INFO - Iter [8200/38400]	lr: 5.715e-05, eta: 5:57:00, time: 0.640, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0836, loss: 0.0836, grad_norm: 2.3659
2022-03-28 10:39:44,472 - depth - INFO - Iter [8250/38400]	lr: 5.741e-05, eta: 5:56:13, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0820, loss: 0.0820, grad_norm: 1.6597
2022-03-28 10:40:16,742 - depth - INFO - Iter [8300/38400]	lr: 5.768e-05, eta: 5:55:26, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0800, loss: 0.0800, grad_norm: 1.2936
2022-03-28 10:40:49,089 - depth - INFO - Iter [8350/38400]	lr: 5.794e-05, eta: 5:54:40, time: 0.648, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0793, loss: 0.0793, grad_norm: 1.6375
2022-03-28 10:41:21,056 - depth - INFO - Iter [8400/38400]	lr: 5.820e-05, eta: 5:53:52, time: 0.639, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0806, loss: 0.0806, grad_norm: 1.7485
2022-03-28 10:41:53,328 - depth - INFO - Iter [8450/38400]	lr: 5.846e-05, eta: 5:53:06, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0793, loss: 0.0793, grad_norm: 1.9629
2022-03-28 10:42:25,498 - depth - INFO - Iter [8500/38400]	lr: 5.872e-05, eta: 5:52:19, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0800, loss: 0.0800, grad_norm: 2.0868
2022-03-28 10:42:57,656 - depth - INFO - Iter [8550/38400]	lr: 5.898e-05, eta: 5:51:33, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0807, loss: 0.0807, grad_norm: 1.7571
2022-03-28 10:43:29,614 - depth - INFO - Iter [8600/38400]	lr: 5.923e-05, eta: 5:50:45, time: 0.639, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0814, loss: 0.0814, grad_norm: 1.9831
2022-03-28 10:44:01,927 - depth - INFO - Iter [8650/38400]	lr: 5.949e-05, eta: 5:50:00, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0793, loss: 0.0793, grad_norm: 1.5716
2022-03-28 10:44:33,939 - depth - INFO - Iter [8700/38400]	lr: 5.974e-05, eta: 5:49:13, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0828, loss: 0.0828, grad_norm: 3.8042
2022-03-28 10:45:06,070 - depth - INFO - Iter [8750/38400]	lr: 5.999e-05, eta: 5:48:27, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0798, loss: 0.0798, grad_norm: 2.5302
2022-03-28 10:45:38,199 - depth - INFO - Iter [8800/38400]	lr: 6.024e-05, eta: 5:47:42, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0787, loss: 0.0787, grad_norm: 1.8890
2022-03-28 10:46:10,695 - depth - INFO - Summary:
2022-03-28 10:46:10,696 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.9018 | 0.9887 | 0.9978 |  0.1003 | 0.3714 | 0.0441 |   0.13   | 10.5908 | 0.0515 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-03-28 10:46:21,700 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_8800.pth.
2022-03-28 10:46:21,701 - depth - INFO - Best abs_rel is 0.1003 at 8800 iter.
2022-03-28 10:46:21,701 - depth - INFO - Iter(val) [82]	a1: 0.9018, a2: 0.9887, a3: 0.9978, abs_rel: 0.10030156373977661, rmse: 0.3713596761226654, log_10: 0.04413054510951042, rmse_log: 0.129956915974617, silog: 10.5908, sq_rel: 0.051462624222040176
2022-03-28 10:46:53,858 - depth - INFO - Iter [8850/38400]	lr: 6.049e-05, eta: 5:49:21, time: 1.514, data_time: 0.876, memory: 24409, decode.loss_depth: 0.0776, loss: 0.0776, grad_norm: 2.2151
2022-03-28 10:47:25,942 - depth - INFO - Iter [8900/38400]	lr: 6.074e-05, eta: 5:48:35, time: 0.641, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0818, loss: 0.0818, grad_norm: 2.1535
2022-03-28 10:47:58,128 - depth - INFO - Iter [8950/38400]	lr: 6.098e-05, eta: 5:47:48, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0788, loss: 0.0788, grad_norm: 2.1772
2022-03-28 10:48:30,395 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 10:48:30,395 - depth - INFO - Iter [9000/38400]	lr: 6.123e-05, eta: 5:47:03, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0792, loss: 0.0792, grad_norm: 1.7660
2022-03-28 10:49:02,753 - depth - INFO - Iter [9050/38400]	lr: 6.147e-05, eta: 5:46:17, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0780, loss: 0.0780, grad_norm: 2.0098
2022-03-28 10:49:44,218 - depth - INFO - Iter [9100/38400]	lr: 6.171e-05, eta: 5:46:01, time: 0.829, data_time: 0.179, memory: 24409, decode.loss_depth: 0.0798, loss: 0.0798, grad_norm: 2.7519
2022-03-28 10:50:16,409 - depth - INFO - Iter [9150/38400]	lr: 6.195e-05, eta: 5:45:16, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0788, loss: 0.0788, grad_norm: 1.5826
2022-03-28 10:50:48,392 - depth - INFO - Iter [9200/38400]	lr: 6.219e-05, eta: 5:44:29, time: 0.640, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0754, loss: 0.0754, grad_norm: 1.8581
2022-03-28 10:51:20,336 - depth - INFO - Iter [9250/38400]	lr: 6.242e-05, eta: 5:43:43, time: 0.639, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0763, loss: 0.0763, grad_norm: 1.9306
2022-03-28 10:51:52,153 - depth - INFO - Iter [9300/38400]	lr: 6.266e-05, eta: 5:42:57, time: 0.637, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0767, loss: 0.0767, grad_norm: 2.3418
2022-03-28 10:52:24,198 - depth - INFO - Iter [9350/38400]	lr: 6.289e-05, eta: 5:42:11, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0743, loss: 0.0743, grad_norm: 1.9219
2022-03-28 10:52:56,536 - depth - INFO - Iter [9400/38400]	lr: 6.312e-05, eta: 5:41:26, time: 0.647, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0772, loss: 0.0772, grad_norm: 2.0711
2022-03-28 10:53:28,647 - depth - INFO - Iter [9450/38400]	lr: 6.335e-05, eta: 5:40:41, time: 0.643, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0766, loss: 0.0766, grad_norm: 1.5971
2022-03-28 10:54:00,976 - depth - INFO - Iter [9500/38400]	lr: 6.358e-05, eta: 5:39:57, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0753, loss: 0.0753, grad_norm: 1.6126
2022-03-28 10:54:33,343 - depth - INFO - Iter [9550/38400]	lr: 6.381e-05, eta: 5:39:13, time: 0.647, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0768, loss: 0.0768, grad_norm: 1.7151
2022-03-28 10:55:05,324 - depth - INFO - Saving checkpoint at 9600 iterations
2022-03-28 10:55:18,777 - depth - INFO - Iter [9600/38400]	lr: 6.403e-05, eta: 5:39:08, time: 0.909, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0768, loss: 0.0768, grad_norm: 1.7974
2022-03-28 10:55:51,114 - depth - INFO - Summary:
2022-03-28 10:55:51,115 - depth - INFO - 
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1  |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.911 | 0.9891 | 0.9977 |  0.1003 | 0.3511 | 0.0424 |  0.126   | 10.2359 | 0.051  |
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-03-28 10:56:03,422 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_9600.pth.
2022-03-28 10:56:03,423 - depth - INFO - Best abs_rel is 0.1003 at 9600 iter.
2022-03-28 10:56:03,423 - depth - INFO - Iter(val) [82]	a1: 0.9110, a2: 0.9891, a3: 0.9977, abs_rel: 0.10025964677333832, rmse: 0.3511005640029907, log_10: 0.042396679520606995, rmse_log: 0.1259603202342987, silog: 10.2359, sq_rel: 0.051003165543079376
2022-03-28 10:56:35,867 - depth - INFO - Iter [9650/38400]	lr: 6.426e-05, eta: 5:40:37, time: 1.542, data_time: 0.898, memory: 24409, decode.loss_depth: 0.0772, loss: 0.0772, grad_norm: 2.3067
2022-03-28 10:57:07,914 - depth - INFO - Iter [9700/38400]	lr: 6.448e-05, eta: 5:39:51, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0765, loss: 0.0765, grad_norm: 1.9199
2022-03-28 10:57:40,130 - depth - INFO - Iter [9750/38400]	lr: 6.470e-05, eta: 5:39:06, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0767, loss: 0.0767, grad_norm: 1.6131
2022-03-28 10:58:12,418 - depth - INFO - Iter [9800/38400]	lr: 6.492e-05, eta: 5:38:21, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0766, loss: 0.0766, grad_norm: 2.1723
2022-03-28 10:58:44,854 - depth - INFO - Iter [9850/38400]	lr: 6.514e-05, eta: 5:37:37, time: 0.648, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0764, loss: 0.0764, grad_norm: 2.0821
2022-03-28 10:59:17,098 - depth - INFO - Iter [9900/38400]	lr: 6.535e-05, eta: 5:36:52, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0748, loss: 0.0748, grad_norm: 1.4306
2022-03-28 10:59:49,281 - depth - INFO - Iter [9950/38400]	lr: 6.557e-05, eta: 5:36:07, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0747, loss: 0.0747, grad_norm: 1.9785
2022-03-28 11:00:21,498 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 11:00:21,499 - depth - INFO - Iter [10000/38400]	lr: 6.578e-05, eta: 5:35:22, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0773, loss: 0.0773, grad_norm: 2.0457
2022-03-28 11:00:53,490 - depth - INFO - Iter [10050/38400]	lr: 6.599e-05, eta: 5:34:37, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0764, loss: 0.0764, grad_norm: 2.1317
2022-03-28 11:01:25,655 - depth - INFO - Iter [10100/38400]	lr: 6.620e-05, eta: 5:33:53, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0763, loss: 0.0763, grad_norm: 2.2836
2022-03-28 11:01:57,830 - depth - INFO - Iter [10150/38400]	lr: 6.641e-05, eta: 5:33:08, time: 0.643, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0761, loss: 0.0761, grad_norm: 1.8903
2022-03-28 11:02:30,103 - depth - INFO - Iter [10200/38400]	lr: 6.661e-05, eta: 5:32:24, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0756, loss: 0.0756, grad_norm: 2.0664
2022-03-28 11:03:02,341 - depth - INFO - Iter [10250/38400]	lr: 6.682e-05, eta: 5:31:40, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0769, loss: 0.0769, grad_norm: 2.0946
2022-03-28 11:03:34,982 - depth - INFO - Iter [10300/38400]	lr: 6.702e-05, eta: 5:30:58, time: 0.653, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0735, loss: 0.0735, grad_norm: 1.9282
2022-03-28 11:04:06,990 - depth - INFO - Iter [10350/38400]	lr: 6.722e-05, eta: 5:30:13, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0804, loss: 0.0804, grad_norm: 1.7122
2022-03-28 11:04:39,161 - depth - INFO - Iter [10400/38400]	lr: 6.742e-05, eta: 5:29:30, time: 0.643, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0773, loss: 0.0773, grad_norm: 2.8240
2022-03-28 11:05:11,504 - depth - INFO - Summary:
2022-03-28 11:05:11,505 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8984 | 0.9883 | 0.9977 |  0.1128 | 0.3749 | 0.0468 |  0.1351  | 10.3156 | 0.0567 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-03-28 11:05:11,506 - depth - INFO - Iter(val) [82]	a1: 0.8984, a2: 0.9883, a3: 0.9977, abs_rel: 0.11282383650541306, rmse: 0.37485992908477783, log_10: 0.046828966587781906, rmse_log: 0.13510219752788544, silog: 10.3156, sq_rel: 0.056670453399419785
2022-03-28 11:05:43,809 - depth - INFO - Iter [10450/38400]	lr: 6.762e-05, eta: 5:30:13, time: 1.293, data_time: 0.653, memory: 24409, decode.loss_depth: 0.0763, loss: 0.0763, grad_norm: 1.7066
2022-03-28 11:06:15,921 - depth - INFO - Iter [10500/38400]	lr: 6.782e-05, eta: 5:29:28, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0724, loss: 0.0724, grad_norm: 1.5274
2022-03-28 11:06:48,039 - depth - INFO - Iter [10550/38400]	lr: 6.801e-05, eta: 5:28:44, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0718, loss: 0.0718, grad_norm: 1.2691
2022-03-28 11:07:29,961 - depth - INFO - Iter [10600/38400]	lr: 6.820e-05, eta: 5:28:26, time: 0.838, data_time: 0.182, memory: 24409, decode.loss_depth: 0.0720, loss: 0.0720, grad_norm: 1.4530
2022-03-28 11:08:02,140 - depth - INFO - Iter [10650/38400]	lr: 6.840e-05, eta: 5:27:42, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0725, loss: 0.0725, grad_norm: 1.5890
2022-03-28 11:08:34,337 - depth - INFO - Iter [10700/38400]	lr: 6.859e-05, eta: 5:26:58, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0742, loss: 0.0742, grad_norm: 1.4260
2022-03-28 11:09:06,435 - depth - INFO - Iter [10750/38400]	lr: 6.877e-05, eta: 5:26:14, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0730, loss: 0.0730, grad_norm: 1.8762
2022-03-28 11:09:38,369 - depth - INFO - Iter [10800/38400]	lr: 6.896e-05, eta: 5:25:30, time: 0.639, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0720, loss: 0.0720, grad_norm: 1.5531
2022-03-28 11:10:10,503 - depth - INFO - Iter [10850/38400]	lr: 6.914e-05, eta: 5:24:46, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0727, loss: 0.0727, grad_norm: 1.8514
2022-03-28 11:10:42,632 - depth - INFO - Iter [10900/38400]	lr: 6.933e-05, eta: 5:24:03, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0723, loss: 0.0723, grad_norm: 1.9560
2022-03-28 11:11:14,670 - depth - INFO - Iter [10950/38400]	lr: 6.951e-05, eta: 5:23:19, time: 0.641, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0710, loss: 0.0710, grad_norm: 1.2225
2022-03-28 11:11:46,718 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 11:11:46,718 - depth - INFO - Iter [11000/38400]	lr: 6.969e-05, eta: 5:22:36, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0690, loss: 0.0690, grad_norm: 1.3054
2022-03-28 11:12:18,859 - depth - INFO - Iter [11050/38400]	lr: 6.987e-05, eta: 5:21:52, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0712, loss: 0.0712, grad_norm: 1.2967
2022-03-28 11:12:50,902 - depth - INFO - Iter [11100/38400]	lr: 7.004e-05, eta: 5:21:09, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0689, loss: 0.0689, grad_norm: 1.4812
2022-03-28 11:13:22,673 - depth - INFO - Iter [11150/38400]	lr: 7.022e-05, eta: 5:20:25, time: 0.635, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0705, loss: 0.0705, grad_norm: 2.3207
2022-03-28 11:13:54,629 - depth - INFO - Saving checkpoint at 11200 iterations
2022-03-28 11:14:07,703 - depth - INFO - Iter [11200/38400]	lr: 7.039e-05, eta: 5:20:14, time: 0.901, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0696, loss: 0.0696, grad_norm: 1.3200
2022-03-28 11:14:40,444 - depth - INFO - Summary:
2022-03-28 11:14:40,445 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.9095 | 0.9874 | 0.9966 |  0.1039 | 0.3539 | 0.0433 |  0.128   | 10.1479 | 0.0526 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-03-28 11:14:40,446 - depth - INFO - Iter(val) [82]	a1: 0.9095, a2: 0.9874, a3: 0.9966, abs_rel: 0.1039457619190216, rmse: 0.353943794965744, log_10: 0.04326499626040459, rmse_log: 0.12796370685100555, silog: 10.1479, sq_rel: 0.05264163762331009
2022-03-28 11:15:12,425 - depth - INFO - Iter [11250/38400]	lr: 7.056e-05, eta: 5:20:49, time: 1.294, data_time: 0.661, memory: 24409, decode.loss_depth: 0.0688, loss: 0.0688, grad_norm: 1.3535
2022-03-28 11:15:44,562 - depth - INFO - Iter [11300/38400]	lr: 7.073e-05, eta: 5:20:06, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0721, loss: 0.0721, grad_norm: 1.5387
2022-03-28 11:16:16,537 - depth - INFO - Iter [11350/38400]	lr: 7.090e-05, eta: 5:19:22, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0720, loss: 0.0720, grad_norm: 1.6960
2022-03-28 11:16:48,653 - depth - INFO - Iter [11400/38400]	lr: 7.106e-05, eta: 5:18:39, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0702, loss: 0.0702, grad_norm: 1.4540
2022-03-28 11:17:20,743 - depth - INFO - Iter [11450/38400]	lr: 7.123e-05, eta: 5:17:56, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0692, loss: 0.0692, grad_norm: 1.3779
2022-03-28 11:17:53,069 - depth - INFO - Iter [11500/38400]	lr: 7.139e-05, eta: 5:17:13, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0727, loss: 0.0727, grad_norm: 1.6900
2022-03-28 11:18:25,305 - depth - INFO - Iter [11550/38400]	lr: 7.155e-05, eta: 5:16:30, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0690, loss: 0.0690, grad_norm: 1.7968
2022-03-28 11:18:57,474 - depth - INFO - Iter [11600/38400]	lr: 7.171e-05, eta: 5:15:48, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0698, loss: 0.0698, grad_norm: 1.6107
2022-03-28 11:19:29,505 - depth - INFO - Iter [11650/38400]	lr: 7.187e-05, eta: 5:15:05, time: 0.640, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0721, loss: 0.0721, grad_norm: 1.7102
2022-03-28 11:20:01,613 - depth - INFO - Iter [11700/38400]	lr: 7.202e-05, eta: 5:14:22, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0673, loss: 0.0673, grad_norm: 1.5242
2022-03-28 11:20:33,863 - depth - INFO - Iter [11750/38400]	lr: 7.218e-05, eta: 5:13:40, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0710, loss: 0.0710, grad_norm: 1.9209
2022-03-28 11:21:05,896 - depth - INFO - Iter [11800/38400]	lr: 7.233e-05, eta: 5:12:57, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0692, loss: 0.0692, grad_norm: 1.6775
2022-03-28 11:21:38,000 - depth - INFO - Iter [11850/38400]	lr: 7.248e-05, eta: 5:12:15, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0697, loss: 0.0697, grad_norm: 1.4887
2022-03-28 11:22:10,088 - depth - INFO - Iter [11900/38400]	lr: 7.263e-05, eta: 5:11:32, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0699, loss: 0.0699, grad_norm: 1.5675
2022-03-28 11:22:42,344 - depth - INFO - Iter [11950/38400]	lr: 7.277e-05, eta: 5:10:50, time: 0.646, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0703, loss: 0.0703, grad_norm: 1.7223
2022-03-28 11:23:14,344 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 11:23:14,345 - depth - INFO - Iter [12000/38400]	lr: 7.292e-05, eta: 5:10:08, time: 0.640, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0682, loss: 0.0682, grad_norm: 1.7374
2022-03-28 11:23:46,170 - depth - INFO - Summary:
2022-03-28 11:23:46,171 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.9144 | 0.9892 | 0.9981 |  0.1001 | 0.3506 | 0.0425 |  0.1256  | 10.0416 | 0.0493 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-03-28 11:23:57,086 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_12000.pth.
2022-03-28 11:23:57,086 - depth - INFO - Best abs_rel is 0.1001 at 12000 iter.
2022-03-28 11:23:57,087 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 11:23:57,087 - depth - INFO - Iter(val) [82]	a1: 0.9144, a2: 0.9892, a3: 0.9981, abs_rel: 0.10008957237005234, rmse: 0.3505673110485077, log_10: 0.04252438619732857, rmse_log: 0.1255793273448944, silog: 10.0416, sq_rel: 0.04933089390397072
2022-03-28 11:24:29,212 - depth - INFO - Iter [12050/38400]	lr: 7.306e-05, eta: 5:10:59, time: 1.498, data_time: 0.861, memory: 24409, decode.loss_depth: 0.0705, loss: 0.0705, grad_norm: 1.7218
2022-03-28 11:25:01,189 - depth - INFO - Iter [12100/38400]	lr: 7.320e-05, eta: 5:10:16, time: 0.639, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0693, loss: 0.0693, grad_norm: 1.7169
2022-03-28 11:25:41,722 - depth - INFO - Iter [12150/38400]	lr: 7.334e-05, eta: 5:09:52, time: 0.811, data_time: 0.180, memory: 24409, decode.loss_depth: 0.0654, loss: 0.0654, grad_norm: 1.4003
2022-03-28 11:26:13,743 - depth - INFO - Iter [12200/38400]	lr: 7.348e-05, eta: 5:09:09, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0688, loss: 0.0688, grad_norm: 1.5953
2022-03-28 11:26:45,748 - depth - INFO - Iter [12250/38400]	lr: 7.362e-05, eta: 5:08:27, time: 0.640, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0688, loss: 0.0688, grad_norm: 1.7528
2022-03-28 11:27:17,805 - depth - INFO - Iter [12300/38400]	lr: 7.375e-05, eta: 5:07:44, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0693, loss: 0.0693, grad_norm: 1.5118
2022-03-28 11:27:49,971 - depth - INFO - Iter [12350/38400]	lr: 7.388e-05, eta: 5:07:02, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0686, loss: 0.0686, grad_norm: 1.9531
2022-03-28 11:28:22,145 - depth - INFO - Iter [12400/38400]	lr: 7.402e-05, eta: 5:06:20, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0683, loss: 0.0683, grad_norm: 1.1694
2022-03-28 11:28:54,370 - depth - INFO - Iter [12450/38400]	lr: 7.414e-05, eta: 5:05:38, time: 0.645, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0677, loss: 0.0677, grad_norm: 1.2425
2022-03-28 11:29:26,456 - depth - INFO - Iter [12500/38400]	lr: 7.427e-05, eta: 5:04:56, time: 0.642, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0679, loss: 0.0679, grad_norm: 1.8368
2022-03-28 11:29:58,532 - depth - INFO - Iter [12550/38400]	lr: 7.440e-05, eta: 5:04:14, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0672, loss: 0.0672, grad_norm: 1.6810
2022-03-28 11:30:30,636 - depth - INFO - Iter [12600/38400]	lr: 7.452e-05, eta: 5:03:32, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0659, loss: 0.0659, grad_norm: 1.4271
2022-03-28 11:31:02,839 - depth - INFO - Iter [12650/38400]	lr: 7.464e-05, eta: 5:02:51, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0668, loss: 0.0668, grad_norm: 1.4282
2022-03-28 11:31:34,922 - depth - INFO - Iter [12700/38400]	lr: 7.476e-05, eta: 5:02:09, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0670, loss: 0.0670, grad_norm: 1.1366
2022-03-28 11:32:07,411 - depth - INFO - Iter [12750/38400]	lr: 7.488e-05, eta: 5:01:28, time: 0.650, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0672, loss: 0.0672, grad_norm: 2.0131
2022-03-28 11:32:39,557 - depth - INFO - Saving checkpoint at 12800 iterations
2022-03-28 11:32:52,565 - depth - INFO - Iter [12800/38400]	lr: 7.500e-05, eta: 5:01:13, time: 0.903, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0667, loss: 0.0667, grad_norm: 1.4547
2022-03-28 11:33:24,395 - depth - INFO - Summary:
2022-03-28 11:33:24,396 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.9049 | 0.9884 | 0.9974 |  0.1078 | 0.3664 | 0.0448 |  0.1303  | 10.0558 | 0.0577 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-03-28 11:33:24,397 - depth - INFO - Iter(val) [82]	a1: 0.9049, a2: 0.9884, a3: 0.9974, abs_rel: 0.10780717432498932, rmse: 0.3664461672306061, log_10: 0.044769659638404846, rmse_log: 0.13030080497264862, silog: 10.0558, sq_rel: 0.057653434574604034
2022-03-28 11:33:56,488 - depth - INFO - Iter [12850/38400]	lr: 7.483e-05, eta: 5:01:34, time: 1.278, data_time: 0.642, memory: 24409, decode.loss_depth: 0.0661, loss: 0.0661, grad_norm: 1.7149
2022-03-28 11:34:28,503 - depth - INFO - Iter [12900/38400]	lr: 7.465e-05, eta: 5:00:52, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0685, loss: 0.0685, grad_norm: 1.8304
2022-03-28 11:35:00,905 - depth - INFO - Iter [12950/38400]	lr: 7.447e-05, eta: 5:00:11, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0652, loss: 0.0652, grad_norm: 1.3368
2022-03-28 11:35:32,920 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 11:35:32,921 - depth - INFO - Iter [13000/38400]	lr: 7.429e-05, eta: 4:59:29, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0672, loss: 0.0672, grad_norm: 1.5637
2022-03-28 11:36:05,186 - depth - INFO - Iter [13050/38400]	lr: 7.411e-05, eta: 4:58:47, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0657, loss: 0.0657, grad_norm: 1.8554
2022-03-28 11:36:37,184 - depth - INFO - Iter [13100/38400]	lr: 7.393e-05, eta: 4:58:06, time: 0.640, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0661, loss: 0.0661, grad_norm: 1.7260
2022-03-28 11:37:09,414 - depth - INFO - Iter [13150/38400]	lr: 7.375e-05, eta: 4:57:24, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0660, loss: 0.0660, grad_norm: 1.6715
2022-03-28 11:37:41,570 - depth - INFO - Iter [13200/38400]	lr: 7.357e-05, eta: 4:56:43, time: 0.643, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0653, loss: 0.0653, grad_norm: 1.2407
2022-03-28 11:38:14,014 - depth - INFO - Iter [13250/38400]	lr: 7.339e-05, eta: 4:56:02, time: 0.649, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0654, loss: 0.0654, grad_norm: 1.8469
2022-03-28 11:38:46,118 - depth - INFO - Iter [13300/38400]	lr: 7.321e-05, eta: 4:55:21, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0658, loss: 0.0658, grad_norm: 1.4086
2022-03-28 11:39:18,473 - depth - INFO - Iter [13350/38400]	lr: 7.303e-05, eta: 4:54:40, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0661, loss: 0.0661, grad_norm: 1.7824
2022-03-28 11:39:50,610 - depth - INFO - Iter [13400/38400]	lr: 7.285e-05, eta: 4:53:59, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0656, loss: 0.0656, grad_norm: 1.1799
2022-03-28 11:40:22,912 - depth - INFO - Iter [13450/38400]	lr: 7.267e-05, eta: 4:53:18, time: 0.647, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0648, loss: 0.0648, grad_norm: 1.2868
2022-03-28 11:40:55,045 - depth - INFO - Iter [13500/38400]	lr: 7.248e-05, eta: 4:52:37, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0626, loss: 0.0626, grad_norm: 1.4412
2022-03-28 11:41:27,486 - depth - INFO - Iter [13550/38400]	lr: 7.230e-05, eta: 4:51:56, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0655, loss: 0.0655, grad_norm: 1.3890
2022-03-28 11:41:59,626 - depth - INFO - Iter [13600/38400]	lr: 7.212e-05, eta: 4:51:16, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0662, loss: 0.0662, grad_norm: 1.6749
2022-03-28 11:42:32,824 - depth - INFO - Summary:
2022-03-28 11:42:32,825 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9083 | 0.9894 | 0.9979 |  0.0971 | 0.3548 | 0.0428 |  0.1252  | 9.8883 | 0.0487 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 11:42:44,359 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_13600.pth.
2022-03-28 11:42:44,360 - depth - INFO - Best abs_rel is 0.0971 at 13600 iter.
2022-03-28 11:42:44,360 - depth - INFO - Iter(val) [82]	a1: 0.9083, a2: 0.9894, a3: 0.9979, abs_rel: 0.09713002294301987, rmse: 0.3547673523426056, log_10: 0.04278373718261719, rmse_log: 0.12523233890533447, silog: 9.8883, sq_rel: 0.0486544631421566
2022-03-28 11:43:27,323 - depth - INFO - Iter [13650/38400]	lr: 7.193e-05, eta: 4:52:15, time: 1.753, data_time: 1.085, memory: 24409, decode.loss_depth: 0.0620, loss: 0.0620, grad_norm: 1.2556
2022-03-28 11:43:59,613 - depth - INFO - Iter [13700/38400]	lr: 7.175e-05, eta: 4:51:34, time: 0.646, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0643, loss: 0.0643, grad_norm: 1.8980
2022-03-28 11:44:31,837 - depth - INFO - Iter [13750/38400]	lr: 7.157e-05, eta: 4:50:53, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0625, loss: 0.0625, grad_norm: 1.8373
2022-03-28 11:45:03,892 - depth - INFO - Iter [13800/38400]	lr: 7.138e-05, eta: 4:50:12, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0639, loss: 0.0639, grad_norm: 1.1212
2022-03-28 11:45:35,966 - depth - INFO - Iter [13850/38400]	lr: 7.120e-05, eta: 4:49:31, time: 0.642, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0626, loss: 0.0626, grad_norm: 1.2283
2022-03-28 11:46:08,234 - depth - INFO - Iter [13900/38400]	lr: 7.101e-05, eta: 4:48:50, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0634, loss: 0.0634, grad_norm: 1.3106
2022-03-28 11:46:40,285 - depth - INFO - Iter [13950/38400]	lr: 7.083e-05, eta: 4:48:08, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0628, loss: 0.0628, grad_norm: 1.1549
2022-03-28 11:47:12,352 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 11:47:12,352 - depth - INFO - Iter [14000/38400]	lr: 7.064e-05, eta: 4:47:27, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0620, loss: 0.0620, grad_norm: 1.2677
2022-03-28 11:47:44,571 - depth - INFO - Iter [14050/38400]	lr: 7.045e-05, eta: 4:46:47, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0591, loss: 0.0591, grad_norm: 1.3294
2022-03-28 11:48:16,536 - depth - INFO - Iter [14100/38400]	lr: 7.027e-05, eta: 4:46:05, time: 0.639, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0594, loss: 0.0594, grad_norm: 1.3250
2022-03-28 11:48:48,826 - depth - INFO - Iter [14150/38400]	lr: 7.008e-05, eta: 4:45:25, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0618, loss: 0.0618, grad_norm: 1.6047
2022-03-28 11:49:20,892 - depth - INFO - Iter [14200/38400]	lr: 6.989e-05, eta: 4:44:44, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0612, loss: 0.0612, grad_norm: 1.4002
2022-03-28 11:49:53,105 - depth - INFO - Iter [14250/38400]	lr: 6.970e-05, eta: 4:44:04, time: 0.644, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0616, loss: 0.0616, grad_norm: 1.3530
2022-03-28 11:50:25,098 - depth - INFO - Iter [14300/38400]	lr: 6.952e-05, eta: 4:43:23, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0630, loss: 0.0630, grad_norm: 0.9669
2022-03-28 11:50:57,141 - depth - INFO - Iter [14350/38400]	lr: 6.933e-05, eta: 4:42:42, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0608, loss: 0.0608, grad_norm: 1.2182
2022-03-28 11:51:29,339 - depth - INFO - Saving checkpoint at 14400 iterations
2022-03-28 11:51:42,628 - depth - INFO - Iter [14400/38400]	lr: 6.914e-05, eta: 4:42:24, time: 0.910, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0621, loss: 0.0621, grad_norm: 1.2059
2022-03-28 11:52:14,398 - depth - INFO - Summary:
2022-03-28 11:52:14,398 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9115 | 0.9893 | 0.9975 |  0.1038 | 0.3509 | 0.0435 |  0.1269  | 9.8923 | 0.0513 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 11:52:14,399 - depth - INFO - Iter(val) [82]	a1: 0.9115, a2: 0.9893, a3: 0.9975, abs_rel: 0.1037791520357132, rmse: 0.3508904278278351, log_10: 0.043522730469703674, rmse_log: 0.1269417107105255, silog: 9.8923, sq_rel: 0.051316987723112106
2022-03-28 11:52:46,528 - depth - INFO - Iter [14450/38400]	lr: 6.895e-05, eta: 4:42:36, time: 1.278, data_time: 0.641, memory: 24409, decode.loss_depth: 0.0611, loss: 0.0611, grad_norm: 1.2829
2022-03-28 11:53:18,482 - depth - INFO - Iter [14500/38400]	lr: 6.876e-05, eta: 4:41:55, time: 0.639, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0613, loss: 0.0613, grad_norm: 1.4901
2022-03-28 11:53:50,648 - depth - INFO - Iter [14550/38400]	lr: 6.857e-05, eta: 4:41:14, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0607, loss: 0.0607, grad_norm: 1.1085
2022-03-28 11:54:22,819 - depth - INFO - Iter [14600/38400]	lr: 6.838e-05, eta: 4:40:34, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0610, loss: 0.0610, grad_norm: 1.6754
2022-03-28 11:54:54,844 - depth - INFO - Iter [14650/38400]	lr: 6.819e-05, eta: 4:39:53, time: 0.641, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0600, loss: 0.0600, grad_norm: 1.3556
2022-03-28 11:55:26,847 - depth - INFO - Iter [14700/38400]	lr: 6.800e-05, eta: 4:39:12, time: 0.640, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0625, loss: 0.0625, grad_norm: 2.0011
2022-03-28 11:55:59,078 - depth - INFO - Iter [14750/38400]	lr: 6.781e-05, eta: 4:38:32, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0630, loss: 0.0630, grad_norm: 1.1788
2022-03-28 11:56:31,141 - depth - INFO - Iter [14800/38400]	lr: 6.762e-05, eta: 4:37:51, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0623, loss: 0.0623, grad_norm: 1.4309
2022-03-28 11:57:03,284 - depth - INFO - Iter [14850/38400]	lr: 6.742e-05, eta: 4:37:11, time: 0.643, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0608, loss: 0.0608, grad_norm: 1.4087
2022-03-28 11:57:35,560 - depth - INFO - Iter [14900/38400]	lr: 6.723e-05, eta: 4:36:31, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0607, loss: 0.0607, grad_norm: 0.9065
2022-03-28 11:58:07,997 - depth - INFO - Iter [14950/38400]	lr: 6.704e-05, eta: 4:35:51, time: 0.649, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0592, loss: 0.0592, grad_norm: 1.3816
2022-03-28 11:58:40,167 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 11:58:40,168 - depth - INFO - Iter [15000/38400]	lr: 6.685e-05, eta: 4:35:11, time: 0.644, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0594, loss: 0.0594, grad_norm: 1.3506
2022-03-28 11:59:12,392 - depth - INFO - Iter [15050/38400]	lr: 6.666e-05, eta: 4:34:31, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0594, loss: 0.0594, grad_norm: 1.3121
2022-03-28 11:59:44,753 - depth - INFO - Iter [15100/38400]	lr: 6.646e-05, eta: 4:33:51, time: 0.647, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0592, loss: 0.0592, grad_norm: 1.3108
2022-03-28 12:00:26,796 - depth - INFO - Iter [15150/38400]	lr: 6.627e-05, eta: 4:33:26, time: 0.841, data_time: 0.169, memory: 24409, decode.loss_depth: 0.0591, loss: 0.0591, grad_norm: 1.7494
2022-03-28 12:00:58,972 - depth - INFO - Iter [15200/38400]	lr: 6.608e-05, eta: 4:32:46, time: 0.644, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0593, loss: 0.0593, grad_norm: 0.8800
2022-03-28 12:01:31,370 - depth - INFO - Summary:
2022-03-28 12:01:31,371 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9121 | 0.9885 | 0.9975 |  0.102  | 0.3508 | 0.0426 |  0.1253  | 9.7904 | 0.0529 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 12:01:31,372 - depth - INFO - Iter(val) [82]	a1: 0.9121, a2: 0.9885, a3: 0.9975, abs_rel: 0.10203906893730164, rmse: 0.3508109450340271, log_10: 0.042629487812519073, rmse_log: 0.12526114284992218, silog: 9.7904, sq_rel: 0.05292839556932449
2022-03-28 12:02:03,408 - depth - INFO - Iter [15250/38400]	lr: 6.588e-05, eta: 4:32:55, time: 1.288, data_time: 0.653, memory: 24409, decode.loss_depth: 0.0580, loss: 0.0580, grad_norm: 1.1331
2022-03-28 12:02:35,703 - depth - INFO - Iter [15300/38400]	lr: 6.569e-05, eta: 4:32:15, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0570, loss: 0.0570, grad_norm: 1.2605
2022-03-28 12:03:07,725 - depth - INFO - Iter [15350/38400]	lr: 6.549e-05, eta: 4:31:35, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0599, loss: 0.0599, grad_norm: 1.4912
2022-03-28 12:03:39,914 - depth - INFO - Iter [15400/38400]	lr: 6.530e-05, eta: 4:30:55, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0590, loss: 0.0590, grad_norm: 1.3021
2022-03-28 12:04:11,907 - depth - INFO - Iter [15450/38400]	lr: 6.510e-05, eta: 4:30:14, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0586, loss: 0.0586, grad_norm: 1.2983
2022-03-28 12:04:44,037 - depth - INFO - Iter [15500/38400]	lr: 6.491e-05, eta: 4:29:34, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0576, loss: 0.0576, grad_norm: 1.1386
2022-03-28 12:05:16,355 - depth - INFO - Iter [15550/38400]	lr: 6.471e-05, eta: 4:28:55, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0571, loss: 0.0571, grad_norm: 1.2731
2022-03-28 12:05:48,447 - depth - INFO - Iter [15600/38400]	lr: 6.452e-05, eta: 4:28:15, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0579, loss: 0.0579, grad_norm: 1.3909
2022-03-28 12:06:20,593 - depth - INFO - Iter [15650/38400]	lr: 6.432e-05, eta: 4:27:35, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0566, loss: 0.0566, grad_norm: 1.1281
2022-03-28 12:06:53,034 - depth - INFO - Iter [15700/38400]	lr: 6.413e-05, eta: 4:26:55, time: 0.649, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0577, loss: 0.0577, grad_norm: 1.1875
2022-03-28 12:07:25,280 - depth - INFO - Iter [15750/38400]	lr: 6.393e-05, eta: 4:26:16, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0560, loss: 0.0560, grad_norm: 1.0494
2022-03-28 12:07:57,441 - depth - INFO - Iter [15800/38400]	lr: 6.373e-05, eta: 4:25:36, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0564, loss: 0.0564, grad_norm: 1.2180
2022-03-28 12:08:29,525 - depth - INFO - Iter [15850/38400]	lr: 6.354e-05, eta: 4:24:56, time: 0.641, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0539, loss: 0.0539, grad_norm: 1.0837
2022-03-28 12:09:01,633 - depth - INFO - Iter [15900/38400]	lr: 6.334e-05, eta: 4:24:16, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0570, loss: 0.0570, grad_norm: 0.9740
2022-03-28 12:09:33,759 - depth - INFO - Iter [15950/38400]	lr: 6.314e-05, eta: 4:23:37, time: 0.643, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0568, loss: 0.0568, grad_norm: 0.9226
2022-03-28 12:10:05,901 - depth - INFO - Saving checkpoint at 16000 iterations
2022-03-28 12:10:18,596 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 12:10:18,596 - depth - INFO - Iter [16000/38400]	lr: 6.294e-05, eta: 4:23:15, time: 0.897, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0581, loss: 0.0581, grad_norm: 1.6754
2022-03-28 12:10:51,287 - depth - INFO - Summary:
2022-03-28 12:10:51,288 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
| 0.9114 | 0.9869 | 0.9968 |  0.1009 | 0.3455 | 0.0422 |  0.125   | 9.883 | 0.052  |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
2022-03-28 12:10:51,289 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 12:10:51,289 - depth - INFO - Iter(val) [82]	a1: 0.9114, a2: 0.9869, a3: 0.9968, abs_rel: 0.10087709128856659, rmse: 0.3455260396003723, log_10: 0.04216742143034935, rmse_log: 0.1250324547290802, silog: 9.8830, sq_rel: 0.051961205899715424
2022-03-28 12:11:23,448 - depth - INFO - Iter [16050/38400]	lr: 6.275e-05, eta: 4:23:21, time: 1.297, data_time: 0.662, memory: 24409, decode.loss_depth: 0.0581, loss: 0.0581, grad_norm: 1.3834
2022-03-28 12:11:55,520 - depth - INFO - Iter [16100/38400]	lr: 6.255e-05, eta: 4:22:41, time: 0.641, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0557, loss: 0.0557, grad_norm: 1.2943
2022-03-28 12:12:27,520 - depth - INFO - Iter [16150/38400]	lr: 6.235e-05, eta: 4:22:01, time: 0.640, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0567, loss: 0.0567, grad_norm: 0.9980
2022-03-28 12:12:59,655 - depth - INFO - Iter [16200/38400]	lr: 6.215e-05, eta: 4:21:21, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0567, loss: 0.0567, grad_norm: 1.5359
2022-03-28 12:13:31,854 - depth - INFO - Iter [16250/38400]	lr: 6.195e-05, eta: 4:20:42, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0574, loss: 0.0574, grad_norm: 1.6156
2022-03-28 12:14:03,968 - depth - INFO - Iter [16300/38400]	lr: 6.176e-05, eta: 4:20:02, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0550, loss: 0.0550, grad_norm: 1.1056
2022-03-28 12:14:36,280 - depth - INFO - Iter [16350/38400]	lr: 6.156e-05, eta: 4:19:23, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0565, loss: 0.0565, grad_norm: 0.9632
2022-03-28 12:15:08,456 - depth - INFO - Iter [16400/38400]	lr: 6.136e-05, eta: 4:18:43, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0566, loss: 0.0566, grad_norm: 1.4325
2022-03-28 12:15:40,722 - depth - INFO - Iter [16450/38400]	lr: 6.116e-05, eta: 4:18:04, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0567, loss: 0.0567, grad_norm: 1.3790
2022-03-28 12:16:12,757 - depth - INFO - Iter [16500/38400]	lr: 6.096e-05, eta: 4:17:25, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0567, loss: 0.0567, grad_norm: 1.4463
2022-03-28 12:16:44,824 - depth - INFO - Iter [16550/38400]	lr: 6.076e-05, eta: 4:16:45, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0543, loss: 0.0543, grad_norm: 0.7583
2022-03-28 12:17:16,910 - depth - INFO - Iter [16600/38400]	lr: 6.056e-05, eta: 4:16:06, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0560, loss: 0.0560, grad_norm: 1.3287
2022-03-28 12:17:48,963 - depth - INFO - Iter [16650/38400]	lr: 6.036e-05, eta: 4:15:26, time: 0.641, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0553, loss: 0.0553, grad_norm: 1.1111
2022-03-28 12:18:31,224 - depth - INFO - Iter [16700/38400]	lr: 6.016e-05, eta: 4:15:00, time: 0.845, data_time: 0.210, memory: 24409, decode.loss_depth: 0.0549, loss: 0.0549, grad_norm: 1.0696
2022-03-28 12:19:03,305 - depth - INFO - Iter [16750/38400]	lr: 5.996e-05, eta: 4:14:21, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0525, loss: 0.0525, grad_norm: 0.8518
2022-03-28 12:19:35,450 - depth - INFO - Iter [16800/38400]	lr: 5.976e-05, eta: 4:13:42, time: 0.643, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0529, loss: 0.0529, grad_norm: 0.8459
2022-03-28 12:20:08,036 - depth - INFO - Summary:
2022-03-28 12:20:08,039 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9191 | 0.9889 | 0.9976 |  0.0955 | 0.3375 | 0.0408 |  0.1209  | 9.6508 | 0.0486 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 12:20:20,279 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_16800.pth.
2022-03-28 12:20:20,280 - depth - INFO - Best abs_rel is 0.0955 at 16800 iter.
2022-03-28 12:20:20,280 - depth - INFO - Iter(val) [82]	a1: 0.9191, a2: 0.9889, a3: 0.9976, abs_rel: 0.09551362693309784, rmse: 0.3374619781970978, log_10: 0.04080021753907204, rmse_log: 0.12092398852109909, silog: 9.6508, sq_rel: 0.048576485365629196
2022-03-28 12:20:52,280 - depth - INFO - Iter [16850/38400]	lr: 5.956e-05, eta: 4:14:00, time: 1.536, data_time: 0.902, memory: 24409, decode.loss_depth: 0.0530, loss: 0.0530, grad_norm: 1.3452
2022-03-28 12:21:24,290 - depth - INFO - Iter [16900/38400]	lr: 5.936e-05, eta: 4:13:20, time: 0.641, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0548, loss: 0.0548, grad_norm: 0.9240
2022-03-28 12:21:56,357 - depth - INFO - Iter [16950/38400]	lr: 5.916e-05, eta: 4:12:40, time: 0.641, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0539, loss: 0.0539, grad_norm: 0.9928
2022-03-28 12:22:28,621 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 12:22:28,621 - depth - INFO - Iter [17000/38400]	lr: 5.895e-05, eta: 4:12:01, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0528, loss: 0.0528, grad_norm: 1.1560
2022-03-28 12:23:00,832 - depth - INFO - Iter [17050/38400]	lr: 5.875e-05, eta: 4:11:22, time: 0.644, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0536, loss: 0.0536, grad_norm: 1.0594
2022-03-28 12:23:32,817 - depth - INFO - Iter [17100/38400]	lr: 5.855e-05, eta: 4:10:42, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0534, loss: 0.0534, grad_norm: 1.1171
2022-03-28 12:24:04,886 - depth - INFO - Iter [17150/38400]	lr: 5.835e-05, eta: 4:10:03, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0529, loss: 0.0529, grad_norm: 1.0710
2022-03-28 12:24:37,122 - depth - INFO - Iter [17200/38400]	lr: 5.815e-05, eta: 4:09:24, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0528, loss: 0.0528, grad_norm: 1.2279
2022-03-28 12:25:09,208 - depth - INFO - Iter [17250/38400]	lr: 5.795e-05, eta: 4:08:45, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0538, loss: 0.0538, grad_norm: 0.9190
2022-03-28 12:25:41,212 - depth - INFO - Iter [17300/38400]	lr: 5.774e-05, eta: 4:08:06, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0528, loss: 0.0528, grad_norm: 0.9937
2022-03-28 12:26:13,509 - depth - INFO - Iter [17350/38400]	lr: 5.754e-05, eta: 4:07:27, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0532, loss: 0.0532, grad_norm: 1.3895
2022-03-28 12:26:45,782 - depth - INFO - Iter [17400/38400]	lr: 5.734e-05, eta: 4:06:48, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0525, loss: 0.0525, grad_norm: 1.8332
2022-03-28 12:27:17,836 - depth - INFO - Iter [17450/38400]	lr: 5.714e-05, eta: 4:06:09, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0529, loss: 0.0529, grad_norm: 1.2170
2022-03-28 12:27:49,988 - depth - INFO - Iter [17500/38400]	lr: 5.694e-05, eta: 4:05:30, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0525, loss: 0.0525, grad_norm: 1.1436
2022-03-28 12:28:22,206 - depth - INFO - Iter [17550/38400]	lr: 5.673e-05, eta: 4:04:51, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0523, loss: 0.0523, grad_norm: 1.0489
2022-03-28 12:28:54,267 - depth - INFO - Saving checkpoint at 17600 iterations
2022-03-28 12:29:07,203 - depth - INFO - Iter [17600/38400]	lr: 5.653e-05, eta: 4:04:27, time: 0.900, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0550, loss: 0.0550, grad_norm: 0.9181
2022-03-28 12:29:39,402 - depth - INFO - Summary:
2022-03-28 12:29:39,403 - depth - INFO - 
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1  |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.909 | 0.9876 | 0.9977 |  0.1016 | 0.3453 | 0.0427 |  0.1255  | 9.8963 | 0.0502 |
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 12:29:39,404 - depth - INFO - Iter(val) [82]	a1: 0.9090, a2: 0.9876, a3: 0.9977, abs_rel: 0.10160589963197708, rmse: 0.3452570140361786, log_10: 0.042654529213905334, rmse_log: 0.1255212277173996, silog: 9.8963, sq_rel: 0.050200797617435455
2022-03-28 12:30:11,436 - depth - INFO - Iter [17650/38400]	lr: 5.633e-05, eta: 4:04:26, time: 1.284, data_time: 0.650, memory: 24409, decode.loss_depth: 0.0543, loss: 0.0543, grad_norm: 1.3746
2022-03-28 12:30:43,575 - depth - INFO - Iter [17700/38400]	lr: 5.612e-05, eta: 4:03:47, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0535, loss: 0.0535, grad_norm: 0.9014
2022-03-28 12:31:15,845 - depth - INFO - Iter [17750/38400]	lr: 5.592e-05, eta: 4:03:08, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0551, loss: 0.0551, grad_norm: 0.9807
2022-03-28 12:31:48,069 - depth - INFO - Iter [17800/38400]	lr: 5.572e-05, eta: 4:02:29, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0536, loss: 0.0536, grad_norm: 0.8096
2022-03-28 12:32:20,439 - depth - INFO - Iter [17850/38400]	lr: 5.552e-05, eta: 4:01:50, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0534, loss: 0.0534, grad_norm: 1.2396
2022-03-28 12:32:52,933 - depth - INFO - Iter [17900/38400]	lr: 5.531e-05, eta: 4:01:12, time: 0.650, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0526, loss: 0.0526, grad_norm: 0.9859
2022-03-28 12:33:25,116 - depth - INFO - Iter [17950/38400]	lr: 5.511e-05, eta: 4:00:33, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0531, loss: 0.0531, grad_norm: 0.8644
2022-03-28 12:33:57,154 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 12:33:57,154 - depth - INFO - Iter [18000/38400]	lr: 5.490e-05, eta: 3:59:54, time: 0.641, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0513, loss: 0.0513, grad_norm: 0.9033
2022-03-28 12:34:29,360 - depth - INFO - Iter [18050/38400]	lr: 5.470e-05, eta: 3:59:15, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0510, loss: 0.0510, grad_norm: 0.9208
2022-03-28 12:35:01,481 - depth - INFO - Iter [18100/38400]	lr: 5.450e-05, eta: 3:58:36, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0524, loss: 0.0524, grad_norm: 1.3546
2022-03-28 12:35:33,632 - depth - INFO - Iter [18150/38400]	lr: 5.429e-05, eta: 3:57:58, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0519, loss: 0.0519, grad_norm: 0.9910
2022-03-28 12:36:16,265 - depth - INFO - Iter [18200/38400]	lr: 5.409e-05, eta: 3:57:31, time: 0.853, data_time: 0.168, memory: 24409, decode.loss_depth: 0.0520, loss: 0.0520, grad_norm: 0.8489
2022-03-28 12:36:48,888 - depth - INFO - Iter [18250/38400]	lr: 5.389e-05, eta: 3:56:52, time: 0.652, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0506, loss: 0.0506, grad_norm: 1.0398
2022-03-28 12:37:20,933 - depth - INFO - Iter [18300/38400]	lr: 5.368e-05, eta: 3:56:14, time: 0.642, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0517, loss: 0.0517, grad_norm: 1.0812
2022-03-28 12:37:53,183 - depth - INFO - Iter [18350/38400]	lr: 5.348e-05, eta: 3:55:35, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0505, loss: 0.0505, grad_norm: 1.1066
2022-03-28 12:38:25,590 - depth - INFO - Iter [18400/38400]	lr: 5.327e-05, eta: 3:54:57, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0522, loss: 0.0522, grad_norm: 1.2176
2022-03-28 12:38:58,865 - depth - INFO - Summary:
2022-03-28 12:38:58,866 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9137 | 0.9877 | 0.9972 |  0.1026 | 0.3469 | 0.0426 |  0.1252  | 9.6073 | 0.053  |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 12:38:58,866 - depth - INFO - Iter(val) [82]	a1: 0.9137, a2: 0.9877, a3: 0.9972, abs_rel: 0.10262620449066162, rmse: 0.34693679213523865, log_10: 0.04257158562541008, rmse_log: 0.12522822618484497, silog: 9.6073, sq_rel: 0.05298438295722008
2022-03-28 12:39:31,039 - depth - INFO - Iter [18450/38400]	lr: 5.307e-05, eta: 3:54:54, time: 1.309, data_time: 0.672, memory: 24409, decode.loss_depth: 0.0498, loss: 0.0498, grad_norm: 0.8107
2022-03-28 12:40:03,079 - depth - INFO - Iter [18500/38400]	lr: 5.287e-05, eta: 3:54:15, time: 0.641, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0508, loss: 0.0508, grad_norm: 0.9944
2022-03-28 12:40:35,334 - depth - INFO - Iter [18550/38400]	lr: 5.266e-05, eta: 3:53:37, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0499, loss: 0.0499, grad_norm: 0.8649
2022-03-28 12:41:07,491 - depth - INFO - Iter [18600/38400]	lr: 5.246e-05, eta: 3:52:58, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0494, loss: 0.0494, grad_norm: 0.8940
2022-03-28 12:41:39,451 - depth - INFO - Iter [18650/38400]	lr: 5.225e-05, eta: 3:52:19, time: 0.639, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0499, loss: 0.0499, grad_norm: 1.1545
2022-03-28 12:42:11,676 - depth - INFO - Iter [18700/38400]	lr: 5.205e-05, eta: 3:51:41, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0511, loss: 0.0511, grad_norm: 1.4068
2022-03-28 12:42:43,866 - depth - INFO - Iter [18750/38400]	lr: 5.184e-05, eta: 3:51:02, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0498, loss: 0.0498, grad_norm: 1.1248
2022-03-28 12:43:16,189 - depth - INFO - Iter [18800/38400]	lr: 5.164e-05, eta: 3:50:24, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0502, loss: 0.0502, grad_norm: 1.1343
2022-03-28 12:43:48,297 - depth - INFO - Iter [18850/38400]	lr: 5.144e-05, eta: 3:49:45, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0504, loss: 0.0504, grad_norm: 0.8543
2022-03-28 12:44:20,562 - depth - INFO - Iter [18900/38400]	lr: 5.123e-05, eta: 3:49:07, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0509, loss: 0.0509, grad_norm: 0.8716
2022-03-28 12:44:52,471 - depth - INFO - Iter [18950/38400]	lr: 5.103e-05, eta: 3:48:28, time: 0.638, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0502, loss: 0.0502, grad_norm: 1.2416
2022-03-28 12:45:24,688 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 12:45:24,689 - depth - INFO - Iter [19000/38400]	lr: 5.082e-05, eta: 3:47:50, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0518, loss: 0.0518, grad_norm: 1.4500
2022-03-28 12:45:57,194 - depth - INFO - Iter [19050/38400]	lr: 5.062e-05, eta: 3:47:12, time: 0.650, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0508, loss: 0.0508, grad_norm: 0.8322
2022-03-28 12:46:29,556 - depth - INFO - Iter [19100/38400]	lr: 5.041e-05, eta: 3:46:34, time: 0.648, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0508, loss: 0.0508, grad_norm: 1.4630
2022-03-28 12:47:01,580 - depth - INFO - Iter [19150/38400]	lr: 5.021e-05, eta: 3:45:55, time: 0.640, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0488, loss: 0.0488, grad_norm: 0.7328
2022-03-28 12:47:33,715 - depth - INFO - Saving checkpoint at 19200 iterations
2022-03-28 12:47:46,991 - depth - INFO - Iter [19200/38400]	lr: 5.000e-05, eta: 3:45:30, time: 0.909, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0514, loss: 0.0514, grad_norm: 1.4565
2022-03-28 12:48:19,870 - depth - INFO - Summary:
2022-03-28 12:48:19,871 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9183 | 0.9887 | 0.9972 |  0.0988 | 0.3395 | 0.0414 |  0.1225  | 9.5618 | 0.0506 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 12:48:19,871 - depth - INFO - Iter(val) [82]	a1: 0.9183, a2: 0.9887, a3: 0.9972, abs_rel: 0.09884839504957199, rmse: 0.33945518732070923, log_10: 0.041449837386608124, rmse_log: 0.12252100557088852, silog: 9.5618, sq_rel: 0.050648242235183716
2022-03-28 12:48:52,105 - depth - INFO - Iter [19250/38400]	lr: 4.980e-05, eta: 3:45:25, time: 1.302, data_time: 0.663, memory: 24409, decode.loss_depth: 0.0479, loss: 0.0479, grad_norm: 0.9351
2022-03-28 12:49:24,215 - depth - INFO - Iter [19300/38400]	lr: 4.960e-05, eta: 3:44:46, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0491, loss: 0.0491, grad_norm: 1.2513
2022-03-28 12:49:56,508 - depth - INFO - Iter [19350/38400]	lr: 4.939e-05, eta: 3:44:08, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0494, loss: 0.0494, grad_norm: 0.8394
2022-03-28 12:50:28,836 - depth - INFO - Iter [19400/38400]	lr: 4.919e-05, eta: 3:43:30, time: 0.646, data_time: 0.009, memory: 24409, decode.loss_depth: 0.0491, loss: 0.0491, grad_norm: 1.1218
2022-03-28 12:51:01,233 - depth - INFO - Iter [19450/38400]	lr: 4.898e-05, eta: 3:42:52, time: 0.648, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0503, loss: 0.0503, grad_norm: 0.9045
2022-03-28 12:51:33,380 - depth - INFO - Iter [19500/38400]	lr: 4.878e-05, eta: 3:42:13, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0490, loss: 0.0490, grad_norm: 0.9475
2022-03-28 12:52:05,535 - depth - INFO - Iter [19550/38400]	lr: 4.857e-05, eta: 3:41:35, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0494, loss: 0.0494, grad_norm: 0.9578
2022-03-28 12:52:37,703 - depth - INFO - Iter [19600/38400]	lr: 4.837e-05, eta: 3:40:57, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0487, loss: 0.0487, grad_norm: 0.7345
2022-03-28 12:53:10,002 - depth - INFO - Iter [19650/38400]	lr: 4.816e-05, eta: 3:40:19, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0496, loss: 0.0496, grad_norm: 1.2455
2022-03-28 12:53:51,155 - depth - INFO - Iter [19700/38400]	lr: 4.796e-05, eta: 3:39:49, time: 0.823, data_time: 0.176, memory: 24409, decode.loss_depth: 0.0479, loss: 0.0479, grad_norm: 1.0843
2022-03-28 12:54:23,407 - depth - INFO - Iter [19750/38400]	lr: 4.776e-05, eta: 3:39:11, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0476, loss: 0.0476, grad_norm: 0.6940
2022-03-28 12:54:55,795 - depth - INFO - Iter [19800/38400]	lr: 4.755e-05, eta: 3:38:33, time: 0.648, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0489, loss: 0.0489, grad_norm: 1.0258
2022-03-28 12:55:28,019 - depth - INFO - Iter [19850/38400]	lr: 4.735e-05, eta: 3:37:55, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0480, loss: 0.0480, grad_norm: 0.7561
2022-03-28 12:56:00,047 - depth - INFO - Iter [19900/38400]	lr: 4.714e-05, eta: 3:37:17, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0486, loss: 0.0486, grad_norm: 0.8364
2022-03-28 12:56:32,650 - depth - INFO - Iter [19950/38400]	lr: 4.694e-05, eta: 3:36:39, time: 0.652, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0478, loss: 0.0478, grad_norm: 0.8557
2022-03-28 12:57:04,873 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 12:57:04,873 - depth - INFO - Iter [20000/38400]	lr: 4.673e-05, eta: 3:36:01, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0480, loss: 0.0480, grad_norm: 0.9872
2022-03-28 12:57:37,737 - depth - INFO - Summary:
2022-03-28 12:57:37,738 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9167 | 0.9883 | 0.9968 |  0.0986 | 0.3397 | 0.0413 |  0.1227  | 9.7295 | 0.0503 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 12:57:37,739 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 12:57:37,739 - depth - INFO - Iter(val) [82]	a1: 0.9167, a2: 0.9883, a3: 0.9968, abs_rel: 0.09857530146837234, rmse: 0.33966928720474243, log_10: 0.04131976142525673, rmse_log: 0.12266227602958679, silog: 9.7295, sq_rel: 0.050289131700992584
2022-03-28 12:58:09,815 - depth - INFO - Iter [20050/38400]	lr: 4.653e-05, eta: 3:35:53, time: 1.298, data_time: 0.662, memory: 24409, decode.loss_depth: 0.0472, loss: 0.0472, grad_norm: 0.8197
2022-03-28 12:58:42,021 - depth - INFO - Iter [20100/38400]	lr: 4.633e-05, eta: 3:35:15, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0473, loss: 0.0473, grad_norm: 0.9027
2022-03-28 12:59:14,235 - depth - INFO - Iter [20150/38400]	lr: 4.612e-05, eta: 3:34:37, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0475, loss: 0.0475, grad_norm: 1.0517
2022-03-28 12:59:46,251 - depth - INFO - Iter [20200/38400]	lr: 4.592e-05, eta: 3:33:59, time: 0.640, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0474, loss: 0.0474, grad_norm: 0.8828
2022-03-28 13:00:18,413 - depth - INFO - Iter [20250/38400]	lr: 4.571e-05, eta: 3:33:21, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0468, loss: 0.0468, grad_norm: 1.0364
2022-03-28 13:00:50,736 - depth - INFO - Iter [20300/38400]	lr: 4.551e-05, eta: 3:32:43, time: 0.646, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0487, loss: 0.0487, grad_norm: 0.9629
2022-03-28 13:01:22,865 - depth - INFO - Iter [20350/38400]	lr: 4.531e-05, eta: 3:32:05, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0470, loss: 0.0470, grad_norm: 1.0750
2022-03-28 13:01:54,912 - depth - INFO - Iter [20400/38400]	lr: 4.510e-05, eta: 3:31:27, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0482, loss: 0.0482, grad_norm: 0.8569
2022-03-28 13:02:27,192 - depth - INFO - Iter [20450/38400]	lr: 4.490e-05, eta: 3:30:49, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0482, loss: 0.0482, grad_norm: 0.9506
2022-03-28 13:02:59,353 - depth - INFO - Iter [20500/38400]	lr: 4.470e-05, eta: 3:30:11, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0474, loss: 0.0474, grad_norm: 1.0073
2022-03-28 13:03:31,638 - depth - INFO - Iter [20550/38400]	lr: 4.449e-05, eta: 3:29:33, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0474, loss: 0.0474, grad_norm: 0.6798
2022-03-28 13:04:03,858 - depth - INFO - Iter [20600/38400]	lr: 4.429e-05, eta: 3:28:55, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0476, loss: 0.0476, grad_norm: 0.8024
2022-03-28 13:04:36,327 - depth - INFO - Iter [20650/38400]	lr: 4.409e-05, eta: 3:28:18, time: 0.649, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0459, loss: 0.0459, grad_norm: 0.9203
2022-03-28 13:05:08,470 - depth - INFO - Iter [20700/38400]	lr: 4.388e-05, eta: 3:27:40, time: 0.643, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0476, loss: 0.0476, grad_norm: 0.9135
2022-03-28 13:05:40,602 - depth - INFO - Iter [20750/38400]	lr: 4.368e-05, eta: 3:27:02, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0466, loss: 0.0466, grad_norm: 0.9129
2022-03-28 13:06:12,675 - depth - INFO - Saving checkpoint at 20800 iterations
2022-03-28 13:06:25,700 - depth - INFO - Iter [20800/38400]	lr: 4.348e-05, eta: 3:26:35, time: 0.902, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0463, loss: 0.0463, grad_norm: 0.8880
2022-03-28 13:06:57,789 - depth - INFO - Summary:
2022-03-28 13:06:57,792 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9201 | 0.9888 | 0.9976 |  0.0958 | 0.3335 | 0.0406 |  0.1208  | 9.5856 | 0.0472 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 13:06:57,795 - depth - INFO - Iter(val) [82]	a1: 0.9201, a2: 0.9888, a3: 0.9976, abs_rel: 0.0957745909690857, rmse: 0.333484947681427, log_10: 0.04057109355926514, rmse_log: 0.1207808256149292, silog: 9.5856, sq_rel: 0.04716552793979645
2022-03-28 13:07:30,033 - depth - INFO - Iter [20850/38400]	lr: 4.328e-05, eta: 3:26:25, time: 1.286, data_time: 0.649, memory: 24409, decode.loss_depth: 0.0466, loss: 0.0466, grad_norm: 0.8955
2022-03-28 13:08:02,373 - depth - INFO - Iter [20900/38400]	lr: 4.307e-05, eta: 3:25:47, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0453, loss: 0.0453, grad_norm: 0.8212
2022-03-28 13:08:34,618 - depth - INFO - Iter [20950/38400]	lr: 4.287e-05, eta: 3:25:09, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0467, loss: 0.0467, grad_norm: 1.1350
2022-03-28 13:09:06,877 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 13:09:06,877 - depth - INFO - Iter [21000/38400]	lr: 4.267e-05, eta: 3:24:31, time: 0.645, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0459, loss: 0.0459, grad_norm: 0.7761
2022-03-28 13:09:39,056 - depth - INFO - Iter [21050/38400]	lr: 4.247e-05, eta: 3:23:53, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0469, loss: 0.0469, grad_norm: 1.1518
2022-03-28 13:10:11,234 - depth - INFO - Iter [21100/38400]	lr: 4.226e-05, eta: 3:23:16, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0474, loss: 0.0474, grad_norm: 0.8197
2022-03-28 13:10:43,571 - depth - INFO - Iter [21150/38400]	lr: 4.206e-05, eta: 3:22:38, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0458, loss: 0.0458, grad_norm: 1.0292
2022-03-28 13:11:25,792 - depth - INFO - Iter [21200/38400]	lr: 4.186e-05, eta: 3:22:08, time: 0.844, data_time: 0.168, memory: 24409, decode.loss_depth: 0.0454, loss: 0.0454, grad_norm: 0.7467
2022-03-28 13:11:57,984 - depth - INFO - Iter [21250/38400]	lr: 4.166e-05, eta: 3:21:31, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0459, loss: 0.0459, grad_norm: 0.9027
2022-03-28 13:12:30,508 - depth - INFO - Iter [21300/38400]	lr: 4.146e-05, eta: 3:20:53, time: 0.650, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0465, loss: 0.0465, grad_norm: 1.0322
2022-03-28 13:13:02,720 - depth - INFO - Iter [21350/38400]	lr: 4.125e-05, eta: 3:20:16, time: 0.644, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0454, loss: 0.0454, grad_norm: 0.9771
2022-03-28 13:13:35,179 - depth - INFO - Iter [21400/38400]	lr: 4.105e-05, eta: 3:19:38, time: 0.649, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0449, loss: 0.0449, grad_norm: 0.8943
2022-03-28 13:14:07,507 - depth - INFO - Iter [21450/38400]	lr: 4.085e-05, eta: 3:19:01, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0495, loss: 0.0495, grad_norm: 0.8886
2022-03-28 13:14:39,613 - depth - INFO - Iter [21500/38400]	lr: 4.065e-05, eta: 3:18:23, time: 0.642, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0476, loss: 0.0476, grad_norm: 0.7678
2022-03-28 13:15:11,910 - depth - INFO - Iter [21550/38400]	lr: 4.045e-05, eta: 3:17:45, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0454, loss: 0.0454, grad_norm: 0.9435
2022-03-28 13:15:44,229 - depth - INFO - Iter [21600/38400]	lr: 4.025e-05, eta: 3:17:08, time: 0.646, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0447, loss: 0.0447, grad_norm: 0.6265
2022-03-28 13:16:17,779 - depth - INFO - Summary:
2022-03-28 13:16:17,781 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9084 | 0.9873 | 0.9973 |  0.1063 | 0.3506 | 0.0437 |  0.1279  | 9.6665 | 0.0548 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 13:16:17,782 - depth - INFO - Iter(val) [82]	a1: 0.9084, a2: 0.9873, a3: 0.9973, abs_rel: 0.10625533759593964, rmse: 0.35062167048454285, log_10: 0.04371797665953636, rmse_log: 0.12787288427352905, silog: 9.6665, sq_rel: 0.05480331927537918
2022-03-28 13:16:49,946 - depth - INFO - Iter [21650/38400]	lr: 4.005e-05, eta: 3:16:56, time: 1.314, data_time: 0.677, memory: 24409, decode.loss_depth: 0.0466, loss: 0.0466, grad_norm: 0.9838
2022-03-28 13:17:22,348 - depth - INFO - Iter [21700/38400]	lr: 3.985e-05, eta: 3:16:19, time: 0.649, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0465, loss: 0.0465, grad_norm: 0.9963
2022-03-28 13:17:54,497 - depth - INFO - Iter [21750/38400]	lr: 3.965e-05, eta: 3:15:41, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0447, loss: 0.0447, grad_norm: 1.0799
2022-03-28 13:18:26,569 - depth - INFO - Iter [21800/38400]	lr: 3.945e-05, eta: 3:15:04, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0447, loss: 0.0447, grad_norm: 0.9930
2022-03-28 13:18:58,832 - depth - INFO - Iter [21850/38400]	lr: 3.925e-05, eta: 3:14:26, time: 0.645, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0429, loss: 0.0429, grad_norm: 1.0753
2022-03-28 13:19:31,115 - depth - INFO - Iter [21900/38400]	lr: 3.905e-05, eta: 3:13:49, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0450, loss: 0.0450, grad_norm: 0.8423
2022-03-28 13:20:03,342 - depth - INFO - Iter [21950/38400]	lr: 3.885e-05, eta: 3:13:11, time: 0.644, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0451, loss: 0.0451, grad_norm: 0.7176
2022-03-28 13:20:35,685 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 13:20:35,685 - depth - INFO - Iter [22000/38400]	lr: 3.865e-05, eta: 3:12:34, time: 0.647, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0449, loss: 0.0449, grad_norm: 0.8192
2022-03-28 13:21:07,969 - depth - INFO - Iter [22050/38400]	lr: 3.845e-05, eta: 3:11:56, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0448, loss: 0.0448, grad_norm: 1.0566
2022-03-28 13:21:40,353 - depth - INFO - Iter [22100/38400]	lr: 3.825e-05, eta: 3:11:19, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0449, loss: 0.0449, grad_norm: 1.0133
2022-03-28 13:22:12,637 - depth - INFO - Iter [22150/38400]	lr: 3.805e-05, eta: 3:10:42, time: 0.646, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0444, loss: 0.0444, grad_norm: 0.7574
2022-03-28 13:22:44,624 - depth - INFO - Iter [22200/38400]	lr: 3.785e-05, eta: 3:10:04, time: 0.640, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0444, loss: 0.0444, grad_norm: 0.7353
2022-03-28 13:23:16,814 - depth - INFO - Iter [22250/38400]	lr: 3.766e-05, eta: 3:09:27, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0443, loss: 0.0443, grad_norm: 0.8724
2022-03-28 13:23:48,878 - depth - INFO - Iter [22300/38400]	lr: 3.746e-05, eta: 3:08:49, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0448, loss: 0.0448, grad_norm: 0.6591
2022-03-28 13:24:21,136 - depth - INFO - Iter [22350/38400]	lr: 3.726e-05, eta: 3:08:12, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0457, loss: 0.0457, grad_norm: 1.0828
2022-03-28 13:24:53,390 - depth - INFO - Saving checkpoint at 22400 iterations
2022-03-28 13:25:06,752 - depth - INFO - Iter [22400/38400]	lr: 3.706e-05, eta: 3:07:44, time: 0.913, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0448, loss: 0.0448, grad_norm: 0.7911
2022-03-28 13:25:39,657 - depth - INFO - Summary:
2022-03-28 13:25:39,658 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9196 | 0.9886 | 0.9978 |  0.0943 | 0.3348 | 0.0405 |  0.1202  | 9.5187 | 0.0463 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 13:25:52,452 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_22400.pth.
2022-03-28 13:25:52,453 - depth - INFO - Best abs_rel is 0.0943 at 22400 iter.
2022-03-28 13:25:52,453 - depth - INFO - Iter(val) [82]	a1: 0.9196, a2: 0.9886, a3: 0.9978, abs_rel: 0.0942656621336937, rmse: 0.33477839827537537, log_10: 0.040523868054151535, rmse_log: 0.12016966193914413, silog: 9.5187, sq_rel: 0.04628927260637283
2022-03-28 13:26:24,822 - depth - INFO - Iter [22450/38400]	lr: 3.687e-05, eta: 3:07:39, time: 1.561, data_time: 0.919, memory: 24409, decode.loss_depth: 0.0441, loss: 0.0441, grad_norm: 1.0055
2022-03-28 13:26:57,373 - depth - INFO - Iter [22500/38400]	lr: 3.667e-05, eta: 3:07:02, time: 0.651, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0439, loss: 0.0439, grad_norm: 0.8440
2022-03-28 13:27:29,566 - depth - INFO - Iter [22550/38400]	lr: 3.647e-05, eta: 3:06:25, time: 0.643, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0438, loss: 0.0438, grad_norm: 0.8871
2022-03-28 13:28:01,674 - depth - INFO - Iter [22600/38400]	lr: 3.627e-05, eta: 3:05:47, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0447, loss: 0.0447, grad_norm: 1.0096
2022-03-28 13:28:33,957 - depth - INFO - Iter [22650/38400]	lr: 3.608e-05, eta: 3:05:10, time: 0.646, data_time: 0.009, memory: 24409, decode.loss_depth: 0.0441, loss: 0.0441, grad_norm: 1.1706
2022-03-28 13:29:05,856 - depth - INFO - Iter [22700/38400]	lr: 3.588e-05, eta: 3:04:32, time: 0.638, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0451, loss: 0.0451, grad_norm: 0.6864
2022-03-28 13:29:47,579 - depth - INFO - Iter [22750/38400]	lr: 3.569e-05, eta: 3:04:01, time: 0.834, data_time: 0.188, memory: 24409, decode.loss_depth: 0.0431, loss: 0.0431, grad_norm: 0.7977
2022-03-28 13:30:20,190 - depth - INFO - Iter [22800/38400]	lr: 3.549e-05, eta: 3:03:24, time: 0.652, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0430, loss: 0.0430, grad_norm: 1.0440
2022-03-28 13:30:52,346 - depth - INFO - Iter [22850/38400]	lr: 3.529e-05, eta: 3:02:47, time: 0.644, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0433, loss: 0.0433, grad_norm: 0.6030
2022-03-28 13:31:24,462 - depth - INFO - Iter [22900/38400]	lr: 3.510e-05, eta: 3:02:10, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0438, loss: 0.0438, grad_norm: 1.1118
2022-03-28 13:31:56,552 - depth - INFO - Iter [22950/38400]	lr: 3.490e-05, eta: 3:01:32, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0430, loss: 0.0430, grad_norm: 1.1796
2022-03-28 13:32:28,859 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 13:32:28,860 - depth - INFO - Iter [23000/38400]	lr: 3.471e-05, eta: 3:00:55, time: 0.646, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0431, loss: 0.0431, grad_norm: 0.8889
2022-03-28 13:33:00,809 - depth - INFO - Iter [23050/38400]	lr: 3.451e-05, eta: 3:00:18, time: 0.639, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0441, loss: 0.0441, grad_norm: 0.7559
2022-03-28 13:33:32,905 - depth - INFO - Iter [23100/38400]	lr: 3.432e-05, eta: 2:59:40, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0432, loss: 0.0432, grad_norm: 0.7642
2022-03-28 13:34:05,497 - depth - INFO - Iter [23150/38400]	lr: 3.413e-05, eta: 2:59:03, time: 0.652, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0420, loss: 0.0420, grad_norm: 0.7279
2022-03-28 13:34:37,734 - depth - INFO - Iter [23200/38400]	lr: 3.393e-05, eta: 2:58:26, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0432, loss: 0.0432, grad_norm: 0.9387
2022-03-28 13:35:11,001 - depth - INFO - Summary:
2022-03-28 13:35:11,002 - depth - INFO - 
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
| 0.9184 | 0.9893 | 0.9978 |  0.0967 | 0.335 | 0.0407 |  0.1207  | 9.4878 | 0.048  |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
2022-03-28 13:35:11,002 - depth - INFO - Iter(val) [82]	a1: 0.9184, a2: 0.9893, a3: 0.9978, abs_rel: 0.09670025110244751, rmse: 0.33502840995788574, log_10: 0.040677860379219055, rmse_log: 0.12065518647432327, silog: 9.4878, sq_rel: 0.0480206161737442
2022-03-28 13:35:43,099 - depth - INFO - Iter [23250/38400]	lr: 3.374e-05, eta: 2:58:10, time: 1.308, data_time: 0.671, memory: 24409, decode.loss_depth: 0.0432, loss: 0.0432, grad_norm: 0.8481
2022-03-28 13:36:15,539 - depth - INFO - Iter [23300/38400]	lr: 3.355e-05, eta: 2:57:33, time: 0.648, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0429, loss: 0.0429, grad_norm: 0.5965
2022-03-28 13:36:47,864 - depth - INFO - Iter [23350/38400]	lr: 3.335e-05, eta: 2:56:56, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0432, loss: 0.0432, grad_norm: 0.9479
2022-03-28 13:37:20,035 - depth - INFO - Iter [23400/38400]	lr: 3.316e-05, eta: 2:56:19, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0417, loss: 0.0417, grad_norm: 0.6383
2022-03-28 13:37:52,114 - depth - INFO - Iter [23450/38400]	lr: 3.297e-05, eta: 2:55:42, time: 0.641, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0424, loss: 0.0424, grad_norm: 0.8854
2022-03-28 13:38:24,355 - depth - INFO - Iter [23500/38400]	lr: 3.277e-05, eta: 2:55:04, time: 0.645, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0430, loss: 0.0430, grad_norm: 1.0146
2022-03-28 13:38:56,677 - depth - INFO - Iter [23550/38400]	lr: 3.258e-05, eta: 2:54:27, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0421, loss: 0.0421, grad_norm: 0.6235
2022-03-28 13:39:28,834 - depth - INFO - Iter [23600/38400]	lr: 3.239e-05, eta: 2:53:50, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0423, loss: 0.0423, grad_norm: 0.8587
2022-03-28 13:40:01,155 - depth - INFO - Iter [23650/38400]	lr: 3.220e-05, eta: 2:53:13, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0424, loss: 0.0424, grad_norm: 0.7470
2022-03-28 13:40:33,399 - depth - INFO - Iter [23700/38400]	lr: 3.201e-05, eta: 2:52:36, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0425, loss: 0.0425, grad_norm: 0.6602
2022-03-28 13:41:05,646 - depth - INFO - Iter [23750/38400]	lr: 3.182e-05, eta: 2:51:59, time: 0.645, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0423, loss: 0.0423, grad_norm: 0.7465
2022-03-28 13:41:37,932 - depth - INFO - Iter [23800/38400]	lr: 3.163e-05, eta: 2:51:22, time: 0.646, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0423, loss: 0.0423, grad_norm: 1.2617
2022-03-28 13:42:10,160 - depth - INFO - Iter [23850/38400]	lr: 3.144e-05, eta: 2:50:45, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0421, loss: 0.0421, grad_norm: 0.9528
2022-03-28 13:42:42,396 - depth - INFO - Iter [23900/38400]	lr: 3.125e-05, eta: 2:50:08, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0422, loss: 0.0422, grad_norm: 0.7419
2022-03-28 13:43:14,771 - depth - INFO - Iter [23950/38400]	lr: 3.106e-05, eta: 2:49:31, time: 0.648, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0417, loss: 0.0417, grad_norm: 0.7734
2022-03-28 13:43:47,076 - depth - INFO - Saving checkpoint at 24000 iterations
2022-03-28 13:44:00,745 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 13:44:00,746 - depth - INFO - Iter [24000/38400]	lr: 3.087e-05, eta: 2:49:02, time: 0.920, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0423, loss: 0.0423, grad_norm: 0.8415
2022-03-28 13:44:33,433 - depth - INFO - Summary:
2022-03-28 13:44:33,434 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9193 | 0.9887 | 0.9975 |  0.0956 | 0.3346 | 0.0405 |  0.1204  | 9.5071 | 0.0478 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 13:44:33,436 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 13:44:33,436 - depth - INFO - Iter(val) [82]	a1: 0.9193, a2: 0.9887, a3: 0.9975, abs_rel: 0.09557349979877472, rmse: 0.33462968468666077, log_10: 0.04053265228867531, rmse_log: 0.12035409361124039, silog: 9.5071, sq_rel: 0.04784331098198891
2022-03-28 13:45:05,450 - depth - INFO - Iter [24050/38400]	lr: 3.068e-05, eta: 2:48:45, time: 1.294, data_time: 0.659, memory: 24409, decode.loss_depth: 0.0428, loss: 0.0428, grad_norm: 0.9718
2022-03-28 13:45:37,506 - depth - INFO - Iter [24100/38400]	lr: 3.049e-05, eta: 2:48:07, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0423, loss: 0.0423, grad_norm: 0.8688
2022-03-28 13:46:09,675 - depth - INFO - Iter [24150/38400]	lr: 3.030e-05, eta: 2:47:30, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0423, loss: 0.0423, grad_norm: 0.7136
2022-03-28 13:46:41,773 - depth - INFO - Iter [24200/38400]	lr: 3.012e-05, eta: 2:46:53, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0412, loss: 0.0412, grad_norm: 0.6297
2022-03-28 13:47:23,946 - depth - INFO - Iter [24250/38400]	lr: 2.993e-05, eta: 2:46:22, time: 0.844, data_time: 0.206, memory: 24409, decode.loss_depth: 0.0415, loss: 0.0415, grad_norm: 1.0809
2022-03-28 13:47:56,381 - depth - INFO - Iter [24300/38400]	lr: 2.974e-05, eta: 2:45:45, time: 0.649, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0403, loss: 0.0403, grad_norm: 0.7115
2022-03-28 13:48:28,284 - depth - INFO - Iter [24350/38400]	lr: 2.955e-05, eta: 2:45:08, time: 0.638, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0418, loss: 0.0418, grad_norm: 0.6914
2022-03-28 13:49:00,361 - depth - INFO - Iter [24400/38400]	lr: 2.937e-05, eta: 2:44:31, time: 0.641, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0411, loss: 0.0411, grad_norm: 0.9365
2022-03-28 13:49:32,642 - depth - INFO - Iter [24450/38400]	lr: 2.918e-05, eta: 2:43:54, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0421, loss: 0.0421, grad_norm: 0.6349
2022-03-28 13:50:04,880 - depth - INFO - Iter [24500/38400]	lr: 2.900e-05, eta: 2:43:17, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0420, loss: 0.0420, grad_norm: 0.9528
2022-03-28 13:50:37,130 - depth - INFO - Iter [24550/38400]	lr: 2.881e-05, eta: 2:42:40, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0411, loss: 0.0411, grad_norm: 0.6613
2022-03-28 13:51:09,018 - depth - INFO - Iter [24600/38400]	lr: 2.863e-05, eta: 2:42:03, time: 0.638, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0409, loss: 0.0409, grad_norm: 0.7742
2022-03-28 13:51:41,199 - depth - INFO - Iter [24650/38400]	lr: 2.844e-05, eta: 2:41:26, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0409, loss: 0.0409, grad_norm: 0.6933
2022-03-28 13:52:13,075 - depth - INFO - Iter [24700/38400]	lr: 2.826e-05, eta: 2:40:49, time: 0.638, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0405, loss: 0.0405, grad_norm: 0.9885
2022-03-28 13:52:45,168 - depth - INFO - Iter [24750/38400]	lr: 2.807e-05, eta: 2:40:12, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0422, loss: 0.0422, grad_norm: 0.6596
2022-03-28 13:53:17,338 - depth - INFO - Iter [24800/38400]	lr: 2.789e-05, eta: 2:39:35, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0407, loss: 0.0407, grad_norm: 0.8629
2022-03-28 13:53:49,256 - depth - INFO - Summary:
2022-03-28 13:53:49,259 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9198 | 0.9892 | 0.9977 |  0.096  | 0.3339 | 0.0406 |  0.1202  | 9.4501 | 0.0475 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 13:53:49,260 - depth - INFO - Iter(val) [82]	a1: 0.9198, a2: 0.9892, a3: 0.9977, abs_rel: 0.0960143581032753, rmse: 0.3338824212551117, log_10: 0.040604736655950546, rmse_log: 0.12023008614778519, silog: 9.4501, sq_rel: 0.04753115400671959
2022-03-28 13:54:21,576 - depth - INFO - Iter [24850/38400]	lr: 2.771e-05, eta: 2:39:16, time: 1.285, data_time: 0.644, memory: 24409, decode.loss_depth: 0.0420, loss: 0.0420, grad_norm: 0.8065
2022-03-28 13:54:53,816 - depth - INFO - Iter [24900/38400]	lr: 2.752e-05, eta: 2:38:39, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0408, loss: 0.0408, grad_norm: 0.8367
2022-03-28 13:55:25,916 - depth - INFO - Iter [24950/38400]	lr: 2.734e-05, eta: 2:38:02, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0406, loss: 0.0406, grad_norm: 0.7306
2022-03-28 13:55:58,019 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 13:55:58,020 - depth - INFO - Iter [25000/38400]	lr: 2.716e-05, eta: 2:37:25, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0418, loss: 0.0418, grad_norm: 0.7235
2022-03-28 13:56:30,167 - depth - INFO - Iter [25050/38400]	lr: 2.698e-05, eta: 2:36:48, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0406, loss: 0.0406, grad_norm: 0.6497
2022-03-28 13:57:02,502 - depth - INFO - Iter [25100/38400]	lr: 2.680e-05, eta: 2:36:11, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0407, loss: 0.0407, grad_norm: 0.9970
2022-03-28 13:57:34,552 - depth - INFO - Iter [25150/38400]	lr: 2.661e-05, eta: 2:35:34, time: 0.641, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0404, loss: 0.0404, grad_norm: 0.7942
2022-03-28 13:58:06,543 - depth - INFO - Iter [25200/38400]	lr: 2.643e-05, eta: 2:34:57, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0411, loss: 0.0411, grad_norm: 0.6613
2022-03-28 13:58:38,793 - depth - INFO - Iter [25250/38400]	lr: 2.625e-05, eta: 2:34:21, time: 0.645, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0399, loss: 0.0399, grad_norm: 0.7575
2022-03-28 13:59:11,098 - depth - INFO - Iter [25300/38400]	lr: 2.607e-05, eta: 2:33:44, time: 0.646, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0412, loss: 0.0412, grad_norm: 0.7188
2022-03-28 13:59:43,086 - depth - INFO - Iter [25350/38400]	lr: 2.589e-05, eta: 2:33:07, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0398, loss: 0.0398, grad_norm: 0.8886
2022-03-28 14:00:15,454 - depth - INFO - Iter [25400/38400]	lr: 2.572e-05, eta: 2:32:30, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0397, loss: 0.0397, grad_norm: 0.8120
2022-03-28 14:00:47,641 - depth - INFO - Iter [25450/38400]	lr: 2.554e-05, eta: 2:31:54, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0405, loss: 0.0405, grad_norm: 0.6652
2022-03-28 14:01:19,995 - depth - INFO - Iter [25500/38400]	lr: 2.536e-05, eta: 2:31:17, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0398, loss: 0.0398, grad_norm: 0.6593
2022-03-28 14:01:51,994 - depth - INFO - Iter [25550/38400]	lr: 2.518e-05, eta: 2:30:40, time: 0.641, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0410, loss: 0.0410, grad_norm: 0.7122
2022-03-28 14:02:24,114 - depth - INFO - Saving checkpoint at 25600 iterations
2022-03-28 14:02:36,716 - depth - INFO - Iter [25600/38400]	lr: 2.500e-05, eta: 2:30:10, time: 0.895, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0415, loss: 0.0415, grad_norm: 0.6365
2022-03-28 14:03:09,675 - depth - INFO - Summary:
2022-03-28 14:03:09,681 - depth - INFO - 
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1  |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.921 | 0.9895 | 0.9974 |  0.0943 | 0.3317 | 0.0401 |  0.1192  | 9.4372 | 0.047  |
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 14:03:09,682 - depth - INFO - Iter(val) [82]	a1: 0.9210, a2: 0.9895, a3: 0.9974, abs_rel: 0.09432613104581833, rmse: 0.33170202374458313, log_10: 0.04012314975261688, rmse_log: 0.11915171891450882, silog: 9.4372, sq_rel: 0.04696575924754143
2022-03-28 14:03:41,762 - depth - INFO - Iter [25650/38400]	lr: 2.483e-05, eta: 2:29:49, time: 1.300, data_time: 0.664, memory: 24409, decode.loss_depth: 0.0403, loss: 0.0403, grad_norm: 0.6758
2022-03-28 14:04:13,712 - depth - INFO - Iter [25700/38400]	lr: 2.465e-05, eta: 2:29:13, time: 0.639, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0403, loss: 0.0403, grad_norm: 0.6989
2022-03-28 14:04:56,168 - depth - INFO - Iter [25750/38400]	lr: 2.447e-05, eta: 2:28:41, time: 0.849, data_time: 0.208, memory: 24409, decode.loss_depth: 0.0406, loss: 0.0406, grad_norm: 0.8457
2022-03-28 14:05:28,588 - depth - INFO - Iter [25800/38400]	lr: 2.430e-05, eta: 2:28:04, time: 0.649, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0399, loss: 0.0399, grad_norm: 0.9577
2022-03-28 14:06:00,946 - depth - INFO - Iter [25850/38400]	lr: 2.412e-05, eta: 2:27:27, time: 0.647, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0388, loss: 0.0388, grad_norm: 0.7753
2022-03-28 14:06:33,139 - depth - INFO - Iter [25900/38400]	lr: 2.395e-05, eta: 2:26:51, time: 0.644, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0406, loss: 0.0406, grad_norm: 1.1540
2022-03-28 14:07:05,439 - depth - INFO - Iter [25950/38400]	lr: 2.377e-05, eta: 2:26:14, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0396, loss: 0.0396, grad_norm: 0.8117
2022-03-28 14:07:37,636 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 14:07:37,636 - depth - INFO - Iter [26000/38400]	lr: 2.360e-05, eta: 2:25:37, time: 0.644, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0389, loss: 0.0389, grad_norm: 0.5819
2022-03-28 14:08:09,767 - depth - INFO - Iter [26050/38400]	lr: 2.343e-05, eta: 2:25:01, time: 0.643, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0398, loss: 0.0398, grad_norm: 0.9035
2022-03-28 14:08:41,893 - depth - INFO - Iter [26100/38400]	lr: 2.325e-05, eta: 2:24:24, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0387, loss: 0.0387, grad_norm: 0.4656
2022-03-28 14:09:13,912 - depth - INFO - Iter [26150/38400]	lr: 2.308e-05, eta: 2:23:47, time: 0.640, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0394, loss: 0.0394, grad_norm: 0.6886
2022-03-28 14:09:46,124 - depth - INFO - Iter [26200/38400]	lr: 2.291e-05, eta: 2:23:11, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0400, loss: 0.0400, grad_norm: 0.7206
2022-03-28 14:10:18,476 - depth - INFO - Iter [26250/38400]	lr: 2.274e-05, eta: 2:22:34, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0392, loss: 0.0392, grad_norm: 0.5523
2022-03-28 14:10:50,441 - depth - INFO - Iter [26300/38400]	lr: 2.257e-05, eta: 2:21:57, time: 0.639, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0399, loss: 0.0399, grad_norm: 0.9243
2022-03-28 14:11:22,946 - depth - INFO - Iter [26350/38400]	lr: 2.240e-05, eta: 2:21:21, time: 0.650, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0385, loss: 0.0385, grad_norm: 0.7931
2022-03-28 14:11:55,244 - depth - INFO - Iter [26400/38400]	lr: 2.222e-05, eta: 2:20:45, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0398, loss: 0.0398, grad_norm: 0.6532
2022-03-28 14:12:28,821 - depth - INFO - Summary:
2022-03-28 14:12:28,823 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9223 | 0.9895 | 0.9974 |  0.0948 | 0.3321 | 0.0401 |  0.1193  | 9.4198 | 0.0477 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 14:12:28,824 - depth - INFO - Iter(val) [82]	a1: 0.9223, a2: 0.9895, a3: 0.9974, abs_rel: 0.09480748325586319, rmse: 0.3320542573928833, log_10: 0.040147170424461365, rmse_log: 0.11926603317260742, silog: 9.4198, sq_rel: 0.04769847169518471
2022-03-28 14:13:01,214 - depth - INFO - Iter [26450/38400]	lr: 2.206e-05, eta: 2:20:23, time: 1.319, data_time: 0.679, memory: 24409, decode.loss_depth: 0.0391, loss: 0.0391, grad_norm: 0.6346
2022-03-28 14:13:33,608 - depth - INFO - Iter [26500/38400]	lr: 2.189e-05, eta: 2:19:47, time: 0.648, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0398, loss: 0.0398, grad_norm: 0.6399
2022-03-28 14:14:05,441 - depth - INFO - Iter [26550/38400]	lr: 2.172e-05, eta: 2:19:10, time: 0.637, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0384, loss: 0.0384, grad_norm: 0.6715
2022-03-28 14:14:37,615 - depth - INFO - Iter [26600/38400]	lr: 2.155e-05, eta: 2:18:33, time: 0.644, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0394, loss: 0.0394, grad_norm: 0.8320
2022-03-28 14:15:09,941 - depth - INFO - Iter [26650/38400]	lr: 2.138e-05, eta: 2:17:57, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0394, loss: 0.0394, grad_norm: 0.6153
2022-03-28 14:15:41,981 - depth - INFO - Iter [26700/38400]	lr: 2.121e-05, eta: 2:17:20, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0393, loss: 0.0393, grad_norm: 0.5822
2022-03-28 14:16:14,037 - depth - INFO - Iter [26750/38400]	lr: 2.105e-05, eta: 2:16:44, time: 0.641, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0393, loss: 0.0393, grad_norm: 0.7667
2022-03-28 14:16:46,513 - depth - INFO - Iter [26800/38400]	lr: 2.088e-05, eta: 2:16:07, time: 0.650, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0385, loss: 0.0385, grad_norm: 0.6633
2022-03-28 14:17:18,845 - depth - INFO - Iter [26850/38400]	lr: 2.071e-05, eta: 2:15:31, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0404, loss: 0.0404, grad_norm: 0.7591
2022-03-28 14:17:50,948 - depth - INFO - Iter [26900/38400]	lr: 2.055e-05, eta: 2:14:54, time: 0.642, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0381, loss: 0.0381, grad_norm: 0.5707
2022-03-28 14:18:23,220 - depth - INFO - Iter [26950/38400]	lr: 2.038e-05, eta: 2:14:18, time: 0.646, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0390, loss: 0.0390, grad_norm: 0.6458
2022-03-28 14:18:55,327 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 14:18:55,327 - depth - INFO - Iter [27000/38400]	lr: 2.022e-05, eta: 2:13:41, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0389, loss: 0.0389, grad_norm: 0.9318
2022-03-28 14:19:27,418 - depth - INFO - Iter [27050/38400]	lr: 2.005e-05, eta: 2:13:05, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0393, loss: 0.0393, grad_norm: 0.7546
2022-03-28 14:19:59,626 - depth - INFO - Iter [27100/38400]	lr: 1.989e-05, eta: 2:12:28, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0395, loss: 0.0395, grad_norm: 0.7286
2022-03-28 14:20:31,855 - depth - INFO - Iter [27150/38400]	lr: 1.973e-05, eta: 2:11:52, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0385, loss: 0.0385, grad_norm: 0.5145
2022-03-28 14:21:03,902 - depth - INFO - Saving checkpoint at 27200 iterations
2022-03-28 14:21:17,083 - depth - INFO - Iter [27200/38400]	lr: 1.957e-05, eta: 2:11:21, time: 0.905, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0387, loss: 0.0387, grad_norm: 0.7763
2022-03-28 14:21:49,374 - depth - INFO - Summary:
2022-03-28 14:21:49,375 - depth - INFO - 
+------+--------+--------+---------+-------+--------+----------+--------+--------+
|  a1  |   a2   |   a3   | abs_rel |  rmse | log_10 | rmse_log | silog  | sq_rel |
+------+--------+--------+---------+-------+--------+----------+--------+--------+
| 0.92 | 0.9891 | 0.9974 |  0.0964 | 0.334 | 0.0406 |  0.1204  | 9.4411 | 0.0484 |
+------+--------+--------+---------+-------+--------+----------+--------+--------+
2022-03-28 14:21:49,376 - depth - INFO - Iter(val) [82]	a1: 0.9200, a2: 0.9891, a3: 0.9974, abs_rel: 0.09643939882516861, rmse: 0.3340267241001129, log_10: 0.04056868702173233, rmse_log: 0.12037955224514008, silog: 9.4411, sq_rel: 0.048447079956531525
2022-03-28 14:22:21,379 - depth - INFO - Iter [27250/38400]	lr: 1.940e-05, eta: 2:10:58, time: 1.286, data_time: 0.652, memory: 24409, decode.loss_depth: 0.0382, loss: 0.0382, grad_norm: 0.5049
2022-03-28 14:23:04,497 - depth - INFO - Iter [27300/38400]	lr: 1.924e-05, eta: 2:10:26, time: 0.863, data_time: 0.171, memory: 24409, decode.loss_depth: 0.0391, loss: 0.0391, grad_norm: 0.7830
2022-03-28 14:23:36,691 - depth - INFO - Iter [27350/38400]	lr: 1.908e-05, eta: 2:09:49, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0379, loss: 0.0379, grad_norm: 0.6785
2022-03-28 14:24:08,932 - depth - INFO - Iter [27400/38400]	lr: 1.892e-05, eta: 2:09:13, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0381, loss: 0.0381, grad_norm: 0.7094
2022-03-28 14:24:41,346 - depth - INFO - Iter [27450/38400]	lr: 1.876e-05, eta: 2:08:36, time: 0.649, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0383, loss: 0.0383, grad_norm: 0.5757
2022-03-28 14:25:13,850 - depth - INFO - Iter [27500/38400]	lr: 1.860e-05, eta: 2:08:00, time: 0.651, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0378, loss: 0.0378, grad_norm: 0.4877
2022-03-28 14:25:46,114 - depth - INFO - Iter [27550/38400]	lr: 1.844e-05, eta: 2:07:24, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0386, loss: 0.0386, grad_norm: 0.6664
2022-03-28 14:26:18,467 - depth - INFO - Iter [27600/38400]	lr: 1.828e-05, eta: 2:06:47, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0384, loss: 0.0384, grad_norm: 0.6234
2022-03-28 14:26:50,789 - depth - INFO - Iter [27650/38400]	lr: 1.813e-05, eta: 2:06:11, time: 0.647, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0382, loss: 0.0382, grad_norm: 0.7297
2022-03-28 14:27:23,103 - depth - INFO - Iter [27700/38400]	lr: 1.797e-05, eta: 2:05:35, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0369, loss: 0.0369, grad_norm: 0.7594
2022-03-28 14:27:55,356 - depth - INFO - Iter [27750/38400]	lr: 1.781e-05, eta: 2:04:58, time: 0.643, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0375, loss: 0.0375, grad_norm: 0.4667
2022-03-28 14:28:27,782 - depth - INFO - Iter [27800/38400]	lr: 1.766e-05, eta: 2:04:22, time: 0.650, data_time: 0.009, memory: 24409, decode.loss_depth: 0.0380, loss: 0.0380, grad_norm: 0.5604
2022-03-28 14:29:00,032 - depth - INFO - Iter [27850/38400]	lr: 1.750e-05, eta: 2:03:46, time: 0.645, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0375, loss: 0.0375, grad_norm: 0.5607
2022-03-28 14:29:32,535 - depth - INFO - Iter [27900/38400]	lr: 1.734e-05, eta: 2:03:10, time: 0.651, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0381, loss: 0.0381, grad_norm: 0.5740
2022-03-28 14:30:04,996 - depth - INFO - Iter [27950/38400]	lr: 1.719e-05, eta: 2:02:33, time: 0.649, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0372, loss: 0.0372, grad_norm: 0.5096
2022-03-28 14:30:37,368 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 14:30:37,369 - depth - INFO - Iter [28000/38400]	lr: 1.704e-05, eta: 2:01:57, time: 0.647, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0372, loss: 0.0372, grad_norm: 0.5372
2022-03-28 14:31:09,165 - depth - INFO - Summary:
2022-03-28 14:31:09,166 - depth - INFO - 
+--------+-------+--------+---------+-------+--------+----------+--------+--------+
|   a1   |   a2  |   a3   | abs_rel |  rmse | log_10 | rmse_log | silog  | sq_rel |
+--------+-------+--------+---------+-------+--------+----------+--------+--------+
| 0.9216 | 0.989 | 0.9972 |  0.0949 | 0.333 | 0.0404 |  0.1198  | 9.4816 | 0.0475 |
+--------+-------+--------+---------+-------+--------+----------+--------+--------+
2022-03-28 14:31:09,175 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 14:31:09,175 - depth - INFO - Iter(val) [82]	a1: 0.9216, a2: 0.9890, a3: 0.9972, abs_rel: 0.09488808363676071, rmse: 0.3329685628414154, log_10: 0.040363796055316925, rmse_log: 0.11981809139251709, silog: 9.4816, sq_rel: 0.04749375209212303
2022-03-28 14:31:41,464 - depth - INFO - Iter [28050/38400]	lr: 1.688e-05, eta: 2:01:33, time: 1.282, data_time: 0.642, memory: 24409, decode.loss_depth: 0.0390, loss: 0.0390, grad_norm: 0.6790
2022-03-28 14:32:13,770 - depth - INFO - Iter [28100/38400]	lr: 1.673e-05, eta: 2:00:56, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0385, loss: 0.0385, grad_norm: 0.7427
2022-03-28 14:32:46,104 - depth - INFO - Iter [28150/38400]	lr: 1.658e-05, eta: 2:00:20, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0379, loss: 0.0379, grad_norm: 0.5745
2022-03-28 14:33:18,224 - depth - INFO - Iter [28200/38400]	lr: 1.643e-05, eta: 1:59:44, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0368, loss: 0.0368, grad_norm: 0.5659
2022-03-28 14:33:50,678 - depth - INFO - Iter [28250/38400]	lr: 1.627e-05, eta: 1:59:07, time: 0.649, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0368, loss: 0.0368, grad_norm: 0.6599
2022-03-28 14:34:23,064 - depth - INFO - Iter [28300/38400]	lr: 1.612e-05, eta: 1:58:31, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0389, loss: 0.0389, grad_norm: 0.6981
2022-03-28 14:34:55,512 - depth - INFO - Iter [28350/38400]	lr: 1.597e-05, eta: 1:57:55, time: 0.649, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0376, loss: 0.0376, grad_norm: 0.6869
2022-03-28 14:35:27,796 - depth - INFO - Iter [28400/38400]	lr: 1.582e-05, eta: 1:57:19, time: 0.646, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0376, loss: 0.0376, grad_norm: 0.5161
2022-03-28 14:36:00,278 - depth - INFO - Iter [28450/38400]	lr: 1.567e-05, eta: 1:56:43, time: 0.650, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0386, loss: 0.0386, grad_norm: 0.5839
2022-03-28 14:36:32,663 - depth - INFO - Iter [28500/38400]	lr: 1.553e-05, eta: 1:56:06, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0373, loss: 0.0373, grad_norm: 0.6786
2022-03-28 14:37:05,105 - depth - INFO - Iter [28550/38400]	lr: 1.538e-05, eta: 1:55:30, time: 0.649, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0372, loss: 0.0372, grad_norm: 0.8173
2022-03-28 14:37:37,538 - depth - INFO - Iter [28600/38400]	lr: 1.523e-05, eta: 1:54:54, time: 0.649, data_time: 0.009, memory: 24409, decode.loss_depth: 0.0380, loss: 0.0380, grad_norm: 0.5460
2022-03-28 14:38:10,042 - depth - INFO - Iter [28650/38400]	lr: 1.508e-05, eta: 1:54:18, time: 0.650, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0372, loss: 0.0372, grad_norm: 0.5243
2022-03-28 14:38:42,567 - depth - INFO - Iter [28700/38400]	lr: 1.494e-05, eta: 1:53:42, time: 0.651, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0380, loss: 0.0380, grad_norm: 0.6714
2022-03-28 14:39:15,146 - depth - INFO - Iter [28750/38400]	lr: 1.479e-05, eta: 1:53:06, time: 0.651, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0378, loss: 0.0378, grad_norm: 0.5434
2022-03-28 14:39:57,912 - depth - INFO - Saving checkpoint at 28800 iterations
2022-03-28 14:40:11,198 - depth - INFO - Iter [28800/38400]	lr: 1.465e-05, eta: 1:52:38, time: 1.122, data_time: 0.200, memory: 24409, decode.loss_depth: 0.0368, loss: 0.0368, grad_norm: 0.5208
2022-03-28 14:40:44,150 - depth - INFO - Summary:
2022-03-28 14:40:44,151 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
| 0.9222 | 0.9892 | 0.9975 |  0.0951 | 0.3305 | 0.0402 |  0.1195  | 9.431 | 0.0473 |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
2022-03-28 14:40:44,152 - depth - INFO - Iter(val) [82]	a1: 0.9222, a2: 0.9892, a3: 0.9975, abs_rel: 0.09507635235786438, rmse: 0.3305407464504242, log_10: 0.040173593908548355, rmse_log: 0.11948508769273758, silog: 9.4310, sq_rel: 0.04727955162525177
2022-03-28 14:41:16,488 - depth - INFO - Iter [28850/38400]	lr: 1.450e-05, eta: 1:52:13, time: 1.305, data_time: 0.665, memory: 24409, decode.loss_depth: 0.0364, loss: 0.0364, grad_norm: 0.5304
2022-03-28 14:41:48,872 - depth - INFO - Iter [28900/38400]	lr: 1.436e-05, eta: 1:51:36, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0368, loss: 0.0368, grad_norm: 0.6181
2022-03-28 14:42:21,268 - depth - INFO - Iter [28950/38400]	lr: 1.422e-05, eta: 1:51:00, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0377, loss: 0.0377, grad_norm: 0.4561
2022-03-28 14:42:53,648 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 14:42:53,649 - depth - INFO - Iter [29000/38400]	lr: 1.407e-05, eta: 1:50:24, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0370, loss: 0.0370, grad_norm: 0.4747
2022-03-28 14:43:26,093 - depth - INFO - Iter [29050/38400]	lr: 1.393e-05, eta: 1:49:48, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0376, loss: 0.0376, grad_norm: 0.6376
2022-03-28 14:43:58,357 - depth - INFO - Iter [29100/38400]	lr: 1.379e-05, eta: 1:49:12, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0364, loss: 0.0364, grad_norm: 0.6574
2022-03-28 14:44:30,674 - depth - INFO - Iter [29150/38400]	lr: 1.365e-05, eta: 1:48:36, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0376, loss: 0.0376, grad_norm: 0.5714
2022-03-28 14:45:02,867 - depth - INFO - Iter [29200/38400]	lr: 1.351e-05, eta: 1:47:59, time: 0.644, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0370, loss: 0.0370, grad_norm: 0.6648
2022-03-28 14:45:35,180 - depth - INFO - Iter [29250/38400]	lr: 1.337e-05, eta: 1:47:23, time: 0.646, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 0.5302
2022-03-28 14:46:07,586 - depth - INFO - Iter [29300/38400]	lr: 1.323e-05, eta: 1:46:47, time: 0.648, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0370, loss: 0.0370, grad_norm: 0.7653
2022-03-28 14:46:40,028 - depth - INFO - Iter [29350/38400]	lr: 1.309e-05, eta: 1:46:11, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0379, loss: 0.0379, grad_norm: 0.6448
2022-03-28 14:47:12,348 - depth - INFO - Iter [29400/38400]	lr: 1.296e-05, eta: 1:45:35, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0370, loss: 0.0370, grad_norm: 0.6070
2022-03-28 14:47:44,712 - depth - INFO - Iter [29450/38400]	lr: 1.282e-05, eta: 1:44:59, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0377, loss: 0.0377, grad_norm: 0.6143
2022-03-28 14:48:17,199 - depth - INFO - Iter [29500/38400]	lr: 1.268e-05, eta: 1:44:23, time: 0.650, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 0.5259
2022-03-28 14:48:49,518 - depth - INFO - Iter [29550/38400]	lr: 1.255e-05, eta: 1:43:47, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0374, loss: 0.0374, grad_norm: 0.6344
2022-03-28 14:49:22,055 - depth - INFO - Iter [29600/38400]	lr: 1.241e-05, eta: 1:43:11, time: 0.651, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0362, loss: 0.0362, grad_norm: 0.4880
2022-03-28 14:49:54,796 - depth - INFO - Summary:
2022-03-28 14:49:54,797 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9221 | 0.9891 | 0.9973 |  0.0957 | 0.3322 | 0.0404 |  0.1199  | 9.4267 | 0.0477 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 14:49:54,798 - depth - INFO - Iter(val) [82]	a1: 0.9221, a2: 0.9891, a3: 0.9973, abs_rel: 0.09571636468172073, rmse: 0.33218786120414734, log_10: 0.04040461406111717, rmse_log: 0.11987411975860596, silog: 9.4267, sq_rel: 0.047740183770656586
2022-03-28 14:50:27,265 - depth - INFO - Iter [29650/38400]	lr: 1.228e-05, eta: 1:42:45, time: 1.304, data_time: 0.661, memory: 24409, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 0.6481
2022-03-28 14:50:59,605 - depth - INFO - Iter [29700/38400]	lr: 1.214e-05, eta: 1:42:09, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0366, loss: 0.0366, grad_norm: 0.4548
2022-03-28 14:51:32,201 - depth - INFO - Iter [29750/38400]	lr: 1.201e-05, eta: 1:41:33, time: 0.652, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0363, loss: 0.0363, grad_norm: 0.5622
2022-03-28 14:52:04,874 - depth - INFO - Iter [29800/38400]	lr: 1.188e-05, eta: 1:40:57, time: 0.653, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0365, loss: 0.0365, grad_norm: 0.7012
2022-03-28 14:52:37,189 - depth - INFO - Iter [29850/38400]	lr: 1.174e-05, eta: 1:40:21, time: 0.647, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.7646
2022-03-28 14:53:09,563 - depth - INFO - Iter [29900/38400]	lr: 1.161e-05, eta: 1:39:45, time: 0.647, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0363, loss: 0.0363, grad_norm: 0.5084
2022-03-28 14:53:42,033 - depth - INFO - Iter [29950/38400]	lr: 1.148e-05, eta: 1:39:09, time: 0.650, data_time: 0.009, memory: 24409, decode.loss_depth: 0.0362, loss: 0.0362, grad_norm: 0.6839
2022-03-28 14:54:14,333 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 14:54:14,334 - depth - INFO - Iter [30000/38400]	lr: 1.135e-05, eta: 1:38:33, time: 0.646, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0366, loss: 0.0366, grad_norm: 0.6918
2022-03-28 14:54:46,692 - depth - INFO - Iter [30050/38400]	lr: 1.122e-05, eta: 1:37:57, time: 0.648, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0365, loss: 0.0365, grad_norm: 0.4821
2022-03-28 14:55:19,110 - depth - INFO - Iter [30100/38400]	lr: 1.109e-05, eta: 1:37:21, time: 0.648, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0362, loss: 0.0362, grad_norm: 0.4687
2022-03-28 14:55:51,542 - depth - INFO - Iter [30150/38400]	lr: 1.097e-05, eta: 1:36:45, time: 0.649, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.7128
2022-03-28 14:56:23,903 - depth - INFO - Iter [30200/38400]	lr: 1.084e-05, eta: 1:36:09, time: 0.647, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0360, loss: 0.0360, grad_norm: 0.6359
2022-03-28 14:56:56,717 - depth - INFO - Iter [30250/38400]	lr: 1.071e-05, eta: 1:35:33, time: 0.657, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0371, loss: 0.0371, grad_norm: 0.4444
2022-03-28 14:57:38,443 - depth - INFO - Iter [30300/38400]	lr: 1.059e-05, eta: 1:34:59, time: 0.834, data_time: 0.183, memory: 24409, decode.loss_depth: 0.0366, loss: 0.0366, grad_norm: 0.4754
2022-03-28 14:58:10,780 - depth - INFO - Iter [30350/38400]	lr: 1.046e-05, eta: 1:34:24, time: 0.647, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.3970
2022-03-28 14:58:43,036 - depth - INFO - Saving checkpoint at 30400 iterations
2022-03-28 14:58:56,728 - depth - INFO - Iter [30400/38400]	lr: 1.033e-05, eta: 1:33:51, time: 0.919, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0366, loss: 0.0366, grad_norm: 0.6530
2022-03-28 14:59:29,586 - depth - INFO - Summary:
2022-03-28 14:59:29,587 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9216 | 0.9892 | 0.9972 |  0.0951 | 0.3305 | 0.0402 |  0.1194  | 9.4039 | 0.0474 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 14:59:29,588 - depth - INFO - Iter(val) [82]	a1: 0.9216, a2: 0.9892, a3: 0.9972, abs_rel: 0.09509577602148056, rmse: 0.33053499460220337, log_10: 0.040165986865758896, rmse_log: 0.11936105787754059, silog: 9.4039, sq_rel: 0.047380320727825165
2022-03-28 15:00:01,660 - depth - INFO - Iter [30450/38400]	lr: 1.021e-05, eta: 1:33:24, time: 1.298, data_time: 0.662, memory: 24409, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.5501
2022-03-28 15:00:33,707 - depth - INFO - Iter [30500/38400]	lr: 1.009e-05, eta: 1:32:48, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.5554
2022-03-28 15:01:05,697 - depth - INFO - Iter [30550/38400]	lr: 9.964e-06, eta: 1:32:12, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0371, loss: 0.0371, grad_norm: 0.6031
2022-03-28 15:01:38,045 - depth - INFO - Iter [30600/38400]	lr: 9.842e-06, eta: 1:31:36, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0371, loss: 0.0371, grad_norm: 0.4612
2022-03-28 15:02:10,089 - depth - INFO - Iter [30650/38400]	lr: 9.721e-06, eta: 1:31:00, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.6195
2022-03-28 15:02:42,323 - depth - INFO - Iter [30700/38400]	lr: 9.600e-06, eta: 1:30:24, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.5556
2022-03-28 15:03:14,627 - depth - INFO - Iter [30750/38400]	lr: 9.480e-06, eta: 1:29:48, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.5931
2022-03-28 15:03:46,711 - depth - INFO - Iter [30800/38400]	lr: 9.360e-06, eta: 1:29:12, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0366, loss: 0.0366, grad_norm: 0.4594
2022-03-28 15:04:18,758 - depth - INFO - Iter [30850/38400]	lr: 9.241e-06, eta: 1:28:36, time: 0.641, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0349, loss: 0.0349, grad_norm: 0.4125
2022-03-28 15:04:50,817 - depth - INFO - Iter [30900/38400]	lr: 9.123e-06, eta: 1:28:00, time: 0.641, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0366, loss: 0.0366, grad_norm: 0.5334
2022-03-28 15:05:22,755 - depth - INFO - Iter [30950/38400]	lr: 9.006e-06, eta: 1:27:24, time: 0.639, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0350, loss: 0.0350, grad_norm: 0.5982
2022-03-28 15:05:54,732 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 15:05:54,733 - depth - INFO - Iter [31000/38400]	lr: 8.889e-06, eta: 1:26:48, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0362, loss: 0.0362, grad_norm: 0.4422
2022-03-28 15:06:27,060 - depth - INFO - Iter [31050/38400]	lr: 8.773e-06, eta: 1:26:12, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.5240
2022-03-28 15:06:59,116 - depth - INFO - Iter [31100/38400]	lr: 8.657e-06, eta: 1:25:36, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.7008
2022-03-28 15:07:31,170 - depth - INFO - Iter [31150/38400]	lr: 8.543e-06, eta: 1:25:00, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 0.5164
2022-03-28 15:08:03,473 - depth - INFO - Iter [31200/38400]	lr: 8.429e-06, eta: 1:24:24, time: 0.646, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0350, loss: 0.0350, grad_norm: 0.5150
2022-03-28 15:08:36,362 - depth - INFO - Summary:
2022-03-28 15:08:36,363 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9211 | 0.9887 | 0.9972 |  0.0965 | 0.3329 | 0.0405 |  0.1203  | 9.4119 | 0.0486 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 15:08:36,364 - depth - INFO - Iter(val) [82]	a1: 0.9211, a2: 0.9887, a3: 0.9972, abs_rel: 0.09653370827436447, rmse: 0.3328820466995239, log_10: 0.04049040004611015, rmse_log: 0.12025163322687149, silog: 9.4119, sq_rel: 0.04864513501524925
2022-03-28 15:09:08,563 - depth - INFO - Iter [31250/38400]	lr: 8.315e-06, eta: 1:23:56, time: 1.302, data_time: 0.665, memory: 24409, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.5603
2022-03-28 15:09:40,972 - depth - INFO - Iter [31300/38400]	lr: 8.203e-06, eta: 1:23:20, time: 0.648, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.5418
2022-03-28 15:10:13,162 - depth - INFO - Iter [31350/38400]	lr: 8.091e-06, eta: 1:22:44, time: 0.644, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.5611
2022-03-28 15:10:45,232 - depth - INFO - Iter [31400/38400]	lr: 7.980e-06, eta: 1:22:08, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.4551
2022-03-28 15:11:17,361 - depth - INFO - Iter [31450/38400]	lr: 7.869e-06, eta: 1:21:32, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 0.4883
2022-03-28 15:11:49,687 - depth - INFO - Iter [31500/38400]	lr: 7.760e-06, eta: 1:20:57, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 0.6107
2022-03-28 15:12:21,980 - depth - INFO - Iter [31550/38400]	lr: 7.650e-06, eta: 1:20:21, time: 0.646, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0349, loss: 0.0349, grad_norm: 0.5453
2022-03-28 15:12:54,324 - depth - INFO - Iter [31600/38400]	lr: 7.542e-06, eta: 1:19:45, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0349, loss: 0.0349, grad_norm: 0.5256
2022-03-28 15:13:26,476 - depth - INFO - Iter [31650/38400]	lr: 7.434e-06, eta: 1:19:09, time: 0.643, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0365, loss: 0.0365, grad_norm: 0.5920
2022-03-28 15:13:58,468 - depth - INFO - Iter [31700/38400]	lr: 7.327e-06, eta: 1:18:33, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.5937
2022-03-28 15:14:30,607 - depth - INFO - Iter [31750/38400]	lr: 7.221e-06, eta: 1:17:57, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0366, loss: 0.0366, grad_norm: 0.5248
2022-03-28 15:15:12,533 - depth - INFO - Iter [31800/38400]	lr: 7.116e-06, eta: 1:17:24, time: 0.839, data_time: 0.172, memory: 24409, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 0.4545
2022-03-28 15:15:44,794 - depth - INFO - Iter [31850/38400]	lr: 7.011e-06, eta: 1:16:48, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.4776
2022-03-28 15:16:17,178 - depth - INFO - Iter [31900/38400]	lr: 6.907e-06, eta: 1:16:12, time: 0.647, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 0.5455
2022-03-28 15:16:49,271 - depth - INFO - Iter [31950/38400]	lr: 6.803e-06, eta: 1:15:36, time: 0.641, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.3965
2022-03-28 15:17:21,229 - depth - INFO - Saving checkpoint at 32000 iterations
2022-03-28 15:17:34,590 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 15:17:34,591 - depth - INFO - Iter [32000/38400]	lr: 6.701e-06, eta: 1:15:03, time: 0.907, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 0.4350
2022-03-28 15:18:07,019 - depth - INFO - Summary:
2022-03-28 15:18:07,020 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9227 | 0.9893 | 0.9973 |  0.0937 | 0.3313 | 0.0399 |  0.1187  | 9.3803 | 0.0466 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 15:18:18,992 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_32000.pth.
2022-03-28 15:18:18,993 - depth - INFO - Best abs_rel is 0.0937 at 32000 iter.
2022-03-28 15:18:18,994 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 15:18:18,994 - depth - INFO - Iter(val) [82]	a1: 0.9227, a2: 0.9893, a3: 0.9973, abs_rel: 0.09370788186788559, rmse: 0.3312646150588989, log_10: 0.0399155355989933, rmse_log: 0.11869931221008301, silog: 9.3803, sq_rel: 0.04659599810838699
2022-03-28 15:18:51,357 - depth - INFO - Iter [32050/38400]	lr: 6.599e-06, eta: 1:14:36, time: 1.535, data_time: 0.893, memory: 24409, decode.loss_depth: 0.0362, loss: 0.0362, grad_norm: 0.3859
2022-03-28 15:19:23,615 - depth - INFO - Iter [32100/38400]	lr: 6.498e-06, eta: 1:14:00, time: 0.645, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.4395
2022-03-28 15:19:55,612 - depth - INFO - Iter [32150/38400]	lr: 6.397e-06, eta: 1:13:25, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 0.6376
2022-03-28 15:20:27,954 - depth - INFO - Iter [32200/38400]	lr: 6.297e-06, eta: 1:12:49, time: 0.647, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0350, loss: 0.0350, grad_norm: 0.5315
2022-03-28 15:21:00,187 - depth - INFO - Iter [32250/38400]	lr: 6.198e-06, eta: 1:12:13, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 0.3809
2022-03-28 15:21:32,170 - depth - INFO - Iter [32300/38400]	lr: 6.100e-06, eta: 1:11:37, time: 0.640, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.5263
2022-03-28 15:22:04,456 - depth - INFO - Iter [32350/38400]	lr: 6.003e-06, eta: 1:11:01, time: 0.646, data_time: 0.009, memory: 24409, decode.loss_depth: 0.0345, loss: 0.0345, grad_norm: 0.4675
2022-03-28 15:22:36,647 - depth - INFO - Iter [32400/38400]	lr: 5.906e-06, eta: 1:10:25, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 0.4437
2022-03-28 15:23:08,894 - depth - INFO - Iter [32450/38400]	lr: 5.810e-06, eta: 1:09:50, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0346, loss: 0.0346, grad_norm: 0.4249
2022-03-28 15:23:40,965 - depth - INFO - Iter [32500/38400]	lr: 5.714e-06, eta: 1:09:14, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.4089
2022-03-28 15:24:13,066 - depth - INFO - Iter [32550/38400]	lr: 5.620e-06, eta: 1:08:38, time: 0.642, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.5897
2022-03-28 15:24:45,552 - depth - INFO - Iter [32600/38400]	lr: 5.526e-06, eta: 1:08:02, time: 0.649, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0350, loss: 0.0350, grad_norm: 0.4834
2022-03-28 15:25:17,514 - depth - INFO - Iter [32650/38400]	lr: 5.433e-06, eta: 1:07:27, time: 0.639, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 0.4662
2022-03-28 15:25:49,808 - depth - INFO - Iter [32700/38400]	lr: 5.341e-06, eta: 1:06:51, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0345, loss: 0.0345, grad_norm: 0.3886
2022-03-28 15:26:22,123 - depth - INFO - Iter [32750/38400]	lr: 5.249e-06, eta: 1:06:15, time: 0.647, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.4418
2022-03-28 15:26:54,279 - depth - INFO - Iter [32800/38400]	lr: 5.158e-06, eta: 1:05:40, time: 0.643, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.4140
2022-03-28 15:27:26,385 - depth - INFO - Summary:
2022-03-28 15:27:26,386 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9222 | 0.9891 | 0.9972 |  0.0948 | 0.3301 |  0.04  |  0.1191  | 9.3878 | 0.0472 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 15:27:26,386 - depth - INFO - Iter(val) [82]	a1: 0.9222, a2: 0.9891, a3: 0.9972, abs_rel: 0.09484214335680008, rmse: 0.3301027715206146, log_10: 0.040029384195804596, rmse_log: 0.11914041638374329, silog: 9.3878, sq_rel: 0.04719885438680649
2022-03-28 15:27:58,806 - depth - INFO - Iter [32850/38400]	lr: 5.068e-06, eta: 1:05:09, time: 1.291, data_time: 0.650, memory: 24409, decode.loss_depth: 0.0345, loss: 0.0345, grad_norm: 0.4536
2022-03-28 15:28:30,963 - depth - INFO - Iter [32900/38400]	lr: 4.979e-06, eta: 1:04:34, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0345, loss: 0.0345, grad_norm: 0.5127
2022-03-28 15:29:03,243 - depth - INFO - Iter [32950/38400]	lr: 4.890e-06, eta: 1:03:58, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0365, loss: 0.0365, grad_norm: 0.4285
2022-03-28 15:29:35,414 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 15:29:35,415 - depth - INFO - Iter [33000/38400]	lr: 4.802e-06, eta: 1:03:22, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.4425
2022-03-28 15:30:07,653 - depth - INFO - Iter [33050/38400]	lr: 4.715e-06, eta: 1:02:47, time: 0.645, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.5128
2022-03-28 15:30:39,867 - depth - INFO - Iter [33100/38400]	lr: 4.629e-06, eta: 1:02:11, time: 0.645, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.4612
2022-03-28 15:31:12,085 - depth - INFO - Iter [33150/38400]	lr: 4.543e-06, eta: 1:01:35, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.4613
2022-03-28 15:31:44,335 - depth - INFO - Iter [33200/38400]	lr: 4.458e-06, eta: 1:01:00, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.5199
2022-03-28 15:32:16,580 - depth - INFO - Iter [33250/38400]	lr: 4.374e-06, eta: 1:00:24, time: 0.644, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0346, loss: 0.0346, grad_norm: 0.4168
2022-03-28 15:32:48,860 - depth - INFO - Iter [33300/38400]	lr: 4.291e-06, eta: 0:59:48, time: 0.646, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0345, loss: 0.0345, grad_norm: 0.4421
2022-03-28 15:33:29,968 - depth - INFO - Iter [33350/38400]	lr: 4.209e-06, eta: 0:59:14, time: 0.822, data_time: 0.177, memory: 24409, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.4047
2022-03-28 15:34:02,119 - depth - INFO - Iter [33400/38400]	lr: 4.127e-06, eta: 0:58:38, time: 0.643, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 0.4116
2022-03-28 15:34:34,291 - depth - INFO - Iter [33450/38400]	lr: 4.046e-06, eta: 0:58:03, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.3891
2022-03-28 15:35:06,505 - depth - INFO - Iter [33500/38400]	lr: 3.966e-06, eta: 0:57:27, time: 0.644, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 0.3854
2022-03-28 15:35:38,568 - depth - INFO - Iter [33550/38400]	lr: 3.886e-06, eta: 0:56:51, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.4356
2022-03-28 15:36:10,743 - depth - INFO - Saving checkpoint at 33600 iterations
2022-03-28 15:36:23,897 - depth - INFO - Iter [33600/38400]	lr: 3.808e-06, eta: 0:56:18, time: 0.907, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0345, loss: 0.0345, grad_norm: 0.5292
2022-03-28 15:36:56,599 - depth - INFO - Summary:
2022-03-28 15:36:56,600 - depth - INFO - 
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1  |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.923 | 0.9894 | 0.9973 |  0.0943 | 0.3301 | 0.0399 |  0.1188  | 9.3699 | 0.0469 |
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 15:36:56,601 - depth - INFO - Iter(val) [82]	a1: 0.9230, a2: 0.9894, a3: 0.9973, abs_rel: 0.09427807480096817, rmse: 0.33008140325546265, log_10: 0.03990601375699043, rmse_log: 0.11879299581050873, silog: 9.3699, sq_rel: 0.04692230001091957
2022-03-28 15:37:28,730 - depth - INFO - Iter [33650/38400]	lr: 3.730e-06, eta: 0:55:47, time: 1.296, data_time: 0.660, memory: 24409, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.4578
2022-03-28 15:38:01,040 - depth - INFO - Iter [33700/38400]	lr: 3.653e-06, eta: 0:55:11, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0346, loss: 0.0346, grad_norm: 0.4972
2022-03-28 15:38:33,077 - depth - INFO - Iter [33750/38400]	lr: 3.576e-06, eta: 0:54:35, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.4559
2022-03-28 15:39:05,328 - depth - INFO - Iter [33800/38400]	lr: 3.501e-06, eta: 0:54:00, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.5363
2022-03-28 15:39:37,382 - depth - INFO - Iter [33850/38400]	lr: 3.426e-06, eta: 0:53:24, time: 0.641, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0340, loss: 0.0340, grad_norm: 0.4470
2022-03-28 15:40:09,655 - depth - INFO - Iter [33900/38400]	lr: 3.352e-06, eta: 0:52:49, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.4482
2022-03-28 15:40:42,080 - depth - INFO - Iter [33950/38400]	lr: 3.279e-06, eta: 0:52:13, time: 0.648, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0336, loss: 0.0336, grad_norm: 0.4342
2022-03-28 15:41:14,112 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 15:41:14,113 - depth - INFO - Iter [34000/38400]	lr: 3.206e-06, eta: 0:51:37, time: 0.641, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0338, loss: 0.0338, grad_norm: 0.4670
2022-03-28 15:41:46,278 - depth - INFO - Iter [34050/38400]	lr: 3.134e-06, eta: 0:51:02, time: 0.643, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.4703
2022-03-28 15:42:18,334 - depth - INFO - Iter [34100/38400]	lr: 3.064e-06, eta: 0:50:26, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0349, loss: 0.0349, grad_norm: 0.4224
2022-03-28 15:42:50,564 - depth - INFO - Iter [34150/38400]	lr: 2.993e-06, eta: 0:49:51, time: 0.645, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.3925
2022-03-28 15:43:22,781 - depth - INFO - Iter [34200/38400]	lr: 2.924e-06, eta: 0:49:15, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0349, loss: 0.0349, grad_norm: 0.5286
2022-03-28 15:43:54,850 - depth - INFO - Iter [34250/38400]	lr: 2.856e-06, eta: 0:48:39, time: 0.642, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0349, loss: 0.0349, grad_norm: 0.4126
2022-03-28 15:44:26,956 - depth - INFO - Iter [34300/38400]	lr: 2.788e-06, eta: 0:48:04, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.5281
2022-03-28 15:44:59,081 - depth - INFO - Iter [34350/38400]	lr: 2.721e-06, eta: 0:47:28, time: 0.642, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.4379
2022-03-28 15:45:31,524 - depth - INFO - Iter [34400/38400]	lr: 2.655e-06, eta: 0:46:53, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.4690
2022-03-28 15:46:03,917 - depth - INFO - Summary:
2022-03-28 15:46:03,919 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
| 0.9234 | 0.9893 | 0.9973 |  0.0941 | 0.3294 | 0.0399 |  0.1186  | 9.352 | 0.0467 |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
2022-03-28 15:46:03,920 - depth - INFO - Iter(val) [82]	a1: 0.9234, a2: 0.9893, a3: 0.9973, abs_rel: 0.09405038505792618, rmse: 0.3293980658054352, log_10: 0.039858072996139526, rmse_log: 0.11856695264577866, silog: 9.3520, sq_rel: 0.04670532047748566
2022-03-28 15:46:36,165 - depth - INFO - Iter [34450/38400]	lr: 2.589e-06, eta: 0:46:21, time: 1.294, data_time: 0.656, memory: 24409, decode.loss_depth: 0.0335, loss: 0.0335, grad_norm: 0.3927
2022-03-28 15:47:08,429 - depth - INFO - Iter [34500/38400]	lr: 2.525e-06, eta: 0:45:46, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.3774
2022-03-28 15:47:40,532 - depth - INFO - Iter [34550/38400]	lr: 2.461e-06, eta: 0:45:10, time: 0.642, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.3828
2022-03-28 15:48:12,498 - depth - INFO - Iter [34600/38400]	lr: 2.398e-06, eta: 0:44:34, time: 0.639, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0343, loss: 0.0343, grad_norm: 0.3802
2022-03-28 15:48:44,480 - depth - INFO - Iter [34650/38400]	lr: 2.336e-06, eta: 0:43:59, time: 0.640, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.3683
2022-03-28 15:49:16,604 - depth - INFO - Iter [34700/38400]	lr: 2.275e-06, eta: 0:43:23, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.4870
2022-03-28 15:49:48,694 - depth - INFO - Iter [34750/38400]	lr: 2.214e-06, eta: 0:42:48, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0346, loss: 0.0346, grad_norm: 0.4288
2022-03-28 15:50:20,722 - depth - INFO - Iter [34800/38400]	lr: 2.154e-06, eta: 0:42:12, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.4700
2022-03-28 15:51:03,043 - depth - INFO - Iter [34850/38400]	lr: 2.095e-06, eta: 0:41:38, time: 0.847, data_time: 0.194, memory: 24409, decode.loss_depth: 0.0335, loss: 0.0335, grad_norm: 0.3337
2022-03-28 15:51:35,506 - depth - INFO - Iter [34900/38400]	lr: 2.037e-06, eta: 0:41:02, time: 0.649, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0338, loss: 0.0338, grad_norm: 0.3931
2022-03-28 15:52:07,779 - depth - INFO - Iter [34950/38400]	lr: 1.980e-06, eta: 0:40:27, time: 0.646, data_time: 0.008, memory: 24409, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.3977
2022-03-28 15:52:40,060 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 15:52:40,061 - depth - INFO - Iter [35000/38400]	lr: 1.923e-06, eta: 0:39:52, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.3867
2022-03-28 15:53:12,202 - depth - INFO - Iter [35050/38400]	lr: 1.867e-06, eta: 0:39:16, time: 0.643, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.3943
2022-03-28 15:53:44,485 - depth - INFO - Iter [35100/38400]	lr: 1.812e-06, eta: 0:38:41, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.3892
2022-03-28 15:54:16,854 - depth - INFO - Iter [35150/38400]	lr: 1.758e-06, eta: 0:38:05, time: 0.647, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.3997
2022-03-28 15:54:48,861 - depth - INFO - Saving checkpoint at 35200 iterations
2022-03-28 15:55:02,165 - depth - INFO - Iter [35200/38400]	lr: 1.705e-06, eta: 0:37:31, time: 0.907, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.3995
2022-03-28 15:55:34,521 - depth - INFO - Summary:
2022-03-28 15:55:34,523 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9224 | 0.9891 | 0.9974 |  0.0946 | 0.3296 |  0.04  |  0.1189  | 9.3603 | 0.047  |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 15:55:34,524 - depth - INFO - Iter(val) [82]	a1: 0.9224, a2: 0.9891, a3: 0.9974, abs_rel: 0.09460262209177017, rmse: 0.3295982778072357, log_10: 0.039953455328941345, rmse_log: 0.11885238438844681, silog: 9.3603, sq_rel: 0.047032613307237625
2022-03-28 15:56:06,700 - depth - INFO - Iter [35250/38400]	lr: 1.652e-06, eta: 0:36:58, time: 1.290, data_time: 0.653, memory: 24409, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.3766
2022-03-28 15:56:39,060 - depth - INFO - Iter [35300/38400]	lr: 1.600e-06, eta: 0:36:23, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0329, loss: 0.0329, grad_norm: 0.4327
2022-03-28 15:57:11,417 - depth - INFO - Iter [35350/38400]	lr: 1.550e-06, eta: 0:35:47, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0337, loss: 0.0337, grad_norm: 0.4269
2022-03-28 15:57:43,464 - depth - INFO - Iter [35400/38400]	lr: 1.499e-06, eta: 0:35:12, time: 0.642, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.3654
2022-03-28 15:58:15,686 - depth - INFO - Iter [35450/38400]	lr: 1.450e-06, eta: 0:34:37, time: 0.644, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0349, loss: 0.0349, grad_norm: 0.4060
2022-03-28 15:58:47,920 - depth - INFO - Iter [35500/38400]	lr: 1.402e-06, eta: 0:34:01, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.3427
2022-03-28 15:59:19,985 - depth - INFO - Iter [35550/38400]	lr: 1.354e-06, eta: 0:33:26, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0343, loss: 0.0343, grad_norm: 0.3342
2022-03-28 15:59:52,147 - depth - INFO - Iter [35600/38400]	lr: 1.307e-06, eta: 0:32:50, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.3498
2022-03-28 16:00:24,455 - depth - INFO - Iter [35650/38400]	lr: 1.261e-06, eta: 0:32:15, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0327, loss: 0.0327, grad_norm: 0.3568
2022-03-28 16:00:56,658 - depth - INFO - Iter [35700/38400]	lr: 1.216e-06, eta: 0:31:39, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0343, loss: 0.0343, grad_norm: 0.4839
2022-03-28 16:01:28,926 - depth - INFO - Iter [35750/38400]	lr: 1.171e-06, eta: 0:31:04, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.3942
2022-03-28 16:02:01,145 - depth - INFO - Iter [35800/38400]	lr: 1.128e-06, eta: 0:30:29, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0341, loss: 0.0341, grad_norm: 0.3287
2022-03-28 16:02:33,246 - depth - INFO - Iter [35850/38400]	lr: 1.085e-06, eta: 0:29:53, time: 0.642, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.3552
2022-03-28 16:03:05,523 - depth - INFO - Iter [35900/38400]	lr: 1.043e-06, eta: 0:29:18, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.3424
2022-03-28 16:03:37,755 - depth - INFO - Iter [35950/38400]	lr: 1.002e-06, eta: 0:28:42, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.3861
2022-03-28 16:04:10,018 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 16:04:10,018 - depth - INFO - Iter [36000/38400]	lr: 9.615e-07, eta: 0:28:07, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0331, loss: 0.0331, grad_norm: 0.3967
2022-03-28 16:04:42,898 - depth - INFO - Summary:
2022-03-28 16:04:42,900 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9231 | 0.9893 | 0.9973 |  0.0939 | 0.3293 | 0.0398 |  0.1185  | 9.3561 | 0.0465 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 16:04:42,900 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 16:04:42,900 - depth - INFO - Iter(val) [82]	a1: 0.9231, a2: 0.9893, a3: 0.9973, abs_rel: 0.0938565656542778, rmse: 0.3293076455593109, log_10: 0.039815448224544525, rmse_log: 0.11850406974554062, silog: 9.3561, sq_rel: 0.046507108956575394
2022-03-28 16:05:15,334 - depth - INFO - Iter [36050/38400]	lr: 9.220e-07, eta: 0:27:34, time: 1.307, data_time: 0.664, memory: 24409, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.3759
2022-03-28 16:05:47,575 - depth - INFO - Iter [36100/38400]	lr: 8.833e-07, eta: 0:26:59, time: 0.645, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.3712
2022-03-28 16:06:19,913 - depth - INFO - Iter [36150/38400]	lr: 8.455e-07, eta: 0:26:23, time: 0.647, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.3155
2022-03-28 16:06:52,312 - depth - INFO - Iter [36200/38400]	lr: 8.084e-07, eta: 0:25:48, time: 0.647, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0338, loss: 0.0338, grad_norm: 0.3265
2022-03-28 16:07:24,274 - depth - INFO - Iter [36250/38400]	lr: 7.722e-07, eta: 0:25:12, time: 0.640, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0340, loss: 0.0340, grad_norm: 0.3810
2022-03-28 16:07:56,286 - depth - INFO - Iter [36300/38400]	lr: 7.368e-07, eta: 0:24:37, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0341, loss: 0.0341, grad_norm: 0.4059
2022-03-28 16:08:39,082 - depth - INFO - Iter [36350/38400]	lr: 7.022e-07, eta: 0:24:02, time: 0.856, data_time: 0.196, memory: 24409, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 0.3643
2022-03-28 16:09:11,350 - depth - INFO - Iter [36400/38400]	lr: 6.685e-07, eta: 0:23:27, time: 0.645, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.4083
2022-03-28 16:09:43,592 - depth - INFO - Iter [36450/38400]	lr: 6.356e-07, eta: 0:22:52, time: 0.645, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0336, loss: 0.0336, grad_norm: 0.3613
2022-03-28 16:10:15,654 - depth - INFO - Iter [36500/38400]	lr: 6.035e-07, eta: 0:22:16, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0331, loss: 0.0331, grad_norm: 0.4085
2022-03-28 16:10:47,924 - depth - INFO - Iter [36550/38400]	lr: 5.722e-07, eta: 0:21:41, time: 0.646, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0338, loss: 0.0338, grad_norm: 0.3711
2022-03-28 16:11:19,944 - depth - INFO - Iter [36600/38400]	lr: 5.418e-07, eta: 0:21:06, time: 0.641, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0345, loss: 0.0345, grad_norm: 0.3364
2022-03-28 16:11:52,124 - depth - INFO - Iter [36650/38400]	lr: 5.122e-07, eta: 0:20:30, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.3259
2022-03-28 16:12:24,175 - depth - INFO - Iter [36700/38400]	lr: 4.834e-07, eta: 0:19:55, time: 0.641, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0336, loss: 0.0336, grad_norm: 0.3817
2022-03-28 16:12:56,282 - depth - INFO - Iter [36750/38400]	lr: 4.554e-07, eta: 0:19:20, time: 0.642, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0338, loss: 0.0338, grad_norm: 0.3213
2022-03-28 16:13:28,515 - depth - INFO - Saving checkpoint at 36800 iterations
2022-03-28 16:13:41,641 - depth - INFO - Iter [36800/38400]	lr: 4.283e-07, eta: 0:18:45, time: 0.908, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0335, loss: 0.0335, grad_norm: 0.3746
2022-03-28 16:14:13,399 - depth - INFO - Summary:
2022-03-28 16:14:13,399 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9232 | 0.9892 | 0.9972 |  0.0942 | 0.3298 | 0.0399 |  0.1187  | 9.3631 | 0.047  |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 16:14:13,400 - depth - INFO - Iter(val) [82]	a1: 0.9232, a2: 0.9892, a3: 0.9972, abs_rel: 0.09418284893035889, rmse: 0.32984068989753723, log_10: 0.03989284858107567, rmse_log: 0.11869937181472778, silog: 9.3631, sq_rel: 0.04700608178973198
2022-03-28 16:14:45,441 - depth - INFO - Iter [36850/38400]	lr: 4.020e-07, eta: 0:18:11, time: 1.276, data_time: 0.641, memory: 24409, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.3483
2022-03-28 16:15:17,652 - depth - INFO - Iter [36900/38400]	lr: 3.765e-07, eta: 0:17:36, time: 0.644, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0349, loss: 0.0349, grad_norm: 0.3558
2022-03-28 16:15:49,821 - depth - INFO - Iter [36950/38400]	lr: 3.519e-07, eta: 0:17:00, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0340, loss: 0.0340, grad_norm: 0.3681
2022-03-28 16:16:21,980 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 16:16:21,981 - depth - INFO - Iter [37000/38400]	lr: 3.281e-07, eta: 0:16:25, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0335, loss: 0.0335, grad_norm: 0.3599
2022-03-28 16:16:54,351 - depth - INFO - Iter [37050/38400]	lr: 3.051e-07, eta: 0:15:50, time: 0.648, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.3376
2022-03-28 16:17:26,308 - depth - INFO - Iter [37100/38400]	lr: 2.830e-07, eta: 0:15:14, time: 0.639, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.3540
2022-03-28 16:17:58,319 - depth - INFO - Iter [37150/38400]	lr: 2.616e-07, eta: 0:14:39, time: 0.640, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0334, loss: 0.0334, grad_norm: 0.3640
2022-03-28 16:18:30,520 - depth - INFO - Iter [37200/38400]	lr: 2.412e-07, eta: 0:14:04, time: 0.644, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.3728
2022-03-28 16:19:02,692 - depth - INFO - Iter [37250/38400]	lr: 2.215e-07, eta: 0:13:29, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.3863
2022-03-28 16:19:34,818 - depth - INFO - Iter [37300/38400]	lr: 2.027e-07, eta: 0:12:53, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0338, loss: 0.0338, grad_norm: 0.3742
2022-03-28 16:20:06,837 - depth - INFO - Iter [37350/38400]	lr: 1.847e-07, eta: 0:12:18, time: 0.640, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.3262
2022-03-28 16:20:38,955 - depth - INFO - Iter [37400/38400]	lr: 1.676e-07, eta: 0:11:43, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0340, loss: 0.0340, grad_norm: 0.3707
2022-03-28 16:21:11,400 - depth - INFO - Iter [37450/38400]	lr: 1.513e-07, eta: 0:11:08, time: 0.649, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0340, loss: 0.0340, grad_norm: 0.2941
2022-03-28 16:21:43,821 - depth - INFO - Iter [37500/38400]	lr: 1.358e-07, eta: 0:10:32, time: 0.648, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.3298
2022-03-28 16:22:16,092 - depth - INFO - Iter [37550/38400]	lr: 1.211e-07, eta: 0:09:57, time: 0.645, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.3585
2022-03-28 16:22:48,170 - depth - INFO - Iter [37600/38400]	lr: 1.073e-07, eta: 0:09:22, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.3344
2022-03-28 16:23:21,276 - depth - INFO - Summary:
2022-03-28 16:23:21,277 - depth - INFO - 
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1  |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.923 | 0.9892 | 0.9973 |  0.094  | 0.3294 | 0.0398 |  0.1187  | 9.3665 | 0.0467 |
+-------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 16:23:21,278 - depth - INFO - Iter(val) [82]	a1: 0.9230, a2: 0.9892, a3: 0.9973, abs_rel: 0.09396430850028992, rmse: 0.32943016290664673, log_10: 0.03984544426202774, rmse_log: 0.1186579167842865, silog: 9.3665, sq_rel: 0.046696461737155914
2022-03-28 16:23:53,381 - depth - INFO - Iter [37650/38400]	lr: 9.435e-08, eta: 0:08:47, time: 1.304, data_time: 0.669, memory: 24409, decode.loss_depth: 0.0345, loss: 0.0345, grad_norm: 0.3460
2022-03-28 16:24:25,254 - depth - INFO - Iter [37700/38400]	lr: 8.221e-08, eta: 0:08:12, time: 0.638, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0335, loss: 0.0335, grad_norm: 0.3776
2022-03-28 16:24:57,527 - depth - INFO - Iter [37750/38400]	lr: 7.090e-08, eta: 0:07:37, time: 0.646, data_time: 0.007, memory: 24409, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.3023
2022-03-28 16:25:29,877 - depth - INFO - Iter [37800/38400]	lr: 6.043e-08, eta: 0:07:02, time: 0.647, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.3485
2022-03-28 16:26:02,031 - depth - INFO - Iter [37850/38400]	lr: 5.079e-08, eta: 0:06:26, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0338, loss: 0.0338, grad_norm: 0.3490
2022-03-28 16:26:44,372 - depth - INFO - Iter [37900/38400]	lr: 4.200e-08, eta: 0:05:51, time: 0.847, data_time: 0.207, memory: 24409, decode.loss_depth: 0.0334, loss: 0.0334, grad_norm: 0.3335
2022-03-28 16:27:16,510 - depth - INFO - Iter [37950/38400]	lr: 3.403e-08, eta: 0:05:16, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0341, loss: 0.0341, grad_norm: 0.3481
2022-03-28 16:27:48,589 - depth - INFO - Exp name: depthformer_swinl_22k_w7_nyu.py
2022-03-28 16:27:48,589 - depth - INFO - Iter [38000/38400]	lr: 2.691e-08, eta: 0:04:41, time: 0.642, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0343, loss: 0.0343, grad_norm: 0.3461
2022-03-28 16:28:20,888 - depth - INFO - Iter [38050/38400]	lr: 2.061e-08, eta: 0:04:06, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0341, loss: 0.0341, grad_norm: 0.3444
2022-03-28 16:28:53,171 - depth - INFO - Iter [38100/38400]	lr: 1.516e-08, eta: 0:03:30, time: 0.646, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0337, loss: 0.0337, grad_norm: 0.3113
2022-03-28 16:29:25,312 - depth - INFO - Iter [38150/38400]	lr: 1.054e-08, eta: 0:02:55, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0332, loss: 0.0332, grad_norm: 0.3553
2022-03-28 16:29:57,474 - depth - INFO - Iter [38200/38400]	lr: 6.761e-09, eta: 0:02:20, time: 0.643, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0341, loss: 0.0341, grad_norm: 0.3674
2022-03-28 16:30:29,903 - depth - INFO - Iter [38250/38400]	lr: 3.816e-09, eta: 0:01:45, time: 0.649, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0330, loss: 0.0330, grad_norm: 0.3123
2022-03-28 16:31:01,995 - depth - INFO - Iter [38300/38400]	lr: 1.708e-09, eta: 0:01:10, time: 0.642, data_time: 0.006, memory: 24409, decode.loss_depth: 0.0334, loss: 0.0334, grad_norm: 0.3286
2022-03-28 16:31:34,411 - depth - INFO - Iter [38350/38400]	lr: 4.362e-10, eta: 0:00:35, time: 0.648, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.3523
2022-03-28 16:32:06,518 - depth - INFO - Saving checkpoint at 38400 iterations
2022-03-28 16:32:19,681 - depth - INFO - Iter [38400/38400]	lr: 1.167e-12, eta: 0:00:00, time: 0.906, data_time: 0.005, memory: 24409, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.3200
2022-03-28 16:32:52,177 - depth - INFO - Summary:
2022-03-28 16:32:52,178 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9225 | 0.9892 | 0.9973 |  0.0943 | 0.3296 |  0.04  |  0.1188  | 9.3713 | 0.0468 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-03-28 16:32:52,178 - depth - INFO - Iter(val) [82]	a1: 0.9225, a2: 0.9892, a3: 0.9973, abs_rel: 0.09434293210506439, rmse: 0.32956546545028687, log_10: 0.039951033890247345, rmse_log: 0.11883654445409775, silog: 9.3713, sq_rel: 0.04679228365421295
