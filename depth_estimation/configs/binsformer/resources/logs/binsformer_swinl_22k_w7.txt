2022-01-11 10:56:56,802 - depth - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.11 (default, Jul 27 2021, 14:32:16) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: Tesla V100-SXM2-32GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.1, V10.1.243
GCC: gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
PyTorch: 1.8.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.9.0
OpenCV: 4.5.5
MMCV: 1.3.13
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.1
Depth: 0.0.0+0193246
------------------------------------------------------------

2022-01-11 10:56:56,802 - depth - INFO - Distributed training: True
2022-01-11 10:56:57,000 - depth - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
backbone_norm_cfg = dict(type='LN', requires_grad=True)
model = dict(
    type='DepthEncoderDecoder',
    pretrained='./nfs/checkpoints/swin_large_patch4_window7_224_22k.pth',
    backbone=dict(
        type='SwinTransformer',
        pretrain_img_size=224,
        embed_dims=192,
        patch_size=4,
        window_size=7,
        mlp_ratio=4,
        depths=[2, 2, 18, 2],
        num_heads=[6, 12, 24, 48],
        strides=(4, 2, 2, 2),
        out_indices=(0, 1, 2, 3),
        qkv_bias=True,
        qk_scale=None,
        patch_norm=True,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.3,
        use_abs_pos_embed=False,
        act_cfg=dict(type='GELU'),
        norm_cfg=dict(type='LN', requires_grad=True),
        pretrain_style='official'),
    decode_head=dict(
        type='BinsFormerDecodeHead',
        in_channels=[192, 384, 768, 1536],
        up_sample_channels=[768, 384, 192, 96],
        channels=96,
        align_corners=True,
        loss_decode=dict(type='SigLoss', valid_mask=True, loss_weight=10),
        fpn=True,
        uncertainty=False,
        class_num=25,
        conv_dim=512,
        min_depth=0.001,
        max_depth=10,
        n_bins=64,
        index=[0, 1, 2, 3],
        trans_index=[1, 2, 3],
        chamfer_loss=False,
        loss_chamfer=dict(type='BinsChamferLoss', loss_weight=0.1),
        classify=True,
        loss_class=dict(type='CrossEntropyLoss', loss_weight=0.01),
        norm_cfg=dict(type='BN', requires_grad=True),
        transformer_encoder=dict(
            type='PureMSDEnTransformer',
            num_feature_levels=3,
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=512,
                        num_levels=3,
                        num_points=8),
                    ffn_cfgs=dict(
                        embed_dims=512,
                        feedforward_channels=1024,
                        ffn_dropout=0.1),
                    operation_order=('self_attn', 'norm', 'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding', num_feats=256, normalize=True),
        transformer_decoder=dict(
            type='PixelMSDeTransformer',
            decoder=dict(
                type='PixelTransformerDecoder',
                return_intermediate=True,
                num_layers=9,
                num_feature_levels=3,
                hidden_dim=512,
                transformerlayers=dict(
                    type='PixelTransformerDecoderLayer',
                    attn_cfgs=dict(
                        type='MultiheadAttention',
                        embed_dims=512,
                        num_heads=8,
                        dropout=0.0),
                    ffn_cfgs=dict(
                        embed_dims=512,
                        feedforward_channels=2048,
                        ffn_drop=0.0),
                    operation_order=('cross_attn', 'norm', 'self_attn', 'norm',
                                     'ffn', 'norm'))))),
    train_cfg=dict(aux_loss=True, aux_index=[2, 5], aux_weight=[0.25, 0.5]),
    test_cfg=dict(mode='whole'))
dataset_type = 'NYUBinFormerDataset'
data_root = 'data/nyu/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (416, 544)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='DepthLoadAnnotations'),
    dict(type='NYUCrop', depth=True),
    dict(type='RandomRotate', prob=0.5, degree=2.5),
    dict(type='RandomFlip', prob=0.5),
    dict(type='RandomCrop', crop_size=(416, 544)),
    dict(
        type='ColorAug',
        prob=0.5,
        gamma_range=[0.9, 1.1],
        brightness_range=[0.75, 1.25],
        color_range=[0.9, 1.1]),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'depth_gt', 'class_label'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(480, 640),
        flip=True,
        flip_direction='horizontal',
        transforms=[
            dict(type='RandomFlip', direction='horizontal'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='NYUBinFormerDataset',
        data_root='data/nyu/',
        depth_scale=1000,
        split='nyu_train.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='DepthLoadAnnotations'),
            dict(type='NYUCrop', depth=True),
            dict(type='RandomRotate', prob=0.5, degree=2.5),
            dict(type='RandomFlip', prob=0.5),
            dict(type='RandomCrop', crop_size=(416, 544)),
            dict(
                type='ColorAug',
                prob=0.5,
                gamma_range=[0.9, 1.1],
                brightness_range=[0.75, 1.25],
                color_range=[0.9, 1.1]),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'depth_gt', 'class_label'])
        ],
        garg_crop=False,
        eigen_crop=True,
        min_depth=0.001,
        max_depth=10),
    val=dict(
        type='NYUBinFormerDataset',
        data_root='data/nyu/',
        depth_scale=1000,
        split='nyu_test.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(480, 640),
                flip=True,
                flip_direction='horizontal',
                transforms=[
                    dict(type='RandomFlip', direction='horizontal'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        garg_crop=False,
        eigen_crop=True,
        min_depth=0.001,
        max_depth=10),
    test=dict(
        type='NYUBinFormerDataset',
        data_root='data/nyu/',
        depth_scale=1000,
        split='nyu_test.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(480, 640),
                flip=True,
                flip_direction='horizontal',
                transforms=[
                    dict(type='RandomFlip', direction='horizontal'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        garg_crop=False,
        eigen_crop=True,
        min_depth=0.001,
        max_depth=10))
log_config = dict(
    interval=100,
    hooks=[
        dict(type='TextLoggerHook', by_epoch=False),
        dict(type='TensorboardLoggerHook')
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=0.0001,
    betas=(0.9, 0.999),
    weight_decay=0.01,
    paramwise_cfg=dict(
        custom_keys=dict(
            absolute_pos_embed=dict(decay_mult=0.0),
            relative_position_bias_table=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0),
            backbone=dict(decay_mult=0.1))))
lr_config = dict(
    policy='OneCycle',
    max_lr=0.0001,
    warmup_iters=12800,
    div_factor=25,
    final_div_factor=100,
    by_epoch=False)
optimizer_config = dict(grad_clip=dict(max_norm=0.01, norm_type=2))
runner = dict(type='IterBasedRunner', max_iters=38400)
checkpoint_config = dict(by_epoch=False, max_keep_ckpts=2, interval=1600)
evaluation = dict(
    by_epoch=False,
    start=0,
    interval=800,
    pre_eval=True,
    rule='less',
    save_best='abs_rel',
    greater_keys=('a1', 'a2', 'a3'),
    less_keys=('abs_rel', 'rmse'))
find_unused_parameters = True
work_dir = './nfs/binsformer/binsformer_full_fpn_wcls_swinl_22k_nyu'
gpu_ids = range(0, 1)

2022-01-11 10:56:58,784 - depth - INFO - Use load_from_local loader
Name of parameter - Initialization information

backbone.patch_embed.projection.weight - torch.Size([192, 3, 4, 4]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.projection.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.norm.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.norm.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm1.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.qkv.bias - torch.Size([576]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.proj.weight - torch.Size([192, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.proj.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm2.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm2.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.0.0.weight - torch.Size([768, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.0.0.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.1.weight - torch.Size([192, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm1.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.qkv.bias - torch.Size([576]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.proj.weight - torch.Size([192, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.proj.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm2.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm2.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.0.0.weight - torch.Size([768, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.0.0.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.1.weight - torch.Size([192, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.norm.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.norm.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.reduction.weight - torch.Size([384, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.norm.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.norm.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.reduction.weight - torch.Size([768, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.norm.weight - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.norm.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.reduction.weight - torch.Size([1536, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm1.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm1.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 48]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.qkv.weight - torch.Size([4608, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.qkv.bias - torch.Size([4608]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.proj.weight - torch.Size([1536, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.proj.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm2.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm2.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.0.0.weight - torch.Size([6144, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.0.0.bias - torch.Size([6144]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.1.weight - torch.Size([1536, 6144]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.1.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm1.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm1.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 48]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.qkv.weight - torch.Size([4608, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.qkv.bias - torch.Size([4608]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.proj.weight - torch.Size([1536, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.proj.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm2.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm2.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.0.0.weight - torch.Size([6144, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.0.0.bias - torch.Size([6144]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.1.weight - torch.Size([1536, 6144]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.1.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.norm0.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm0.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm3.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm3.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_depth.weight - torch.Size([1, 96, 3, 3]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.conv_depth.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.level_embeds - torch.Size([3, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([384, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([192, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([1024, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([512, 1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.0.norms.0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.norms.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.norms.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.norms.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([384, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([192, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([1024, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([512, 1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.1.norms.0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.norms.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.norms.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.norms.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([384, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([192, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([1024, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([512, 1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.2.norms.0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.norms.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.norms.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.norms.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([384, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([192, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([1024, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([512, 1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.3.norms.0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.norms.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.norms.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.norms.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([384, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([192, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([1024, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([512, 1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.4.norms.0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.norms.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.norms.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.norms.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([384, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([192, 512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([1024, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([512, 1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.5.norms.0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.norms.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.norms.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.norms.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.reference_points.weight - torch.Size([2, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.level_embed.weight - torch.Size([3, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.skip_proj.0.conv.weight - torch.Size([512, 384, 1, 1]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.skip_proj.0.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.skip_proj.0.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.skip_proj.1.conv.weight - torch.Size([512, 768, 1, 1]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.skip_proj.1.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.skip_proj.1.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.skip_proj.2.conv.weight - torch.Size([512, 1536, 1, 1]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.skip_proj.2.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.skip_proj.2.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.lateral_convs.0.conv.weight - torch.Size([512, 768, 1, 1]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.lateral_convs.0.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.lateral_convs.0.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.lateral_convs.1.conv.weight - torch.Size([512, 384, 1, 1]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.lateral_convs.1.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.lateral_convs.1.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.lateral_convs.2.conv.weight - torch.Size([512, 192, 1, 1]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.lateral_convs.2.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.lateral_convs.2.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.output_convs.0.conv.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.output_convs.0.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.output_convs.0.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.output_convs.1.conv.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.output_convs.1.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.output_convs.1.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.output_convs.2.conv.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.output_convs.2.bn.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.output_convs.2.bn.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.query_feat.weight - torch.Size([65, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.query_embed.weight - torch.Size([65, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.attentions.1.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.attentions.1.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.attentions.1.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.attentions.1.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.norms.0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.norms.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.norms.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.norms.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.norms.2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.norms.2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.attentions.1.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.1.attentions.1.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.attentions.1.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.1.attentions.1.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.1.norms.0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.norms.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.norms.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.norms.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.norms.2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.norms.2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.attentions.1.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.2.attentions.1.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.attentions.1.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.2.attentions.1.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.2.norms.0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.norms.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.norms.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.norms.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.norms.2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.norms.2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.attentions.1.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.3.attentions.1.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.attentions.1.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.3.attentions.1.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.3.norms.0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.norms.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.norms.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.norms.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.norms.2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.norms.2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.attentions.1.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.4.attentions.1.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.attentions.1.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.4.attentions.1.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.4.norms.0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.norms.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.norms.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.norms.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.norms.2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.norms.2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.attentions.1.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.5.attentions.1.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.attentions.1.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.5.attentions.1.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.5.norms.0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.norms.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.norms.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.norms.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.norms.2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.norms.2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.attentions.0.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.6.attentions.0.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.attentions.0.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.6.attentions.0.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.attentions.1.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.6.attentions.1.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.attentions.1.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.6.attentions.1.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.ffns.0.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.6.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.6.ffns.0.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.6.ffns.0.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.6.norms.0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.norms.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.norms.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.norms.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.norms.2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.norms.2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.attentions.0.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.7.attentions.0.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.attentions.0.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.7.attentions.0.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.attentions.1.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.7.attentions.1.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.attentions.1.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.7.attentions.1.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.ffns.0.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.7.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.7.ffns.0.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.7.ffns.0.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.7.norms.0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.norms.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.norms.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.norms.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.norms.2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.norms.2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.attentions.0.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.8.attentions.0.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.attentions.0.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.8.attentions.0.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.attentions.1.attn.in_proj_weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.8.attentions.1.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.attentions.1.attn.out_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.8.attentions.1.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.ffns.0.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.8.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.8.ffns.0.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.8.ffns.0.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.8.norms.0.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.norms.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.norms.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.norms.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.norms.2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.norms.2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.post_norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.post_norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.bins_embed.weight - torch.Size([1, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.bins_embed.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.mask_embed.layers.0.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.mask_embed.layers.0.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.mask_embed.layers.1.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.mask_embed.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.mask_embed.layers.2.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.mask_embed.layers.2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.class_embed.layers.0.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.class_embed.layers.0.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.class_embed.layers.1.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.class_embed.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.class_embed.layers.2.weight - torch.Size([25, 512]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.class_embed.layers.2.bias - torch.Size([25]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.decoder_norm.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.decoder_norm.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  
2022-01-11 10:57:00,130 - depth - INFO - DepthEncoderDecoder(
  (backbone): SwinTransformer(
    (patch_embed): PatchEmbed(
      (projection): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    )
    (drop_after_pos): Dropout(p=0.0, inplace=False)
    (stages): ModuleList(
      (0): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=192, out_features=768, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=768, out_features=192, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=192, out_features=768, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=768, out_features=192, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (downsample): PatchMerging(
          (sampler): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=768, out_features=384, bias=False)
        )
      )
      (1): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=384, out_features=1536, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=1536, out_features=384, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=384, out_features=1536, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=1536, out_features=384, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (downsample): PatchMerging(
          (sampler): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
        )
      )
      (2): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (3): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (4): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (5): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (6): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (7): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (8): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (9): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (10): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (11): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (12): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (13): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (14): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (15): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (16): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (17): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (downsample): PatchMerging(
          (sampler): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)
          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=3072, out_features=1536, bias=False)
        )
      )
      (3): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=1536, out_features=4608, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1536, out_features=1536, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1536, out_features=6144, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=6144, out_features=1536, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=1536, out_features=4608, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1536, out_features=1536, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1536, out_features=6144, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=6144, out_features=1536, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
      )
    )
    (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (decode_head): BinsFormerDecodeHead(
    align_corners=True
    (loss_decode): SigLoss()
    (conv_depth): Conv2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu): ReLU()
    (sigmoid): Sigmoid()
    (transformer_encoder): PureMSDEnTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=384, bias=True)
                (attention_weights): Linear(in_features=512, out_features=192, bias=True)
                (value_proj): Linear(in_features=512, out_features=512, bias=True)
                (output_proj): Linear(in_features=512, out_features=512, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=512, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=512, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=384, bias=True)
                (attention_weights): Linear(in_features=512, out_features=192, bias=True)
                (value_proj): Linear(in_features=512, out_features=512, bias=True)
                (output_proj): Linear(in_features=512, out_features=512, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=512, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=512, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=384, bias=True)
                (attention_weights): Linear(in_features=512, out_features=192, bias=True)
                (value_proj): Linear(in_features=512, out_features=512, bias=True)
                (output_proj): Linear(in_features=512, out_features=512, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=512, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=512, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=384, bias=True)
                (attention_weights): Linear(in_features=512, out_features=192, bias=True)
                (value_proj): Linear(in_features=512, out_features=512, bias=True)
                (output_proj): Linear(in_features=512, out_features=512, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=512, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=512, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=384, bias=True)
                (attention_weights): Linear(in_features=512, out_features=192, bias=True)
                (value_proj): Linear(in_features=512, out_features=512, bias=True)
                (output_proj): Linear(in_features=512, out_features=512, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=512, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=512, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=384, bias=True)
                (attention_weights): Linear(in_features=512, out_features=192, bias=True)
                (value_proj): Linear(in_features=512, out_features=512, bias=True)
                (output_proj): Linear(in_features=512, out_features=512, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=512, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=512, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=512, out_features=2, bias=True)
    )
    (positional_encoding): SinePositionalEncoding(num_feats=256, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (pe_layer): SinePositionalEncoding(num_feats=256, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (level_embed): Embedding(3, 512)
    (skip_proj): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (output_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (loss_class): CrossEntropyLoss()
    (query_feat): Embedding(65, 512)
    (query_embed): Embedding(65, 512)
    (transformer_decoder): PixelMSDeTransformer(
      (decoder): PixelTransformerDecoder(
        (layers): ModuleList(
          (0): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=512, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=512, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=512, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=512, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=512, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=512, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=512, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=512, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=512, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=512, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=512, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=512, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (6): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=512, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=512, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (7): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=512, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=512, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
          (8): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=512, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=512, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (bins_embed): Linear(in_features=512, out_features=1, bias=True)
        (mask_embed): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
          )
        )
        (class_embed): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=25, bias=True)
          )
        )
        (decoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
2022-01-11 10:57:00,239 - depth - INFO - Loaded 24231 images. Totally 0 invalid pairs are filtered
2022-01-11 10:57:07,765 - depth - INFO - Loaded 654 images. Totally 0 invalid pairs are filtered
2022-01-11 10:57:07,765 - depth - INFO - Start running, host: lizhenyu2@lzy-dev, work_dir: /nfs/lizhenyu2/codes/DepthFormer_Release/nfs/binsformer/binsformer_full_fpn_wcls_swinl_22k_nyu
2022-01-11 10:57:07,766 - depth - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) OneCycleLrUpdaterHook              
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) OneCycleLrUpdaterHook              
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) OneCycleLrUpdaterHook              
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2022-01-11 10:57:07,766 - depth - INFO - workflow: [('train', 1)], max: 38400 iters
2022-01-11 10:58:22,439 - depth - INFO - Iter [100/38400]	lr: 4.017e-06, eta: 6:54:27, time: 0.649, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0308, decode.aux_loss_depth_2: 1.0398, decode.aux_loss_ce_5: 0.0328, decode.aux_loss_depth_5: 2.0632, decode.loss_ce: 0.0326, decode.ce_acc_level_0: 1.1875, decode.ce_acc_level_1: 24.4375, decode.loss_depth: 4.2713, loss: 7.4705, grad_norm: 44.9190
2022-01-11 10:59:22,144 - depth - INFO - Iter [200/38400]	lr: 4.071e-06, eta: 6:36:46, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0300, decode.aux_loss_depth_2: 0.8263, decode.aux_loss_ce_5: 0.0326, decode.aux_loss_depth_5: 1.5976, decode.loss_ce: 0.0309, decode.ce_acc_level_0: 11.0625, decode.ce_acc_level_1: 32.3125, decode.loss_depth: 3.2124, loss: 5.7296, grad_norm: 77.7490
2022-01-11 11:00:22,344 - depth - INFO - Iter [300/38400]	lr: 4.160e-06, eta: 6:31:13, time: 0.602, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0293, decode.aux_loss_depth_2: 0.6876, decode.aux_loss_ce_5: 0.0309, decode.aux_loss_depth_5: 1.3459, decode.loss_ce: 0.0296, decode.ce_acc_level_0: 20.3125, decode.ce_acc_level_1: 45.6250, decode.loss_depth: 2.6902, loss: 4.8135, grad_norm: 61.4561
2022-01-11 11:01:22,274 - depth - INFO - Iter [400/38400]	lr: 4.284e-06, eta: 6:27:33, time: 0.599, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0273, decode.aux_loss_depth_2: 0.6187, decode.aux_loss_ce_5: 0.0290, decode.aux_loss_depth_5: 1.2129, decode.loss_ce: 0.0287, decode.ce_acc_level_0: 19.0625, decode.ce_acc_level_1: 55.7500, decode.loss_depth: 2.4338, loss: 4.3503, grad_norm: 52.5689
2022-01-11 11:02:21,569 - depth - INFO - Iter [500/38400]	lr: 4.444e-06, eta: 6:24:08, time: 0.593, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0270, decode.aux_loss_depth_2: 0.5903, decode.aux_loss_ce_5: 0.0282, decode.aux_loss_depth_5: 1.1629, decode.loss_ce: 0.0278, decode.ce_acc_level_0: 21.0625, decode.ce_acc_level_1: 61.1875, decode.loss_depth: 2.3270, loss: 4.1631, grad_norm: 49.1130
2022-01-11 11:03:21,111 - depth - INFO - Iter [600/38400]	lr: 4.639e-06, eta: 6:21:46, time: 0.595, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0261, decode.aux_loss_depth_2: 0.5700, decode.aux_loss_ce_5: 0.0269, decode.aux_loss_depth_5: 1.1265, decode.loss_ce: 0.0264, decode.ce_acc_level_0: 21.8750, decode.ce_acc_level_1: 68.8750, decode.loss_depth: 2.2561, loss: 4.0320, grad_norm: 52.0581
2022-01-11 11:04:21,317 - depth - INFO - Iter [700/38400]	lr: 4.870e-06, eta: 6:20:25, time: 0.602, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0257, decode.aux_loss_depth_2: 0.5522, decode.aux_loss_ce_5: 0.0265, decode.aux_loss_depth_5: 1.0915, decode.loss_ce: 0.0263, decode.ce_acc_level_0: 18.6875, decode.ce_acc_level_1: 67.6875, decode.loss_depth: 2.1893, loss: 3.9115, grad_norm: 49.5281
2022-01-11 11:05:21,480 - depth - INFO - Iter [800/38400]	lr: 5.135e-06, eta: 6:19:05, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0254, decode.aux_loss_depth_2: 0.5311, decode.aux_loss_ce_5: 0.0261, decode.aux_loss_depth_5: 1.0528, decode.loss_ce: 0.0257, decode.ce_acc_level_0: 21.4375, decode.ce_acc_level_1: 70.2500, decode.loss_depth: 2.1091, loss: 3.7702, grad_norm: 47.8712
2022-01-11 11:05:53,474 - depth - INFO - Summary:
2022-01-11 11:05:53,475 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.7097 | 0.9517 | 0.9913 |  0.1959 | 0.5793 | 0.076  |  0.2126  | 15.5509 | 0.1482 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-11 11:06:08,683 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_800.pth.
2022-01-11 11:06:08,684 - depth - INFO - Best abs_rel is 0.1959 at 800 iter.
2022-01-11 11:06:08,685 - depth - INFO - Iter(val) [82]	a1: 0.7097, a2: 0.9517, a3: 0.9913, abs_rel: 0.1958841234445572, rmse: 0.5792787671089172, log_10: 0.07595061510801315, rmse_log: 0.21262770891189575, silog: 15.5509, sq_rel: 0.14823059737682343
2022-01-11 11:07:08,450 - depth - INFO - Iter [900/38400]	lr: 5.436e-06, eta: 6:50:22, time: 1.070, data_time: 0.480, memory: 30431, decode.aux_loss_ce_2: 0.0253, decode.aux_loss_depth_2: 0.5207, decode.aux_loss_ce_5: 0.0254, decode.aux_loss_depth_5: 1.0338, decode.loss_ce: 0.0253, decode.ce_acc_level_0: 22.1250, decode.ce_acc_level_1: 69.5625, decode.loss_depth: 2.0746, loss: 3.7052, grad_norm: 46.9753
2022-01-11 11:08:08,636 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 11:08:08,636 - depth - INFO - Iter [1000/38400]	lr: 5.771e-06, eta: 6:45:51, time: 0.602, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0251, decode.aux_loss_depth_2: 0.5123, decode.aux_loss_ce_5: 0.0251, decode.aux_loss_depth_5: 1.0161, decode.loss_ce: 0.0250, decode.ce_acc_level_0: 22.3125, decode.ce_acc_level_1: 69.7500, decode.loss_depth: 2.0314, loss: 3.6350, grad_norm: 43.3477
2022-01-11 11:09:08,323 - depth - INFO - Iter [1100/38400]	lr: 6.140e-06, eta: 6:41:42, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0246, decode.aux_loss_depth_2: 0.4994, decode.aux_loss_ce_5: 0.0250, decode.aux_loss_depth_5: 0.9875, decode.loss_ce: 0.0251, decode.ce_acc_level_0: 20.5000, decode.ce_acc_level_1: 69.1875, decode.loss_depth: 1.9800, loss: 3.5418, grad_norm: 39.6617
2022-01-11 11:10:07,637 - depth - INFO - Iter [1200/38400]	lr: 6.544e-06, eta: 6:37:53, time: 0.593, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0249, decode.aux_loss_depth_2: 0.4901, decode.aux_loss_ce_5: 0.0253, decode.aux_loss_depth_5: 0.9661, decode.loss_ce: 0.0255, decode.ce_acc_level_0: 18.0625, decode.ce_acc_level_1: 69.5000, decode.loss_depth: 1.9407, loss: 3.4727, grad_norm: 45.1958
2022-01-11 11:11:07,105 - depth - INFO - Iter [1300/38400]	lr: 6.981e-06, eta: 6:34:34, time: 0.595, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0247, decode.aux_loss_depth_2: 0.4847, decode.aux_loss_ce_5: 0.0254, decode.aux_loss_depth_5: 0.9627, decode.loss_ce: 0.0252, decode.ce_acc_level_0: 20.2500, decode.ce_acc_level_1: 70.0625, decode.loss_depth: 1.9271, loss: 3.4499, grad_norm: 42.1541
2022-01-11 11:12:07,035 - depth - INFO - Iter [1400/38400]	lr: 7.452e-06, eta: 6:31:49, time: 0.600, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0245, decode.aux_loss_depth_2: 0.4784, decode.aux_loss_ce_5: 0.0247, decode.aux_loss_depth_5: 0.9548, decode.loss_ce: 0.0247, decode.ce_acc_level_0: 21.6875, decode.ce_acc_level_1: 70.8750, decode.loss_depth: 1.9168, loss: 3.4238, grad_norm: 41.8376
2022-01-11 11:13:06,894 - depth - INFO - Iter [1500/38400]	lr: 7.956e-06, eta: 6:29:13, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0241, decode.aux_loss_depth_2: 0.4623, decode.aux_loss_ce_5: 0.0244, decode.aux_loss_depth_5: 0.9159, decode.loss_ce: 0.0247, decode.ce_acc_level_0: 20.6875, decode.ce_acc_level_1: 70.3125, decode.loss_depth: 1.8360, loss: 3.2873, grad_norm: 37.7168
2022-01-11 11:14:06,475 - depth - INFO - Saving checkpoint at 1600 iterations
2022-01-11 11:14:22,346 - depth - INFO - Iter [1600/38400]	lr: 8.492e-06, eta: 6:32:51, time: 0.755, data_time: 0.010, memory: 30431, decode.aux_loss_ce_2: 0.0241, decode.aux_loss_depth_2: 0.4508, decode.aux_loss_ce_5: 0.0244, decode.aux_loss_depth_5: 0.8961, decode.loss_ce: 0.0245, decode.ce_acc_level_0: 19.0625, decode.ce_acc_level_1: 70.9375, decode.loss_depth: 1.7914, loss: 3.2113, grad_norm: 37.6959
2022-01-11 11:14:43,991 - depth - INFO - Summary:
2022-01-11 11:14:43,991 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.7838 | 0.9713 | 0.9954 |  0.1602 | 0.4919 | 0.0649 |  0.1828  | 13.5662 | 0.1045 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-11 11:14:57,557 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_1600.pth.
2022-01-11 11:14:57,558 - depth - INFO - Best abs_rel is 0.1602 at 1600 iter.
2022-01-11 11:14:57,559 - depth - INFO - Iter(val) [82]	a1: 0.7838, a2: 0.9713, a3: 0.9954, abs_rel: 0.1602426916360855, rmse: 0.49187785387039185, log_10: 0.06493029743432999, rmse_log: 0.1827888935804367, silog: 13.5662, sq_rel: 0.10449553281068802
2022-01-11 11:15:57,449 - depth - INFO - Iter [1700/38400]	lr: 9.062e-06, eta: 6:42:57, time: 0.951, data_time: 0.359, memory: 30431, decode.aux_loss_ce_2: 0.0240, decode.aux_loss_depth_2: 0.4354, decode.aux_loss_ce_5: 0.0243, decode.aux_loss_depth_5: 0.8648, decode.loss_ce: 0.0248, decode.ce_acc_level_0: 22.0000, decode.ce_acc_level_1: 69.2500, decode.loss_depth: 1.7320, loss: 3.1053, grad_norm: 35.4153
2022-01-11 11:16:56,807 - depth - INFO - Iter [1800/38400]	lr: 9.663e-06, eta: 6:39:39, time: 0.593, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0242, decode.aux_loss_depth_2: 0.4340, decode.aux_loss_ce_5: 0.0247, decode.aux_loss_depth_5: 0.8650, decode.loss_ce: 0.0250, decode.ce_acc_level_0: 20.8750, decode.ce_acc_level_1: 68.3125, decode.loss_depth: 1.7329, loss: 3.1059, grad_norm: 36.4895
2022-01-11 11:17:56,761 - depth - INFO - Iter [1900/38400]	lr: 1.030e-05, eta: 6:36:46, time: 0.600, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0237, decode.aux_loss_depth_2: 0.4282, decode.aux_loss_ce_5: 0.0240, decode.aux_loss_depth_5: 0.8505, decode.loss_ce: 0.0243, decode.ce_acc_level_0: 21.9375, decode.ce_acc_level_1: 72.6250, decode.loss_depth: 1.6992, loss: 3.0500, grad_norm: 36.8657
2022-01-11 11:18:56,466 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 11:18:56,466 - depth - INFO - Iter [2000/38400]	lr: 1.096e-05, eta: 6:34:01, time: 0.597, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0238, decode.aux_loss_depth_2: 0.4086, decode.aux_loss_ce_5: 0.0240, decode.aux_loss_depth_5: 0.8095, decode.loss_ce: 0.0245, decode.ce_acc_level_0: 24.5625, decode.ce_acc_level_1: 70.6875, decode.loss_depth: 1.6217, loss: 2.9120, grad_norm: 33.0095
2022-01-11 11:19:56,131 - depth - INFO - Iter [2100/38400]	lr: 1.165e-05, eta: 6:31:24, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0237, decode.aux_loss_depth_2: 0.4034, decode.aux_loss_ce_5: 0.0243, decode.aux_loss_depth_5: 0.8007, decode.loss_ce: 0.0248, decode.ce_acc_level_0: 21.8125, decode.ce_acc_level_1: 69.2500, decode.loss_depth: 1.6001, loss: 2.8769, grad_norm: 31.4540
2022-01-11 11:20:55,383 - depth - INFO - Iter [2200/38400]	lr: 1.238e-05, eta: 6:28:50, time: 0.593, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0228, decode.aux_loss_depth_2: 0.3929, decode.aux_loss_ce_5: 0.0233, decode.aux_loss_depth_5: 0.7834, decode.loss_ce: 0.0240, decode.ce_acc_level_0: 26.0625, decode.ce_acc_level_1: 71.5000, decode.loss_depth: 1.5655, loss: 2.8119, grad_norm: 31.0763
2022-01-11 11:21:54,937 - depth - INFO - Iter [2300/38400]	lr: 1.313e-05, eta: 6:26:29, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0231, decode.aux_loss_depth_2: 0.3917, decode.aux_loss_ce_5: 0.0234, decode.aux_loss_depth_5: 0.7815, decode.loss_ce: 0.0241, decode.ce_acc_level_0: 24.1250, decode.ce_acc_level_1: 72.5625, decode.loss_depth: 1.5633, loss: 2.8071, grad_norm: 29.8033
2022-01-11 11:22:54,387 - depth - INFO - Iter [2400/38400]	lr: 1.391e-05, eta: 6:24:12, time: 0.594, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0231, decode.aux_loss_depth_2: 0.3805, decode.aux_loss_ce_5: 0.0234, decode.aux_loss_depth_5: 0.7603, decode.loss_ce: 0.0240, decode.ce_acc_level_0: 25.1250, decode.ce_acc_level_1: 72.5625, decode.loss_depth: 1.5178, loss: 2.7291, grad_norm: 30.4777
2022-01-11 11:23:16,274 - depth - INFO - Summary:
2022-01-11 11:23:16,275 - depth - INFO - 
+-------+-------+--------+---------+-------+--------+----------+---------+--------+
|   a1  |   a2  |   a3   | abs_rel |  rmse | log_10 | rmse_log |  silog  | sq_rel |
+-------+-------+--------+---------+-------+--------+----------+---------+--------+
| 0.852 | 0.984 | 0.9975 |  0.127  | 0.434 | 0.0536 |  0.1549  | 12.3643 | 0.0792 |
+-------+-------+--------+---------+-------+--------+----------+---------+--------+
2022-01-11 11:23:32,522 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_2400.pth.
2022-01-11 11:23:32,522 - depth - INFO - Best abs_rel is 0.1270 at 2400 iter.
2022-01-11 11:23:32,524 - depth - INFO - Iter(val) [82]	a1: 0.8520, a2: 0.9840, a3: 0.9975, abs_rel: 0.12697911262512207, rmse: 0.434048056602478, log_10: 0.05360062047839165, rmse_log: 0.15485879778862, silog: 12.3643, sq_rel: 0.07918046414852142
2022-01-11 11:24:32,752 - depth - INFO - Iter [2500/38400]	lr: 1.472e-05, eta: 6:31:20, time: 0.983, data_time: 0.389, memory: 30431, decode.aux_loss_ce_2: 0.0225, decode.aux_loss_depth_2: 0.3767, decode.aux_loss_ce_5: 0.0228, decode.aux_loss_depth_5: 0.7522, decode.loss_ce: 0.0237, decode.ce_acc_level_0: 22.7500, decode.ce_acc_level_1: 74.2500, decode.loss_depth: 1.5044, loss: 2.7022, grad_norm: 28.9631
2022-01-11 11:25:32,363 - depth - INFO - Iter [2600/38400]	lr: 1.556e-05, eta: 6:28:56, time: 0.597, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0227, decode.aux_loss_depth_2: 0.3634, decode.aux_loss_ce_5: 0.0231, decode.aux_loss_depth_5: 0.7237, decode.loss_ce: 0.0237, decode.ce_acc_level_0: 26.3125, decode.ce_acc_level_1: 72.9375, decode.loss_depth: 1.4462, loss: 2.6028, grad_norm: 26.3734
2022-01-11 11:26:32,556 - depth - INFO - Iter [2700/38400]	lr: 1.643e-05, eta: 6:26:44, time: 0.602, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0221, decode.aux_loss_depth_2: 0.3666, decode.aux_loss_ce_5: 0.0227, decode.aux_loss_depth_5: 0.7371, decode.loss_ce: 0.0231, decode.ce_acc_level_0: 28.8750, decode.ce_acc_level_1: 75.3125, decode.loss_depth: 1.4710, loss: 2.6425, grad_norm: 29.2977
2022-01-11 11:27:32,784 - depth - INFO - Iter [2800/38400]	lr: 1.732e-05, eta: 6:24:39, time: 0.603, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0215, decode.aux_loss_depth_2: 0.3512, decode.aux_loss_ce_5: 0.0219, decode.aux_loss_depth_5: 0.7031, decode.loss_ce: 0.0228, decode.ce_acc_level_0: 27.1250, decode.ce_acc_level_1: 76.5625, decode.loss_depth: 1.4046, loss: 2.5250, grad_norm: 26.9338
2022-01-11 11:28:32,781 - depth - INFO - Iter [2900/38400]	lr: 1.824e-05, eta: 6:22:35, time: 0.600, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0217, decode.aux_loss_depth_2: 0.3477, decode.aux_loss_ce_5: 0.0222, decode.aux_loss_depth_5: 0.6985, decode.loss_ce: 0.0228, decode.ce_acc_level_0: 28.5625, decode.ce_acc_level_1: 75.6250, decode.loss_depth: 1.3940, loss: 2.5069, grad_norm: 38.4420
2022-01-11 11:29:32,379 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 11:29:32,379 - depth - INFO - Iter [3000/38400]	lr: 1.918e-05, eta: 6:20:30, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0213, decode.aux_loss_depth_2: 0.3428, decode.aux_loss_ce_5: 0.0219, decode.aux_loss_depth_5: 0.6832, decode.loss_ce: 0.0226, decode.ce_acc_level_0: 27.1875, decode.ce_acc_level_1: 78.4375, decode.loss_depth: 1.3638, loss: 2.4555, grad_norm: 27.9650
2022-01-11 11:30:32,300 - depth - INFO - Iter [3100/38400]	lr: 2.015e-05, eta: 6:18:34, time: 0.599, data_time: 0.011, memory: 30431, decode.aux_loss_ce_2: 0.0216, decode.aux_loss_depth_2: 0.3377, decode.aux_loss_ce_5: 0.0224, decode.aux_loss_depth_5: 0.6753, decode.loss_ce: 0.0228, decode.ce_acc_level_0: 26.5625, decode.ce_acc_level_1: 77.5000, decode.loss_depth: 1.3468, loss: 2.4266, grad_norm: 23.9203
2022-01-11 11:31:32,862 - depth - INFO - Saving checkpoint at 3200 iterations
2022-01-11 11:31:48,340 - depth - INFO - Iter [3200/38400]	lr: 2.114e-05, eta: 6:19:39, time: 0.761, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0218, decode.aux_loss_depth_2: 0.3301, decode.aux_loss_ce_5: 0.0226, decode.aux_loss_depth_5: 0.6640, decode.loss_ce: 0.0231, decode.ce_acc_level_0: 28.0625, decode.ce_acc_level_1: 74.7500, decode.loss_depth: 1.3262, loss: 2.3879, grad_norm: 24.1758
2022-01-11 11:32:10,042 - depth - INFO - Summary:
2022-01-11 11:32:10,043 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.8569 | 0.9823 | 0.9975 |  0.1287 | 0.4465 | 0.0539 |  0.1538  | 11.955 | 0.0795 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 11:32:10,043 - depth - INFO - Iter(val) [82]	a1: 0.8569, a2: 0.9823, a3: 0.9975, abs_rel: 0.12874282896518707, rmse: 0.4465421736240387, log_10: 0.05388465151190758, rmse_log: 0.15383894741535187, silog: 11.9550, sq_rel: 0.07946378737688065
2022-01-11 11:33:10,401 - depth - INFO - Iter [3300/38400]	lr: 2.215e-05, eta: 6:21:38, time: 0.820, data_time: 0.225, memory: 30431, decode.aux_loss_ce_2: 0.0209, decode.aux_loss_depth_2: 0.3305, decode.aux_loss_ce_5: 0.0219, decode.aux_loss_depth_5: 0.6638, decode.loss_ce: 0.0226, decode.ce_acc_level_0: 30.7500, decode.ce_acc_level_1: 75.8750, decode.loss_depth: 1.3178, loss: 2.3774, grad_norm: 23.1736
2022-01-11 11:34:10,213 - depth - INFO - Iter [3400/38400]	lr: 2.319e-05, eta: 6:19:37, time: 0.598, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0214, decode.aux_loss_depth_2: 0.3292, decode.aux_loss_ce_5: 0.0222, decode.aux_loss_depth_5: 0.6589, decode.loss_ce: 0.0230, decode.ce_acc_level_0: 27.5625, decode.ce_acc_level_1: 75.6250, decode.loss_depth: 1.3186, loss: 2.3732, grad_norm: 22.2177
2022-01-11 11:35:10,398 - depth - INFO - Iter [3500/38400]	lr: 2.425e-05, eta: 6:17:43, time: 0.602, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0209, decode.aux_loss_depth_2: 0.3318, decode.aux_loss_ce_5: 0.0217, decode.aux_loss_depth_5: 0.6642, decode.loss_ce: 0.0228, decode.ce_acc_level_0: 30.1250, decode.ce_acc_level_1: 76.8750, decode.loss_depth: 1.3353, loss: 2.3968, grad_norm: 21.9000
2022-01-11 11:36:10,763 - depth - INFO - Iter [3600/38400]	lr: 2.533e-05, eta: 6:15:54, time: 0.604, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0207, decode.aux_loss_depth_2: 0.3171, decode.aux_loss_ce_5: 0.0214, decode.aux_loss_depth_5: 0.6353, decode.loss_ce: 0.0224, decode.ce_acc_level_0: 29.9375, decode.ce_acc_level_1: 76.3750, decode.loss_depth: 1.2799, loss: 2.2967, grad_norm: 20.6697
2022-01-11 11:37:11,029 - depth - INFO - Iter [3700/38400]	lr: 2.642e-05, eta: 6:14:07, time: 0.603, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0207, decode.aux_loss_depth_2: 0.3212, decode.aux_loss_ce_5: 0.0214, decode.aux_loss_depth_5: 0.6489, decode.loss_ce: 0.0224, decode.ce_acc_level_0: 30.5000, decode.ce_acc_level_1: 76.3125, decode.loss_depth: 1.2993, loss: 2.3338, grad_norm: 23.2546
2022-01-11 11:38:11,374 - depth - INFO - Iter [3800/38400]	lr: 2.754e-05, eta: 6:12:22, time: 0.603, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0206, decode.aux_loss_depth_2: 0.3138, decode.aux_loss_ce_5: 0.0212, decode.aux_loss_depth_5: 0.6307, decode.loss_ce: 0.0223, decode.ce_acc_level_0: 29.8750, decode.ce_acc_level_1: 77.6875, decode.loss_depth: 1.2604, loss: 2.2691, grad_norm: 22.7214
2022-01-11 11:39:11,488 - depth - INFO - Iter [3900/38400]	lr: 2.868e-05, eta: 6:10:38, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0208, decode.aux_loss_depth_2: 0.3081, decode.aux_loss_ce_5: 0.0214, decode.aux_loss_depth_5: 0.6185, decode.loss_ce: 0.0224, decode.ce_acc_level_0: 29.3125, decode.ce_acc_level_1: 77.3125, decode.loss_depth: 1.2365, loss: 2.2277, grad_norm: 18.1694
2022-01-11 11:40:11,271 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 11:40:11,271 - depth - INFO - Iter [4000/38400]	lr: 2.983e-05, eta: 6:08:54, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0199, decode.aux_loss_depth_2: 0.2998, decode.aux_loss_ce_5: 0.0208, decode.aux_loss_depth_5: 0.6017, decode.loss_ce: 0.0218, decode.ce_acc_level_0: 32.5625, decode.ce_acc_level_1: 78.1250, decode.loss_depth: 1.2044, loss: 2.1684, grad_norm: 17.5250
2022-01-11 11:40:33,057 - depth - INFO - Summary:
2022-01-11 11:40:33,058 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8441 | 0.9819 | 0.9968 |  0.1393 | 0.4447 | 0.0555 |  0.1571  | 11.3607 | 0.0855 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-11 11:40:33,059 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 11:40:33,059 - depth - INFO - Iter(val) [82]	a1: 0.8441, a2: 0.9819, a3: 0.9968, abs_rel: 0.13927112519741058, rmse: 0.44471511244773865, log_10: 0.055466242134571075, rmse_log: 0.15706872940063477, silog: 11.3607, sq_rel: 0.08548684418201447
2022-01-11 11:41:33,249 - depth - INFO - Iter [4100/38400]	lr: 3.100e-05, eta: 6:10:17, time: 0.820, data_time: 0.224, memory: 30431, decode.aux_loss_ce_2: 0.0197, decode.aux_loss_depth_2: 0.3033, decode.aux_loss_ce_5: 0.0204, decode.aux_loss_depth_5: 0.6103, decode.loss_ce: 0.0214, decode.ce_acc_level_0: 32.8125, decode.ce_acc_level_1: 80.0000, decode.loss_depth: 1.2208, loss: 2.1960, grad_norm: 17.9965
2022-01-11 11:42:33,657 - depth - INFO - Iter [4200/38400]	lr: 3.218e-05, eta: 6:08:36, time: 0.604, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0200, decode.aux_loss_depth_2: 0.3022, decode.aux_loss_ce_5: 0.0207, decode.aux_loss_depth_5: 0.6034, decode.loss_ce: 0.0215, decode.ce_acc_level_0: 33.0000, decode.ce_acc_level_1: 78.7500, decode.loss_depth: 1.2095, loss: 2.1772, grad_norm: 19.3372
2022-01-11 11:43:33,815 - depth - INFO - Iter [4300/38400]	lr: 3.338e-05, eta: 6:06:56, time: 0.601, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0196, decode.aux_loss_depth_2: 0.2923, decode.aux_loss_ce_5: 0.0203, decode.aux_loss_depth_5: 0.5867, decode.loss_ce: 0.0212, decode.ce_acc_level_0: 33.9375, decode.ce_acc_level_1: 79.3750, decode.loss_depth: 1.1709, loss: 2.1109, grad_norm: 17.5970
2022-01-11 11:44:34,416 - depth - INFO - Iter [4400/38400]	lr: 3.460e-05, eta: 6:05:21, time: 0.606, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0196, decode.aux_loss_depth_2: 0.2978, decode.aux_loss_ce_5: 0.0203, decode.aux_loss_depth_5: 0.5989, decode.loss_ce: 0.0209, decode.ce_acc_level_0: 35.2500, decode.ce_acc_level_1: 78.1250, decode.loss_depth: 1.1945, loss: 2.1520, grad_norm: 16.4092
2022-01-11 11:45:34,625 - depth - INFO - Iter [4500/38400]	lr: 3.582e-05, eta: 6:03:44, time: 0.602, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0196, decode.aux_loss_depth_2: 0.2946, decode.aux_loss_ce_5: 0.0202, decode.aux_loss_depth_5: 0.5904, decode.loss_ce: 0.0208, decode.ce_acc_level_0: 36.0625, decode.ce_acc_level_1: 78.8750, decode.loss_depth: 1.1799, loss: 2.1255, grad_norm: 15.9774
2022-01-11 11:46:34,813 - depth - INFO - Iter [4600/38400]	lr: 3.706e-05, eta: 6:02:09, time: 0.601, data_time: 0.010, memory: 30431, decode.aux_loss_ce_2: 0.0193, decode.aux_loss_depth_2: 0.2814, decode.aux_loss_ce_5: 0.0202, decode.aux_loss_depth_5: 0.5649, decode.loss_ce: 0.0208, decode.ce_acc_level_0: 35.9375, decode.ce_acc_level_1: 79.5000, decode.loss_depth: 1.1255, loss: 2.0321, grad_norm: 16.0788
2022-01-11 11:47:35,251 - depth - INFO - Iter [4700/38400]	lr: 3.831e-05, eta: 6:00:37, time: 0.604, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0191, decode.aux_loss_depth_2: 0.2882, decode.aux_loss_ce_5: 0.0197, decode.aux_loss_depth_5: 0.5768, decode.loss_ce: 0.0207, decode.ce_acc_level_0: 34.5000, decode.ce_acc_level_1: 80.1875, decode.loss_depth: 1.1534, loss: 2.0779, grad_norm: 18.6701
2022-01-11 11:48:35,227 - depth - INFO - Saving checkpoint at 4800 iterations
2022-01-11 11:48:50,657 - depth - INFO - Iter [4800/38400]	lr: 3.957e-05, eta: 6:00:51, time: 0.755, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0186, decode.aux_loss_depth_2: 0.2853, decode.aux_loss_ce_5: 0.0194, decode.aux_loss_depth_5: 0.5705, decode.loss_ce: 0.0204, decode.ce_acc_level_0: 36.0625, decode.ce_acc_level_1: 80.2500, decode.loss_depth: 1.1387, loss: 2.0531, grad_norm: 13.7846
2022-01-11 11:49:12,285 - depth - INFO - Summary:
2022-01-11 11:49:12,286 - depth - INFO - 
+--------+-------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2  |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+-------+--------+---------+--------+--------+----------+---------+--------+
| 0.8871 | 0.987 | 0.9976 |  0.1127 | 0.3926 | 0.0485 |  0.1397  | 11.0443 | 0.0599 |
+--------+-------+--------+---------+--------+--------+----------+---------+--------+
2022-01-11 11:49:29,065 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_4800.pth.
2022-01-11 11:49:29,066 - depth - INFO - Best abs_rel is 0.1127 at 4800 iter.
2022-01-11 11:49:29,068 - depth - INFO - Iter(val) [82]	a1: 0.8871, a2: 0.9870, a3: 0.9976, abs_rel: 0.11269491910934448, rmse: 0.3926411271095276, log_10: 0.04854324832558632, rmse_log: 0.13969019055366516, silog: 11.0443, sq_rel: 0.059885382652282715
2022-01-11 11:50:29,001 - depth - INFO - Iter [4900/38400]	lr: 4.084e-05, eta: 6:03:39, time: 0.983, data_time: 0.391, memory: 30431, decode.aux_loss_ce_2: 0.0188, decode.aux_loss_depth_2: 0.2785, decode.aux_loss_ce_5: 0.0196, decode.aux_loss_depth_5: 0.5560, decode.loss_ce: 0.0203, decode.ce_acc_level_0: 39.4375, decode.ce_acc_level_1: 79.6250, decode.loss_depth: 1.1128, loss: 2.0061, grad_norm: 14.6296
2022-01-11 11:51:29,837 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 11:51:29,838 - depth - INFO - Iter [5000/38400]	lr: 4.212e-05, eta: 6:02:05, time: 0.608, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0193, decode.aux_loss_depth_2: 0.2746, decode.aux_loss_ce_5: 0.0199, decode.aux_loss_depth_5: 0.5500, decode.loss_ce: 0.0208, decode.ce_acc_level_0: 35.5000, decode.ce_acc_level_1: 79.8750, decode.loss_depth: 1.1007, loss: 1.9854, grad_norm: 15.5025
2022-01-11 11:52:30,408 - depth - INFO - Iter [5100/38400]	lr: 4.340e-05, eta: 6:00:30, time: 0.606, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0186, decode.aux_loss_depth_2: 0.2740, decode.aux_loss_ce_5: 0.0194, decode.aux_loss_depth_5: 0.5472, decode.loss_ce: 0.0198, decode.ce_acc_level_0: 39.3750, decode.ce_acc_level_1: 80.8750, decode.loss_depth: 1.0942, loss: 1.9731, grad_norm: 16.4204
2022-01-11 11:53:30,442 - depth - INFO - Iter [5200/38400]	lr: 4.469e-05, eta: 5:58:54, time: 0.600, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0183, decode.aux_loss_depth_2: 0.2724, decode.aux_loss_ce_5: 0.0190, decode.aux_loss_depth_5: 0.5445, decode.loss_ce: 0.0195, decode.ce_acc_level_0: 38.4375, decode.ce_acc_level_1: 81.1875, decode.loss_depth: 1.0883, loss: 1.9619, grad_norm: 13.0766
2022-01-11 11:54:30,571 - depth - INFO - Iter [5300/38400]	lr: 4.599e-05, eta: 5:57:20, time: 0.601, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0177, decode.aux_loss_depth_2: 0.2741, decode.aux_loss_ce_5: 0.0182, decode.aux_loss_depth_5: 0.5469, decode.loss_ce: 0.0192, decode.ce_acc_level_0: 41.5000, decode.ce_acc_level_1: 81.0625, decode.loss_depth: 1.0926, loss: 1.9686, grad_norm: 13.7635
2022-01-11 11:55:30,374 - depth - INFO - Iter [5400/38400]	lr: 4.729e-05, eta: 5:55:44, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0176, decode.aux_loss_depth_2: 0.2687, decode.aux_loss_ce_5: 0.0184, decode.aux_loss_depth_5: 0.5378, decode.loss_ce: 0.0190, decode.ce_acc_level_0: 42.6875, decode.ce_acc_level_1: 81.2500, decode.loss_depth: 1.0751, loss: 1.9367, grad_norm: 13.6204
2022-01-11 11:56:30,475 - depth - INFO - Iter [5500/38400]	lr: 4.859e-05, eta: 5:54:12, time: 0.602, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0175, decode.aux_loss_depth_2: 0.2679, decode.aux_loss_ce_5: 0.0181, decode.aux_loss_depth_5: 0.5356, decode.loss_ce: 0.0185, decode.ce_acc_level_0: 42.1875, decode.ce_acc_level_1: 83.3125, decode.loss_depth: 1.0684, loss: 1.9261, grad_norm: 13.2063
2022-01-11 11:57:30,898 - depth - INFO - Iter [5600/38400]	lr: 4.990e-05, eta: 5:52:43, time: 0.604, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0180, decode.aux_loss_depth_2: 0.2676, decode.aux_loss_ce_5: 0.0185, decode.aux_loss_depth_5: 0.5318, decode.loss_ce: 0.0187, decode.ce_acc_level_0: 43.1250, decode.ce_acc_level_1: 81.0625, decode.loss_depth: 1.0596, loss: 1.9142, grad_norm: 12.8648
2022-01-11 11:57:52,506 - depth - INFO - Summary:
2022-01-11 11:57:52,507 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8744 | 0.9852 | 0.9974 |  0.1276 | 0.4108 | 0.0516 |  0.1469  | 10.9635 | 0.0709 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-11 11:57:52,508 - depth - INFO - Iter(val) [82]	a1: 0.8744, a2: 0.9852, a3: 0.9974, abs_rel: 0.12763385474681854, rmse: 0.41083982586860657, log_10: 0.05160215497016907, rmse_log: 0.14691995084285736, silog: 10.9635, sq_rel: 0.07090242207050323
2022-01-11 11:58:52,677 - depth - INFO - Iter [5700/38400]	lr: 5.121e-05, eta: 5:53:18, time: 0.818, data_time: 0.223, memory: 30431, decode.aux_loss_ce_2: 0.0173, decode.aux_loss_depth_2: 0.2631, decode.aux_loss_ce_5: 0.0180, decode.aux_loss_depth_5: 0.5245, decode.loss_ce: 0.0182, decode.ce_acc_level_0: 43.1250, decode.ce_acc_level_1: 82.1875, decode.loss_depth: 1.0431, loss: 1.8841, grad_norm: 15.0343
2022-01-11 11:59:52,873 - depth - INFO - Iter [5800/38400]	lr: 5.252e-05, eta: 5:51:47, time: 0.602, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0170, decode.aux_loss_depth_2: 0.2660, decode.aux_loss_ce_5: 0.0178, decode.aux_loss_depth_5: 0.5299, decode.loss_ce: 0.0181, decode.ce_acc_level_0: 42.8125, decode.ce_acc_level_1: 82.1875, decode.loss_depth: 1.0563, loss: 1.9051, grad_norm: 12.1289
2022-01-11 12:00:53,370 - depth - INFO - Iter [5900/38400]	lr: 5.383e-05, eta: 5:50:19, time: 0.605, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0170, decode.aux_loss_depth_2: 0.2640, decode.aux_loss_ce_5: 0.0175, decode.aux_loss_depth_5: 0.5270, decode.loss_ce: 0.0178, decode.ce_acc_level_0: 45.2500, decode.ce_acc_level_1: 84.3750, decode.loss_depth: 1.0506, loss: 1.8939, grad_norm: 12.5913
2022-01-11 12:01:53,231 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 12:01:53,231 - depth - INFO - Iter [6000/38400]	lr: 5.513e-05, eta: 5:48:48, time: 0.598, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0166, decode.aux_loss_depth_2: 0.2588, decode.aux_loss_ce_5: 0.0171, decode.aux_loss_depth_5: 0.5162, decode.loss_ce: 0.0182, decode.ce_acc_level_0: 44.0625, decode.ce_acc_level_1: 84.0625, decode.loss_depth: 1.0347, loss: 1.8617, grad_norm: 12.1679
2022-01-11 12:02:53,700 - depth - INFO - Iter [6100/38400]	lr: 5.644e-05, eta: 5:47:22, time: 0.605, data_time: 0.011, memory: 30431, decode.aux_loss_ce_2: 0.0158, decode.aux_loss_depth_2: 0.2549, decode.aux_loss_ce_5: 0.0164, decode.aux_loss_depth_5: 0.5087, decode.loss_ce: 0.0172, decode.ce_acc_level_0: 46.1250, decode.ce_acc_level_1: 85.0625, decode.loss_depth: 1.0181, loss: 1.8310, grad_norm: 11.2039
2022-01-11 12:03:53,828 - depth - INFO - Iter [6200/38400]	lr: 5.774e-05, eta: 5:45:54, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0169, decode.aux_loss_depth_2: 0.2499, decode.aux_loss_ce_5: 0.0177, decode.aux_loss_depth_5: 0.4971, decode.loss_ce: 0.0184, decode.ce_acc_level_0: 41.8750, decode.ce_acc_level_1: 83.6250, decode.loss_depth: 0.9906, loss: 1.7906, grad_norm: 13.3161
2022-01-11 12:04:54,110 - depth - INFO - Iter [6300/38400]	lr: 5.904e-05, eta: 5:44:28, time: 0.603, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0154, decode.aux_loss_depth_2: 0.2518, decode.aux_loss_ce_5: 0.0161, decode.aux_loss_depth_5: 0.4996, decode.loss_ce: 0.0169, decode.ce_acc_level_0: 48.0000, decode.ce_acc_level_1: 85.9375, decode.loss_depth: 1.0027, loss: 1.8026, grad_norm: 12.4633
2022-01-11 12:05:54,256 - depth - INFO - Saving checkpoint at 6400 iterations
2022-01-11 12:06:10,619 - depth - INFO - Iter [6400/38400]	lr: 6.033e-05, eta: 5:44:25, time: 0.766, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0168, decode.aux_loss_depth_2: 0.2497, decode.aux_loss_ce_5: 0.0173, decode.aux_loss_depth_5: 0.4966, decode.loss_ce: 0.0177, decode.ce_acc_level_0: 45.6250, decode.ce_acc_level_1: 83.8125, decode.loss_depth: 0.9911, loss: 1.7892, grad_norm: 12.2624
2022-01-11 12:06:32,177 - depth - INFO - Summary:
2022-01-11 12:06:32,178 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8732 | 0.9854 | 0.9977 |  0.1229 | 0.3865 | 0.0499 |  0.1432  | 10.7616 | 0.0634 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-11 12:06:32,179 - depth - INFO - Iter(val) [82]	a1: 0.8732, a2: 0.9854, a3: 0.9977, abs_rel: 0.12287261337041855, rmse: 0.38646364212036133, log_10: 0.04993749037384987, rmse_log: 0.14316779375076294, silog: 10.7616, sq_rel: 0.063355453312397
2022-01-11 12:07:32,125 - depth - INFO - Iter [6500/38400]	lr: 6.162e-05, eta: 5:44:43, time: 0.815, data_time: 0.223, memory: 30431, decode.aux_loss_ce_2: 0.0161, decode.aux_loss_depth_2: 0.2437, decode.aux_loss_ce_5: 0.0172, decode.aux_loss_depth_5: 0.4880, decode.loss_ce: 0.0171, decode.ce_acc_level_0: 46.1250, decode.ce_acc_level_1: 84.7500, decode.loss_depth: 0.9670, loss: 1.7492, grad_norm: 12.4541
2022-01-11 12:08:32,468 - depth - INFO - Iter [6600/38400]	lr: 6.289e-05, eta: 5:43:17, time: 0.603, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0161, decode.aux_loss_depth_2: 0.2501, decode.aux_loss_ce_5: 0.0170, decode.aux_loss_depth_5: 0.4983, decode.loss_ce: 0.0170, decode.ce_acc_level_0: 49.8125, decode.ce_acc_level_1: 84.8750, decode.loss_depth: 0.9907, loss: 1.7891, grad_norm: 11.0776
2022-01-11 12:09:32,565 - depth - INFO - Iter [6700/38400]	lr: 6.416e-05, eta: 5:41:50, time: 0.601, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0152, decode.aux_loss_depth_2: 0.2478, decode.aux_loss_ce_5: 0.0161, decode.aux_loss_depth_5: 0.4948, decode.loss_ce: 0.0164, decode.ce_acc_level_0: 50.7500, decode.ce_acc_level_1: 85.6875, decode.loss_depth: 0.9864, loss: 1.7767, grad_norm: 11.5730
2022-01-11 12:10:33,004 - depth - INFO - Iter [6800/38400]	lr: 6.543e-05, eta: 5:40:25, time: 0.604, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0151, decode.aux_loss_depth_2: 0.2382, decode.aux_loss_ce_5: 0.0160, decode.aux_loss_depth_5: 0.4768, decode.loss_ce: 0.0164, decode.ce_acc_level_0: 49.4375, decode.ce_acc_level_1: 85.5000, decode.loss_depth: 0.9553, loss: 1.7177, grad_norm: 11.7678
2022-01-11 12:11:33,966 - depth - INFO - Iter [6900/38400]	lr: 6.668e-05, eta: 5:39:04, time: 0.609, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0162, decode.aux_loss_depth_2: 0.2482, decode.aux_loss_ce_5: 0.0169, decode.aux_loss_depth_5: 0.4967, decode.loss_ce: 0.0174, decode.ce_acc_level_0: 47.6875, decode.ce_acc_level_1: 83.9375, decode.loss_depth: 0.9928, loss: 1.7882, grad_norm: 11.7238
2022-01-11 12:12:34,152 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 12:12:34,152 - depth - INFO - Iter [7000/38400]	lr: 6.792e-05, eta: 5:37:40, time: 0.602, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0147, decode.aux_loss_depth_2: 0.2494, decode.aux_loss_ce_5: 0.0152, decode.aux_loss_depth_5: 0.4956, decode.loss_ce: 0.0160, decode.ce_acc_level_0: 49.7500, decode.ce_acc_level_1: 86.3125, decode.loss_depth: 0.9903, loss: 1.7812, grad_norm: 10.4488
2022-01-11 12:13:34,478 - depth - INFO - Iter [7100/38400]	lr: 6.915e-05, eta: 5:36:16, time: 0.603, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0145, decode.aux_loss_depth_2: 0.2508, decode.aux_loss_ce_5: 0.0149, decode.aux_loss_depth_5: 0.4984, decode.loss_ce: 0.0155, decode.ce_acc_level_0: 54.0000, decode.ce_acc_level_1: 85.8750, decode.loss_depth: 0.9948, loss: 1.7888, grad_norm: 10.2260
2022-01-11 12:14:34,798 - depth - INFO - Iter [7200/38400]	lr: 7.036e-05, eta: 5:34:54, time: 0.604, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0148, decode.aux_loss_depth_2: 0.2408, decode.aux_loss_ce_5: 0.0149, decode.aux_loss_depth_5: 0.4803, decode.loss_ce: 0.0152, decode.ce_acc_level_0: 52.8750, decode.ce_acc_level_1: 87.2500, decode.loss_depth: 0.9575, loss: 1.7236, grad_norm: 9.9146
2022-01-11 12:14:56,422 - depth - INFO - Summary:
2022-01-11 12:14:56,423 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8444 | 0.9779 | 0.9952 |  0.1402 | 0.4306 | 0.0562 |  0.159   | 11.7233 | 0.0792 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-11 12:14:56,424 - depth - INFO - Iter(val) [82]	a1: 0.8444, a2: 0.9779, a3: 0.9952, abs_rel: 0.1402195394039154, rmse: 0.430562287569046, log_10: 0.05624348297715187, rmse_log: 0.1589624136686325, silog: 11.7233, sq_rel: 0.07915838807821274
2022-01-11 12:15:57,483 - depth - INFO - Iter [7300/38400]	lr: 7.157e-05, eta: 5:35:08, time: 0.827, data_time: 0.223, memory: 30431, decode.aux_loss_ce_2: 0.0141, decode.aux_loss_depth_2: 0.2482, decode.aux_loss_ce_5: 0.0146, decode.aux_loss_depth_5: 0.4936, decode.loss_ce: 0.0148, decode.ce_acc_level_0: 54.4375, decode.ce_acc_level_1: 87.3750, decode.loss_depth: 0.9846, loss: 1.7700, grad_norm: 9.4060
2022-01-11 12:16:57,685 - depth - INFO - Iter [7400/38400]	lr: 7.275e-05, eta: 5:33:44, time: 0.602, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0136, decode.aux_loss_depth_2: 0.2410, decode.aux_loss_ce_5: 0.0141, decode.aux_loss_depth_5: 0.4802, decode.loss_ce: 0.0142, decode.ce_acc_level_0: 56.8125, decode.ce_acc_level_1: 88.8125, decode.loss_depth: 0.9585, loss: 1.7215, grad_norm: 11.4588
2022-01-11 12:17:58,016 - depth - INFO - Iter [7500/38400]	lr: 7.393e-05, eta: 5:32:22, time: 0.603, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0144, decode.aux_loss_depth_2: 0.2445, decode.aux_loss_ce_5: 0.0147, decode.aux_loss_depth_5: 0.4852, decode.loss_ce: 0.0147, decode.ce_acc_level_0: 54.5625, decode.ce_acc_level_1: 88.7500, decode.loss_depth: 0.9689, loss: 1.7424, grad_norm: 9.5218
2022-01-11 12:18:58,670 - depth - INFO - Iter [7600/38400]	lr: 7.508e-05, eta: 5:31:02, time: 0.606, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0131, decode.aux_loss_depth_2: 0.2396, decode.aux_loss_ce_5: 0.0131, decode.aux_loss_depth_5: 0.4743, decode.loss_ce: 0.0134, decode.ce_acc_level_0: 58.8750, decode.ce_acc_level_1: 89.8750, decode.loss_depth: 0.9487, loss: 1.7022, grad_norm: 9.6824
2022-01-11 12:19:58,728 - depth - INFO - Iter [7700/38400]	lr: 7.622e-05, eta: 5:29:40, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0131, decode.aux_loss_depth_2: 0.2288, decode.aux_loss_ce_5: 0.0134, decode.aux_loss_depth_5: 0.4548, decode.loss_ce: 0.0136, decode.ce_acc_level_0: 59.0000, decode.ce_acc_level_1: 89.4375, decode.loss_depth: 0.9077, loss: 1.6315, grad_norm: 9.5369
2022-01-11 12:20:58,865 - depth - INFO - Iter [7800/38400]	lr: 7.734e-05, eta: 5:28:18, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0135, decode.aux_loss_depth_2: 0.2307, decode.aux_loss_ce_5: 0.0138, decode.aux_loss_depth_5: 0.4591, decode.loss_ce: 0.0144, decode.ce_acc_level_0: 56.6875, decode.ce_acc_level_1: 87.4375, decode.loss_depth: 0.9196, loss: 1.6511, grad_norm: 10.9253
2022-01-11 12:21:59,113 - depth - INFO - Iter [7900/38400]	lr: 7.845e-05, eta: 5:26:58, time: 0.602, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0119, decode.aux_loss_depth_2: 0.2309, decode.aux_loss_ce_5: 0.0123, decode.aux_loss_depth_5: 0.4598, decode.loss_ce: 0.0128, decode.ce_acc_level_0: 61.5625, decode.ce_acc_level_1: 90.6875, decode.loss_depth: 0.9190, loss: 1.6467, grad_norm: 11.4196
2022-01-11 12:22:59,309 - depth - INFO - Saving checkpoint at 8000 iterations
2022-01-11 12:23:16,227 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 12:23:16,227 - depth - INFO - Iter [8000/38400]	lr: 7.953e-05, eta: 5:26:42, time: 0.772, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0134, decode.aux_loss_depth_2: 0.2309, decode.aux_loss_ce_5: 0.0138, decode.aux_loss_depth_5: 0.4607, decode.loss_ce: 0.0140, decode.ce_acc_level_0: 57.1875, decode.ce_acc_level_1: 89.0625, decode.loss_depth: 0.9180, loss: 1.6508, grad_norm: 11.1563
2022-01-11 12:23:37,487 - depth - INFO - Summary:
2022-01-11 12:23:37,487 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
| 0.9125 | 0.9893 | 0.998 |   0.1   | 0.3548 | 0.0429 |  0.1264  | 10.2023 | 0.0504 |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
2022-01-11 12:23:54,364 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_8000.pth.
2022-01-11 12:23:54,365 - depth - INFO - Best abs_rel is 0.1000 at 8000 iter.
2022-01-11 12:23:54,367 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 12:23:54,367 - depth - INFO - Iter(val) [82]	a1: 0.9125, a2: 0.9893, a3: 0.9980, abs_rel: 0.09998372942209244, rmse: 0.3547912538051605, log_10: 0.0428617037832737, rmse_log: 0.12638220191001892, silog: 10.2023, sq_rel: 0.050354961305856705
2022-01-11 12:24:54,951 - depth - INFO - Iter [8100/38400]	lr: 8.059e-05, eta: 5:27:46, time: 0.987, data_time: 0.390, memory: 30431, decode.aux_loss_ce_2: 0.0134, decode.aux_loss_depth_2: 0.2309, decode.aux_loss_ce_5: 0.0137, decode.aux_loss_depth_5: 0.4628, decode.loss_ce: 0.0137, decode.ce_acc_level_0: 59.1875, decode.ce_acc_level_1: 89.1250, decode.loss_depth: 0.9200, loss: 1.6545, grad_norm: 11.3329
2022-01-11 12:25:55,086 - depth - INFO - Iter [8200/38400]	lr: 8.163e-05, eta: 5:26:23, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0128, decode.aux_loss_depth_2: 0.2300, decode.aux_loss_ce_5: 0.0135, decode.aux_loss_depth_5: 0.4597, decode.loss_ce: 0.0133, decode.ce_acc_level_0: 60.4375, decode.ce_acc_level_1: 90.3750, decode.loss_depth: 0.9158, loss: 1.6451, grad_norm: 10.7368
2022-01-11 12:26:54,713 - depth - INFO - Iter [8300/38400]	lr: 8.265e-05, eta: 5:25:00, time: 0.596, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0128, decode.aux_loss_depth_2: 0.2246, decode.aux_loss_ce_5: 0.0134, decode.aux_loss_depth_5: 0.4491, decode.loss_ce: 0.0134, decode.ce_acc_level_0: 59.0625, decode.ce_acc_level_1: 89.1875, decode.loss_depth: 0.8968, loss: 1.6101, grad_norm: 9.4476
2022-01-11 12:27:54,897 - depth - INFO - Iter [8400/38400]	lr: 8.365e-05, eta: 5:23:38, time: 0.602, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0129, decode.aux_loss_depth_2: 0.2250, decode.aux_loss_ce_5: 0.0131, decode.aux_loss_depth_5: 0.4498, decode.loss_ce: 0.0130, decode.ce_acc_level_0: 62.8125, decode.ce_acc_level_1: 89.8750, decode.loss_depth: 0.8973, loss: 1.6111, grad_norm: 10.4356
2022-01-11 12:28:55,310 - depth - INFO - Iter [8500/38400]	lr: 8.462e-05, eta: 5:22:19, time: 0.604, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0123, decode.aux_loss_depth_2: 0.2281, decode.aux_loss_ce_5: 0.0125, decode.aux_loss_depth_5: 0.4543, decode.loss_ce: 0.0123, decode.ce_acc_level_0: 62.3750, decode.ce_acc_level_1: 90.6250, decode.loss_depth: 0.9123, loss: 1.6318, grad_norm: 9.0581
2022-01-11 12:29:55,279 - depth - INFO - Iter [8600/38400]	lr: 8.557e-05, eta: 5:20:57, time: 0.599, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0119, decode.aux_loss_depth_2: 0.2266, decode.aux_loss_ce_5: 0.0123, decode.aux_loss_depth_5: 0.4508, decode.loss_ce: 0.0123, decode.ce_acc_level_0: 62.1250, decode.ce_acc_level_1: 91.6875, decode.loss_depth: 0.9013, loss: 1.6152, grad_norm: 8.8943
2022-01-11 12:30:56,004 - depth - INFO - Iter [8700/38400]	lr: 8.649e-05, eta: 5:19:39, time: 0.607, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0118, decode.aux_loss_depth_2: 0.2311, decode.aux_loss_ce_5: 0.0116, decode.aux_loss_depth_5: 0.4584, decode.loss_ce: 0.0116, decode.ce_acc_level_0: 64.9375, decode.ce_acc_level_1: 92.6250, decode.loss_depth: 0.9162, loss: 1.6407, grad_norm: 9.6440
2022-01-11 12:31:55,955 - depth - INFO - Iter [8800/38400]	lr: 8.739e-05, eta: 5:18:19, time: 0.600, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0118, decode.aux_loss_depth_2: 0.2212, decode.aux_loss_ce_5: 0.0117, decode.aux_loss_depth_5: 0.4409, decode.loss_ce: 0.0118, decode.ce_acc_level_0: 64.3750, decode.ce_acc_level_1: 91.6250, decode.loss_depth: 0.8806, loss: 1.5779, grad_norm: 8.4093
2022-01-11 12:32:17,311 - depth - INFO - Summary:
2022-01-11 12:32:17,312 - depth - INFO - 
+--------+-------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2  |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+-------+--------+---------+--------+--------+----------+---------+--------+
| 0.9033 | 0.988 | 0.9981 |  0.108  | 0.3705 | 0.0457 |  0.1323  | 10.5947 | 0.0547 |
+--------+-------+--------+---------+--------+--------+----------+---------+--------+
2022-01-11 12:32:17,314 - depth - INFO - Iter(val) [82]	a1: 0.9033, a2: 0.9880, a3: 0.9981, abs_rel: 0.10795852541923523, rmse: 0.3704792559146881, log_10: 0.045652881264686584, rmse_log: 0.1323290914297104, silog: 10.5947, sq_rel: 0.05471154302358627
2022-01-11 12:33:17,773 - depth - INFO - Iter [8900/38400]	lr: 8.826e-05, eta: 5:18:12, time: 0.818, data_time: 0.221, memory: 30431, decode.aux_loss_ce_2: 0.0117, decode.aux_loss_depth_2: 0.2263, decode.aux_loss_ce_5: 0.0120, decode.aux_loss_depth_5: 0.4484, decode.loss_ce: 0.0121, decode.ce_acc_level_0: 62.7500, decode.ce_acc_level_1: 91.5000, decode.loss_depth: 0.8960, loss: 1.6064, grad_norm: 11.1620
2022-01-11 12:34:17,882 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 12:34:17,885 - depth - INFO - Iter [9000/38400]	lr: 8.910e-05, eta: 5:16:52, time: 0.601, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0124, decode.aux_loss_depth_2: 0.2196, decode.aux_loss_ce_5: 0.0122, decode.aux_loss_depth_5: 0.4358, decode.loss_ce: 0.0124, decode.ce_acc_level_0: 62.1875, decode.ce_acc_level_1: 91.0625, decode.loss_depth: 0.8711, loss: 1.5635, grad_norm: 15.5689
2022-01-11 12:35:18,465 - depth - INFO - Iter [9100/38400]	lr: 8.992e-05, eta: 5:15:34, time: 0.606, data_time: 0.010, memory: 30431, decode.aux_loss_ce_2: 0.0120, decode.aux_loss_depth_2: 0.2196, decode.aux_loss_ce_5: 0.0120, decode.aux_loss_depth_5: 0.4361, decode.loss_ce: 0.0128, decode.ce_acc_level_0: 62.1250, decode.ce_acc_level_1: 90.1875, decode.loss_depth: 0.8759, loss: 1.5684, grad_norm: 8.4424
2022-01-11 12:36:18,749 - depth - INFO - Iter [9200/38400]	lr: 9.071e-05, eta: 5:14:16, time: 0.603, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0114, decode.aux_loss_depth_2: 0.2184, decode.aux_loss_ce_5: 0.0114, decode.aux_loss_depth_5: 0.4350, decode.loss_ce: 0.0123, decode.ce_acc_level_0: 62.6875, decode.ce_acc_level_1: 91.5000, decode.loss_depth: 0.8728, loss: 1.5613, grad_norm: 8.5395
2022-01-11 12:37:18,510 - depth - INFO - Iter [9300/38400]	lr: 9.147e-05, eta: 5:12:57, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0117, decode.aux_loss_depth_2: 0.2175, decode.aux_loss_ce_5: 0.0120, decode.aux_loss_depth_5: 0.4360, decode.loss_ce: 0.0124, decode.ce_acc_level_0: 62.0000, decode.ce_acc_level_1: 92.0000, decode.loss_depth: 0.8737, loss: 1.5633, grad_norm: 8.8637
2022-01-11 12:38:18,529 - depth - INFO - Iter [9400/38400]	lr: 9.220e-05, eta: 5:11:38, time: 0.600, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0119, decode.aux_loss_depth_2: 0.2055, decode.aux_loss_ce_5: 0.0125, decode.aux_loss_depth_5: 0.4105, decode.loss_ce: 0.0124, decode.ce_acc_level_0: 63.6875, decode.ce_acc_level_1: 91.5625, decode.loss_depth: 0.8207, loss: 1.4735, grad_norm: 9.1364
2022-01-11 12:39:18,418 - depth - INFO - Iter [9500/38400]	lr: 9.290e-05, eta: 5:10:20, time: 0.599, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0113, decode.aux_loss_depth_2: 0.2121, decode.aux_loss_ce_5: 0.0122, decode.aux_loss_depth_5: 0.4252, decode.loss_ce: 0.0127, decode.ce_acc_level_0: 61.1250, decode.ce_acc_level_1: 90.8750, decode.loss_depth: 0.8509, loss: 1.5243, grad_norm: 9.1316
2022-01-11 12:40:18,474 - depth - INFO - Saving checkpoint at 9600 iterations
2022-01-11 12:40:34,966 - depth - INFO - Iter [9600/38400]	lr: 9.357e-05, eta: 5:09:52, time: 0.766, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0104, decode.aux_loss_depth_2: 0.2138, decode.aux_loss_ce_5: 0.0113, decode.aux_loss_depth_5: 0.4277, decode.loss_ce: 0.0113, decode.ce_acc_level_0: 65.8125, decode.ce_acc_level_1: 93.2500, decode.loss_depth: 0.8538, loss: 1.5283, grad_norm: 8.8648
2022-01-11 12:40:56,243 - depth - INFO - Summary:
2022-01-11 12:40:56,243 - depth - INFO - 
+-------+-------+--------+---------+--------+--------+----------+---------+--------+
|   a1  |   a2  |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+-------+-------+--------+---------+--------+--------+----------+---------+--------+
| 0.907 | 0.988 | 0.9968 |  0.1021 | 0.3703 | 0.045  |  0.1312  | 10.2728 | 0.0536 |
+-------+-------+--------+---------+--------+--------+----------+---------+--------+
2022-01-11 12:40:56,244 - depth - INFO - Iter(val) [82]	a1: 0.9070, a2: 0.9880, a3: 0.9968, abs_rel: 0.10213117301464081, rmse: 0.37029528617858887, log_10: 0.04498584568500519, rmse_log: 0.13116592168807983, silog: 10.2728, sq_rel: 0.05363825336098671
2022-01-11 12:41:56,280 - depth - INFO - Iter [9700/38400]	lr: 9.421e-05, eta: 5:09:36, time: 0.812, data_time: 0.220, memory: 30431, decode.aux_loss_ce_2: 0.0096, decode.aux_loss_depth_2: 0.2130, decode.aux_loss_ce_5: 0.0102, decode.aux_loss_depth_5: 0.4273, decode.loss_ce: 0.0104, decode.ce_acc_level_0: 68.6250, decode.ce_acc_level_1: 93.2500, decode.loss_depth: 0.8519, loss: 1.5224, grad_norm: 8.3700
2022-01-11 12:42:56,261 - depth - INFO - Iter [9800/38400]	lr: 9.481e-05, eta: 5:08:18, time: 0.601, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0091, decode.aux_loss_depth_2: 0.2185, decode.aux_loss_ce_5: 0.0094, decode.aux_loss_depth_5: 0.4371, decode.loss_ce: 0.0096, decode.ce_acc_level_0: 70.5000, decode.ce_acc_level_1: 94.6875, decode.loss_depth: 0.8722, loss: 1.5559, grad_norm: 8.2135
2022-01-11 12:43:57,286 - depth - INFO - Iter [9900/38400]	lr: 9.539e-05, eta: 5:07:03, time: 0.610, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0102, decode.aux_loss_depth_2: 0.2094, decode.aux_loss_ce_5: 0.0106, decode.aux_loss_depth_5: 0.4184, decode.loss_ce: 0.0108, decode.ce_acc_level_0: 66.1875, decode.ce_acc_level_1: 93.8125, decode.loss_depth: 0.8357, loss: 1.4951, grad_norm: 7.6418
2022-01-11 12:44:57,424 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 12:44:57,424 - depth - INFO - Iter [10000/38400]	lr: 9.593e-05, eta: 5:05:45, time: 0.602, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0093, decode.aux_loss_depth_2: 0.2098, decode.aux_loss_ce_5: 0.0104, decode.aux_loss_depth_5: 0.4213, decode.loss_ce: 0.0099, decode.ce_acc_level_0: 71.5000, decode.ce_acc_level_1: 93.5625, decode.loss_depth: 0.8356, loss: 1.4962, grad_norm: 8.2951
2022-01-11 12:45:57,254 - depth - INFO - Iter [10100/38400]	lr: 9.645e-05, eta: 5:04:27, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0115, decode.aux_loss_depth_2: 0.2073, decode.aux_loss_ce_5: 0.0126, decode.aux_loss_depth_5: 0.4160, decode.loss_ce: 0.0121, decode.ce_acc_level_0: 64.0000, decode.ce_acc_level_1: 91.3750, decode.loss_depth: 0.8251, loss: 1.4846, grad_norm: 7.5252
2022-01-11 12:46:56,931 - depth - INFO - Iter [10200/38400]	lr: 9.692e-05, eta: 5:03:09, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0107, decode.aux_loss_depth_2: 0.2091, decode.aux_loss_ce_5: 0.0110, decode.aux_loss_depth_5: 0.4200, decode.loss_ce: 0.0113, decode.ce_acc_level_0: 65.0625, decode.ce_acc_level_1: 93.5000, decode.loss_depth: 0.8357, loss: 1.4978, grad_norm: 6.9203
2022-01-11 12:47:56,859 - depth - INFO - Iter [10300/38400]	lr: 9.737e-05, eta: 5:01:52, time: 0.599, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0089, decode.aux_loss_depth_2: 0.2057, decode.aux_loss_ce_5: 0.0095, decode.aux_loss_depth_5: 0.4128, decode.loss_ce: 0.0096, decode.ce_acc_level_0: 70.8750, decode.ce_acc_level_1: 94.6875, decode.loss_depth: 0.8215, loss: 1.4680, grad_norm: 7.1794
2022-01-11 12:48:56,684 - depth - INFO - Iter [10400/38400]	lr: 9.778e-05, eta: 5:00:35, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0090, decode.aux_loss_depth_2: 0.2087, decode.aux_loss_ce_5: 0.0095, decode.aux_loss_depth_5: 0.4179, decode.loss_ce: 0.0096, decode.ce_acc_level_0: 72.3750, decode.ce_acc_level_1: 94.6250, decode.loss_depth: 0.8332, loss: 1.4879, grad_norm: 7.5351
2022-01-11 12:49:18,439 - depth - INFO - Summary:
2022-01-11 12:49:18,440 - depth - INFO - 
+-------+--------+--------+---------+------+--------+----------+---------+--------+
|   a1  |   a2   |   a3   | abs_rel | rmse | log_10 | rmse_log |  silog  | sq_rel |
+-------+--------+--------+---------+------+--------+----------+---------+--------+
| 0.892 | 0.9859 | 0.9971 |  0.1132 | 0.38 | 0.047  |  0.1353  | 10.4843 | 0.059  |
+-------+--------+--------+---------+------+--------+----------+---------+--------+
2022-01-11 12:49:18,441 - depth - INFO - Iter(val) [82]	a1: 0.8920, a2: 0.9859, a3: 0.9971, abs_rel: 0.11318541318178177, rmse: 0.3800256848335266, log_10: 0.04698212817311287, rmse_log: 0.1353384554386139, silog: 10.4843, sq_rel: 0.058951858431100845
2022-01-11 12:50:17,833 - depth - INFO - Iter [10500/38400]	lr: 9.815e-05, eta: 5:00:16, time: 0.812, data_time: 0.225, memory: 30431, decode.aux_loss_ce_2: 0.0098, decode.aux_loss_depth_2: 0.2072, decode.aux_loss_ce_5: 0.0100, decode.aux_loss_depth_5: 0.4144, decode.loss_ce: 0.0102, decode.ce_acc_level_0: 68.3750, decode.ce_acc_level_1: 94.6250, decode.loss_depth: 0.8252, loss: 1.4769, grad_norm: 6.6807
2022-01-11 12:51:18,075 - depth - INFO - Iter [10600/38400]	lr: 9.850e-05, eta: 4:58:59, time: 0.602, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0092, decode.aux_loss_depth_2: 0.2048, decode.aux_loss_ce_5: 0.0098, decode.aux_loss_depth_5: 0.4102, decode.loss_ce: 0.0094, decode.ce_acc_level_0: 71.8125, decode.ce_acc_level_1: 95.1875, decode.loss_depth: 0.8167, loss: 1.4602, grad_norm: 6.2848
2022-01-11 12:52:18,037 - depth - INFO - Iter [10700/38400]	lr: 9.880e-05, eta: 4:57:43, time: 0.600, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0097, decode.aux_loss_depth_2: 0.2043, decode.aux_loss_ce_5: 0.0100, decode.aux_loss_depth_5: 0.4090, decode.loss_ce: 0.0098, decode.ce_acc_level_0: 70.4375, decode.ce_acc_level_1: 94.9375, decode.loss_depth: 0.8150, loss: 1.4579, grad_norm: 6.6521
2022-01-11 12:53:17,912 - depth - INFO - Iter [10800/38400]	lr: 9.908e-05, eta: 4:56:27, time: 0.599, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0100, decode.aux_loss_depth_2: 0.2039, decode.aux_loss_ce_5: 0.0101, decode.aux_loss_depth_5: 0.4073, decode.loss_ce: 0.0103, decode.ce_acc_level_0: 68.6250, decode.ce_acc_level_1: 93.3750, decode.loss_depth: 0.8141, loss: 1.4557, grad_norm: 7.0440
2022-01-11 12:54:17,900 - depth - INFO - Iter [10900/38400]	lr: 9.932e-05, eta: 4:55:11, time: 0.600, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0084, decode.aux_loss_depth_2: 0.1958, decode.aux_loss_ce_5: 0.0089, decode.aux_loss_depth_5: 0.3911, decode.loss_ce: 0.0089, decode.ce_acc_level_0: 73.5625, decode.ce_acc_level_1: 94.8750, decode.loss_depth: 0.7789, loss: 1.3919, grad_norm: 6.5144
2022-01-11 12:55:17,761 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 12:55:17,762 - depth - INFO - Iter [11000/38400]	lr: 9.952e-05, eta: 4:53:55, time: 0.599, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0090, decode.aux_loss_depth_2: 0.1989, decode.aux_loss_ce_5: 0.0096, decode.aux_loss_depth_5: 0.3957, decode.loss_ce: 0.0093, decode.ce_acc_level_0: 72.2500, decode.ce_acc_level_1: 95.0000, decode.loss_depth: 0.7930, loss: 1.4155, grad_norm: 6.2574
2022-01-11 12:56:16,785 - depth - INFO - Iter [11100/38400]	lr: 9.969e-05, eta: 4:52:38, time: 0.591, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0079, decode.aux_loss_depth_2: 0.1981, decode.aux_loss_ce_5: 0.0086, decode.aux_loss_depth_5: 0.3983, decode.loss_ce: 0.0084, decode.ce_acc_level_0: 73.6250, decode.ce_acc_level_1: 95.8750, decode.loss_depth: 0.7932, loss: 1.4145, grad_norm: 6.2135
2022-01-11 12:57:16,371 - depth - INFO - Saving checkpoint at 11200 iterations
2022-01-11 12:57:32,011 - depth - INFO - Iter [11200/38400]	lr: 9.982e-05, eta: 4:52:00, time: 0.752, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0086, decode.aux_loss_depth_2: 0.1944, decode.aux_loss_ce_5: 0.0087, decode.aux_loss_depth_5: 0.3887, decode.loss_ce: 0.0090, decode.ce_acc_level_0: 72.0625, decode.ce_acc_level_1: 95.5625, decode.loss_depth: 0.7747, loss: 1.3841, grad_norm: 6.4172
2022-01-11 12:57:53,822 - depth - INFO - Summary:
2022-01-11 12:57:53,822 - depth - INFO - 
+-------+-------+--------+---------+--------+--------+----------+---------+--------+
|   a1  |   a2  |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+-------+-------+--------+---------+--------+--------+----------+---------+--------+
| 0.903 | 0.987 | 0.9971 |  0.1092 | 0.3667 | 0.0456 |  0.1323  | 10.4655 | 0.0547 |
+-------+-------+--------+---------+--------+--------+----------+---------+--------+
2022-01-11 12:57:53,823 - depth - INFO - Iter(val) [82]	a1: 0.9030, a2: 0.9870, a3: 0.9971, abs_rel: 0.10917995870113373, rmse: 0.3667367398738861, log_10: 0.04562970623373985, rmse_log: 0.13232392072677612, silog: 10.4655, sq_rel: 0.05470336973667145
2022-01-11 12:58:53,433 - depth - INFO - Iter [11300/38400]	lr: 9.991e-05, eta: 4:51:36, time: 0.814, data_time: 0.225, memory: 30431, decode.aux_loss_ce_2: 0.0076, decode.aux_loss_depth_2: 0.1932, decode.aux_loss_ce_5: 0.0083, decode.aux_loss_depth_5: 0.3864, decode.loss_ce: 0.0083, decode.ce_acc_level_0: 75.2500, decode.ce_acc_level_1: 95.8750, decode.loss_depth: 0.7698, loss: 1.3737, grad_norm: 6.4949
2022-01-11 12:59:52,957 - depth - INFO - Iter [11400/38400]	lr: 9.997e-05, eta: 4:50:20, time: 0.595, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0080, decode.aux_loss_depth_2: 0.1942, decode.aux_loss_ce_5: 0.0083, decode.aux_loss_depth_5: 0.3868, decode.loss_ce: 0.0086, decode.ce_acc_level_0: 73.5625, decode.ce_acc_level_1: 95.5000, decode.loss_depth: 0.7727, loss: 1.3786, grad_norm: 6.2245
2022-01-11 13:00:52,789 - depth - INFO - Iter [11500/38400]	lr: 1.000e-04, eta: 4:49:04, time: 0.598, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0081, decode.aux_loss_depth_2: 0.1949, decode.aux_loss_ce_5: 0.0087, decode.aux_loss_depth_5: 0.3882, decode.loss_ce: 0.0086, decode.ce_acc_level_0: 74.6875, decode.ce_acc_level_1: 94.7500, decode.loss_depth: 0.7748, loss: 1.3832, grad_norm: 6.9242
2022-01-11 13:01:52,817 - depth - INFO - Iter [11600/38400]	lr: 1.000e-04, eta: 4:47:50, time: 0.601, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0085, decode.aux_loss_depth_2: 0.1898, decode.aux_loss_ce_5: 0.0091, decode.aux_loss_depth_5: 0.3788, decode.loss_ce: 0.0088, decode.ce_acc_level_0: 71.5625, decode.ce_acc_level_1: 94.5625, decode.loss_depth: 0.7552, loss: 1.3502, grad_norm: 5.7976
2022-01-11 13:02:52,645 - depth - INFO - Iter [11700/38400]	lr: 9.999e-05, eta: 4:46:35, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0065, decode.aux_loss_depth_2: 0.1881, decode.aux_loss_ce_5: 0.0070, decode.aux_loss_depth_5: 0.3755, decode.loss_ce: 0.0068, decode.ce_acc_level_0: 79.5000, decode.ce_acc_level_1: 96.7500, decode.loss_depth: 0.7495, loss: 1.3334, grad_norm: 6.3874
2022-01-11 13:03:52,136 - depth - INFO - Iter [11800/38400]	lr: 9.997e-05, eta: 4:45:19, time: 0.594, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0082, decode.aux_loss_depth_2: 0.1884, decode.aux_loss_ce_5: 0.0083, decode.aux_loss_depth_5: 0.3747, decode.loss_ce: 0.0089, decode.ce_acc_level_0: 72.5000, decode.ce_acc_level_1: 96.1875, decode.loss_depth: 0.7486, loss: 1.3372, grad_norm: 6.2366
2022-01-11 13:04:51,770 - depth - INFO - Iter [11900/38400]	lr: 9.995e-05, eta: 4:44:04, time: 0.597, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0066, decode.aux_loss_depth_2: 0.1913, decode.aux_loss_ce_5: 0.0071, decode.aux_loss_depth_5: 0.3817, decode.loss_ce: 0.0069, decode.ce_acc_level_0: 77.8750, decode.ce_acc_level_1: 96.8125, decode.loss_depth: 0.7628, loss: 1.3564, grad_norm: 6.1836
2022-01-11 13:05:51,505 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 13:05:51,505 - depth - INFO - Iter [12000/38400]	lr: 9.992e-05, eta: 4:42:50, time: 0.597, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0063, decode.aux_loss_depth_2: 0.1871, decode.aux_loss_ce_5: 0.0069, decode.aux_loss_depth_5: 0.3735, decode.loss_ce: 0.0069, decode.ce_acc_level_0: 79.2500, decode.ce_acc_level_1: 96.6875, decode.loss_depth: 0.7470, loss: 1.3276, grad_norm: 6.9416
2022-01-11 13:06:12,908 - depth - INFO - Summary:
2022-01-11 13:06:12,909 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.9138 | 0.9886 | 0.9975 |  0.1006 | 0.3559 | 0.0428 |  0.1259  | 10.1299 | 0.0517 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-11 13:06:12,910 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 13:06:12,910 - depth - INFO - Iter(val) [82]	a1: 0.9138, a2: 0.9886, a3: 0.9975, abs_rel: 0.10058166831731796, rmse: 0.3559103310108185, log_10: 0.042787592858076096, rmse_log: 0.12589916586875916, silog: 10.1299, sq_rel: 0.05172932520508766
2022-01-11 13:07:12,806 - depth - INFO - Iter [12100/38400]	lr: 9.989e-05, eta: 4:42:23, time: 0.813, data_time: 0.221, memory: 30431, decode.aux_loss_ce_2: 0.0066, decode.aux_loss_depth_2: 0.1884, decode.aux_loss_ce_5: 0.0069, decode.aux_loss_depth_5: 0.3762, decode.loss_ce: 0.0068, decode.ce_acc_level_0: 79.1875, decode.ce_acc_level_1: 97.6875, decode.loss_depth: 0.7520, loss: 1.3368, grad_norm: 6.4999
2022-01-11 13:08:12,579 - depth - INFO - Iter [12200/38400]	lr: 9.984e-05, eta: 4:41:08, time: 0.597, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0070, decode.aux_loss_depth_2: 0.1823, decode.aux_loss_ce_5: 0.0070, decode.aux_loss_depth_5: 0.3639, decode.loss_ce: 0.0073, decode.ce_acc_level_0: 78.1250, decode.ce_acc_level_1: 96.8125, decode.loss_depth: 0.7280, loss: 1.2956, grad_norm: 6.6019
2022-01-11 13:09:12,570 - depth - INFO - Iter [12300/38400]	lr: 9.979e-05, eta: 4:39:55, time: 0.600, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0070, decode.aux_loss_depth_2: 0.1807, decode.aux_loss_ce_5: 0.0073, decode.aux_loss_depth_5: 0.3608, decode.loss_ce: 0.0076, decode.ce_acc_level_0: 76.3750, decode.ce_acc_level_1: 96.5000, decode.loss_depth: 0.7218, loss: 1.2851, grad_norm: 5.9457
2022-01-11 13:10:12,153 - depth - INFO - Iter [12400/38400]	lr: 9.974e-05, eta: 4:38:40, time: 0.595, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0063, decode.aux_loss_depth_2: 0.1793, decode.aux_loss_ce_5: 0.0068, decode.aux_loss_depth_5: 0.3590, decode.loss_ce: 0.0067, decode.ce_acc_level_0: 78.3125, decode.ce_acc_level_1: 97.3125, decode.loss_depth: 0.7173, loss: 1.2755, grad_norm: 5.7246
2022-01-11 13:11:11,465 - depth - INFO - Iter [12500/38400]	lr: 9.967e-05, eta: 4:37:26, time: 0.593, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0059, decode.aux_loss_depth_2: 0.1797, decode.aux_loss_ce_5: 0.0066, decode.aux_loss_depth_5: 0.3605, decode.loss_ce: 0.0064, decode.ce_acc_level_0: 79.9375, decode.ce_acc_level_1: 96.9375, decode.loss_depth: 0.7193, loss: 1.2783, grad_norm: 6.6233
2022-01-11 13:12:11,515 - depth - INFO - Iter [12600/38400]	lr: 9.960e-05, eta: 4:36:13, time: 0.601, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0055, decode.aux_loss_depth_2: 0.1818, decode.aux_loss_ce_5: 0.0068, decode.aux_loss_depth_5: 0.3660, decode.loss_ce: 0.0063, decode.ce_acc_level_0: 80.5625, decode.ce_acc_level_1: 97.3125, decode.loss_depth: 0.7268, loss: 1.2932, grad_norm: 5.6880
2022-01-11 13:13:11,139 - depth - INFO - Iter [12700/38400]	lr: 9.953e-05, eta: 4:34:59, time: 0.596, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0064, decode.aux_loss_depth_2: 0.1840, decode.aux_loss_ce_5: 0.0081, decode.aux_loss_depth_5: 0.3721, decode.loss_ce: 0.0070, decode.ce_acc_level_0: 77.1250, decode.ce_acc_level_1: 97.0625, decode.loss_depth: 0.7361, loss: 1.3137, grad_norm: 6.0649
2022-01-11 13:14:10,870 - depth - INFO - Saving checkpoint at 12800 iterations
2022-01-11 13:14:27,781 - depth - INFO - Iter [12800/38400]	lr: 9.944e-05, eta: 4:34:20, time: 0.767, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0051, decode.aux_loss_depth_2: 0.1782, decode.aux_loss_ce_5: 0.0060, decode.aux_loss_depth_5: 0.3573, decode.loss_ce: 0.0055, decode.ce_acc_level_0: 83.0000, decode.ce_acc_level_1: 98.0625, decode.loss_depth: 0.7106, loss: 1.2627, grad_norm: 5.9119
2022-01-11 13:14:49,385 - depth - INFO - Summary:
2022-01-11 13:14:49,387 - depth - INFO - 
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1  |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.913 | 0.9882 | 0.9974 |  0.1042 | 0.3638 | 0.0437 |  0.128   | 10.1519 | 0.0577 |
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-11 13:14:49,390 - depth - INFO - Iter(val) [82]	a1: 0.9130, a2: 0.9882, a3: 0.9974, abs_rel: 0.10423143208026886, rmse: 0.36377424001693726, log_10: 0.04367531090974808, rmse_log: 0.1279868483543396, silog: 10.1519, sq_rel: 0.05765669792890549
2022-01-11 13:15:49,193 - depth - INFO - Iter [12900/38400]	lr: 9.935e-05, eta: 4:33:49, time: 0.814, data_time: 0.223, memory: 30431, decode.aux_loss_ce_2: 0.0056, decode.aux_loss_depth_2: 0.1787, decode.aux_loss_ce_5: 0.0063, decode.aux_loss_depth_5: 0.3577, decode.loss_ce: 0.0062, decode.ce_acc_level_0: 81.3750, decode.ce_acc_level_1: 97.5000, decode.loss_depth: 0.7155, loss: 1.2701, grad_norm: 6.1703
2022-01-11 13:16:49,230 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 13:16:49,230 - depth - INFO - Iter [13000/38400]	lr: 9.925e-05, eta: 4:32:36, time: 0.600, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0055, decode.aux_loss_depth_2: 0.1747, decode.aux_loss_ce_5: 0.0060, decode.aux_loss_depth_5: 0.3497, decode.loss_ce: 0.0059, decode.ce_acc_level_0: 81.5625, decode.ce_acc_level_1: 97.9375, decode.loss_depth: 0.6986, loss: 1.2404, grad_norm: 5.7585
2022-01-11 13:17:48,855 - depth - INFO - Iter [13100/38400]	lr: 9.915e-05, eta: 4:31:23, time: 0.596, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0058, decode.aux_loss_depth_2: 0.1748, decode.aux_loss_ce_5: 0.0058, decode.aux_loss_depth_5: 0.3486, decode.loss_ce: 0.0060, decode.ce_acc_level_0: 82.5625, decode.ce_acc_level_1: 97.8125, decode.loss_depth: 0.6981, loss: 1.2391, grad_norm: 5.8904
2022-01-11 13:18:48,997 - depth - INFO - Iter [13200/38400]	lr: 9.904e-05, eta: 4:30:10, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0056, decode.aux_loss_depth_2: 0.1777, decode.aux_loss_ce_5: 0.0057, decode.aux_loss_depth_5: 0.3547, decode.loss_ce: 0.0057, decode.ce_acc_level_0: 82.1875, decode.ce_acc_level_1: 97.8750, decode.loss_depth: 0.7077, loss: 1.2571, grad_norm: 5.6595
2022-01-11 13:19:49,205 - depth - INFO - Iter [13300/38400]	lr: 9.892e-05, eta: 4:28:58, time: 0.602, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0049, decode.aux_loss_depth_2: 0.1772, decode.aux_loss_ce_5: 0.0054, decode.aux_loss_depth_5: 0.3535, decode.loss_ce: 0.0054, decode.ce_acc_level_0: 83.6875, decode.ce_acc_level_1: 97.9375, decode.loss_depth: 0.7058, loss: 1.2521, grad_norm: 5.7647
2022-01-11 13:20:49,001 - depth - INFO - Iter [13400/38400]	lr: 9.880e-05, eta: 4:27:45, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0054, decode.aux_loss_depth_2: 0.1763, decode.aux_loss_ce_5: 0.0062, decode.aux_loss_depth_5: 0.3525, decode.loss_ce: 0.0058, decode.ce_acc_level_0: 81.7500, decode.ce_acc_level_1: 98.1250, decode.loss_depth: 0.7045, loss: 1.2507, grad_norm: 6.6836
2022-01-11 13:21:48,793 - depth - INFO - Iter [13500/38400]	lr: 9.867e-05, eta: 4:26:33, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0051, decode.aux_loss_depth_2: 0.1730, decode.aux_loss_ce_5: 0.0061, decode.aux_loss_depth_5: 0.3452, decode.loss_ce: 0.0056, decode.ce_acc_level_0: 82.7500, decode.ce_acc_level_1: 98.0625, decode.loss_depth: 0.6916, loss: 1.2267, grad_norm: 5.2664
2022-01-11 13:22:48,332 - depth - INFO - Iter [13600/38400]	lr: 9.853e-05, eta: 4:25:20, time: 0.595, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0053, decode.aux_loss_depth_2: 0.1785, decode.aux_loss_ce_5: 0.0059, decode.aux_loss_depth_5: 0.3558, decode.loss_ce: 0.0054, decode.ce_acc_level_0: 83.3125, decode.ce_acc_level_1: 98.1250, decode.loss_depth: 0.7116, loss: 1.2626, grad_norm: 5.8813
2022-01-11 13:23:09,960 - depth - INFO - Summary:
2022-01-11 13:23:09,961 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.9046 | 0.9866 | 0.9973 |  0.1078 | 0.3635 | 0.0448 |  0.1309  | 10.4723 | 0.0583 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-11 13:23:09,962 - depth - INFO - Iter(val) [82]	a1: 0.9046, a2: 0.9866, a3: 0.9973, abs_rel: 0.10784786194562912, rmse: 0.36351269483566284, log_10: 0.044824279844760895, rmse_log: 0.13088369369506836, silog: 10.4723, sq_rel: 0.0582883395254612
2022-01-11 13:24:09,892 - depth - INFO - Iter [13700/38400]	lr: 9.839e-05, eta: 4:24:47, time: 0.816, data_time: 0.228, memory: 30431, decode.aux_loss_ce_2: 0.0045, decode.aux_loss_depth_2: 0.1759, decode.aux_loss_ce_5: 0.0045, decode.aux_loss_depth_5: 0.3517, decode.loss_ce: 0.0044, decode.ce_acc_level_0: 86.3750, decode.ce_acc_level_1: 98.5625, decode.loss_depth: 0.6999, loss: 1.2408, grad_norm: 5.6678
2022-01-11 13:25:09,638 - depth - INFO - Iter [13800/38400]	lr: 9.824e-05, eta: 4:23:35, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0044, decode.aux_loss_depth_2: 0.1714, decode.aux_loss_ce_5: 0.0050, decode.aux_loss_depth_5: 0.3433, decode.loss_ce: 0.0048, decode.ce_acc_level_0: 85.1875, decode.ce_acc_level_1: 98.1875, decode.loss_depth: 0.6846, loss: 1.2136, grad_norm: 5.4789
2022-01-11 13:26:09,096 - depth - INFO - Iter [13900/38400]	lr: 9.808e-05, eta: 4:22:22, time: 0.595, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0043, decode.aux_loss_depth_2: 0.1672, decode.aux_loss_ce_5: 0.0047, decode.aux_loss_depth_5: 0.3337, decode.loss_ce: 0.0045, decode.ce_acc_level_0: 85.1250, decode.ce_acc_level_1: 98.5625, decode.loss_depth: 0.6666, loss: 1.1810, grad_norm: 5.0214
2022-01-11 13:27:08,679 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 13:27:08,679 - depth - INFO - Iter [14000/38400]	lr: 9.792e-05, eta: 4:21:10, time: 0.596, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0047, decode.aux_loss_depth_2: 0.1712, decode.aux_loss_ce_5: 0.0053, decode.aux_loss_depth_5: 0.3417, decode.loss_ce: 0.0051, decode.ce_acc_level_0: 84.0000, decode.ce_acc_level_1: 98.4375, decode.loss_depth: 0.6833, loss: 1.2112, grad_norm: 6.7932
2022-01-11 13:28:08,344 - depth - INFO - Iter [14100/38400]	lr: 9.774e-05, eta: 4:19:57, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0050, decode.aux_loss_depth_2: 0.1669, decode.aux_loss_ce_5: 0.0054, decode.aux_loss_depth_5: 0.3332, decode.loss_ce: 0.0053, decode.ce_acc_level_0: 83.1875, decode.ce_acc_level_1: 98.3125, decode.loss_depth: 0.6665, loss: 1.1823, grad_norm: 5.1048
2022-01-11 13:29:07,711 - depth - INFO - Iter [14200/38400]	lr: 9.757e-05, eta: 4:18:45, time: 0.594, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0039, decode.aux_loss_depth_2: 0.1668, decode.aux_loss_ce_5: 0.0041, decode.aux_loss_depth_5: 0.3334, decode.loss_ce: 0.0042, decode.ce_acc_level_0: 87.9375, decode.ce_acc_level_1: 98.3750, decode.loss_depth: 0.6671, loss: 1.1796, grad_norm: 4.8775
2022-01-11 13:30:07,339 - depth - INFO - Iter [14300/38400]	lr: 9.738e-05, eta: 4:17:33, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0037, decode.aux_loss_depth_2: 0.1678, decode.aux_loss_ce_5: 0.0041, decode.aux_loss_depth_5: 0.3360, decode.loss_ce: 0.0044, decode.ce_acc_level_0: 85.8750, decode.ce_acc_level_1: 98.5625, decode.loss_depth: 0.6712, loss: 1.1872, grad_norm: 5.0994
2022-01-11 13:31:07,386 - depth - INFO - Saving checkpoint at 14400 iterations
2022-01-11 13:31:23,566 - depth - INFO - Iter [14400/38400]	lr: 9.720e-05, eta: 4:16:49, time: 0.763, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0039, decode.aux_loss_depth_2: 0.1645, decode.aux_loss_ce_5: 0.0041, decode.aux_loss_depth_5: 0.3297, decode.loss_ce: 0.0043, decode.ce_acc_level_0: 86.3750, decode.ce_acc_level_1: 98.8750, decode.loss_depth: 0.6588, loss: 1.1653, grad_norm: 4.9427
2022-01-11 13:31:45,004 - depth - INFO - Summary:
2022-01-11 13:31:45,004 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
| 0.9181 | 0.9887 | 0.9973 |  0.099  | 0.3472 | 0.042  |  0.1238  | 9.961 | 0.049  |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
2022-01-11 13:32:01,585 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_14400.pth.
2022-01-11 13:32:01,586 - depth - INFO - Best abs_rel is 0.0990 at 14400 iter.
2022-01-11 13:32:01,588 - depth - INFO - Iter(val) [82]	a1: 0.9181, a2: 0.9887, a3: 0.9973, abs_rel: 0.0990498885512352, rmse: 0.3472457230091095, log_10: 0.042030900716781616, rmse_log: 0.12382864207029343, silog: 9.9610, sq_rel: 0.04901689291000366
2022-01-11 13:33:01,483 - depth - INFO - Iter [14500/38400]	lr: 9.700e-05, eta: 4:16:41, time: 0.979, data_time: 0.387, memory: 30431, decode.aux_loss_ce_2: 0.0041, decode.aux_loss_depth_2: 0.1681, decode.aux_loss_ce_5: 0.0042, decode.aux_loss_depth_5: 0.3360, decode.loss_ce: 0.0045, decode.ce_acc_level_0: 86.4375, decode.ce_acc_level_1: 98.6875, decode.loss_depth: 0.6704, loss: 1.1873, grad_norm: 5.1415
2022-01-11 13:34:01,417 - depth - INFO - Iter [14600/38400]	lr: 9.680e-05, eta: 4:15:29, time: 0.599, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0034, decode.aux_loss_depth_2: 0.1670, decode.aux_loss_ce_5: 0.0037, decode.aux_loss_depth_5: 0.3344, decode.loss_ce: 0.0038, decode.ce_acc_level_0: 87.9375, decode.ce_acc_level_1: 99.0000, decode.loss_depth: 0.6672, loss: 1.1796, grad_norm: 5.0850
2022-01-11 13:35:01,497 - depth - INFO - Iter [14700/38400]	lr: 9.659e-05, eta: 4:14:17, time: 0.601, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0032, decode.aux_loss_depth_2: 0.1641, decode.aux_loss_ce_5: 0.0034, decode.aux_loss_depth_5: 0.3282, decode.loss_ce: 0.0034, decode.ce_acc_level_0: 88.9375, decode.ce_acc_level_1: 99.0625, decode.loss_depth: 0.6557, loss: 1.1581, grad_norm: 5.1341
2022-01-11 13:36:00,758 - depth - INFO - Iter [14800/38400]	lr: 9.637e-05, eta: 4:13:05, time: 0.593, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0036, decode.aux_loss_depth_2: 0.1684, decode.aux_loss_ce_5: 0.0040, decode.aux_loss_depth_5: 0.3373, decode.loss_ce: 0.0043, decode.ce_acc_level_0: 86.0000, decode.ce_acc_level_1: 98.8750, decode.loss_depth: 0.6736, loss: 1.1911, grad_norm: 5.0244
2022-01-11 13:37:00,191 - depth - INFO - Iter [14900/38400]	lr: 9.615e-05, eta: 4:11:53, time: 0.594, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0042, decode.aux_loss_depth_2: 0.1672, decode.aux_loss_ce_5: 0.0047, decode.aux_loss_depth_5: 0.3351, decode.loss_ce: 0.0052, decode.ce_acc_level_0: 84.3125, decode.ce_acc_level_1: 98.3125, decode.loss_depth: 0.6675, loss: 1.1839, grad_norm: 4.7534
2022-01-11 13:37:59,717 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 13:37:59,717 - depth - INFO - Iter [15000/38400]	lr: 9.592e-05, eta: 4:10:41, time: 0.595, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0032, decode.aux_loss_depth_2: 0.1624, decode.aux_loss_ce_5: 0.0036, decode.aux_loss_depth_5: 0.3249, decode.loss_ce: 0.0038, decode.ce_acc_level_0: 88.3750, decode.ce_acc_level_1: 99.0625, decode.loss_depth: 0.6477, loss: 1.1456, grad_norm: 4.5438
2022-01-11 13:38:59,123 - depth - INFO - Iter [15100/38400]	lr: 9.569e-05, eta: 4:09:29, time: 0.594, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0030, decode.aux_loss_depth_2: 0.1605, decode.aux_loss_ce_5: 0.0031, decode.aux_loss_depth_5: 0.3205, decode.loss_ce: 0.0033, decode.ce_acc_level_0: 89.8125, decode.ce_acc_level_1: 99.5625, decode.loss_depth: 0.6399, loss: 1.1304, grad_norm: 4.9764
2022-01-11 13:39:59,106 - depth - INFO - Iter [15200/38400]	lr: 9.545e-05, eta: 4:08:18, time: 0.600, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0029, decode.aux_loss_depth_2: 0.1629, decode.aux_loss_ce_5: 0.0031, decode.aux_loss_depth_5: 0.3251, decode.loss_ce: 0.0031, decode.ce_acc_level_0: 90.4375, decode.ce_acc_level_1: 99.3125, decode.loss_depth: 0.6498, loss: 1.1469, grad_norm: 5.2986
2022-01-11 13:40:20,845 - depth - INFO - Summary:
2022-01-11 13:40:20,845 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.9155 | 0.9891 | 0.9972 |  0.1012 | 0.3512 | 0.0429 |  0.1257  | 10.0613 | 0.0537 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-11 13:40:20,846 - depth - INFO - Iter(val) [82]	a1: 0.9155, a2: 0.9891, a3: 0.9972, abs_rel: 0.10119692981243134, rmse: 0.3511596918106079, log_10: 0.042929764837026596, rmse_log: 0.12568247318267822, silog: 10.0613, sq_rel: 0.053665220737457275
2022-01-11 13:41:20,224 - depth - INFO - Iter [15300/38400]	lr: 9.520e-05, eta: 4:07:40, time: 0.811, data_time: 0.225, memory: 30431, decode.aux_loss_ce_2: 0.0031, decode.aux_loss_depth_2: 0.1564, decode.aux_loss_ce_5: 0.0035, decode.aux_loss_depth_5: 0.3126, decode.loss_ce: 0.0039, decode.ce_acc_level_0: 88.1250, decode.ce_acc_level_1: 99.1250, decode.loss_depth: 0.6229, loss: 1.1024, grad_norm: 5.2889
2022-01-11 13:42:19,808 - depth - INFO - Iter [15400/38400]	lr: 9.495e-05, eta: 4:06:28, time: 0.595, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0030, decode.aux_loss_depth_2: 0.1575, decode.aux_loss_ce_5: 0.0033, decode.aux_loss_depth_5: 0.3148, decode.loss_ce: 0.0036, decode.ce_acc_level_0: 89.0000, decode.ce_acc_level_1: 99.1875, decode.loss_depth: 0.6285, loss: 1.1106, grad_norm: 4.8244
2022-01-11 13:43:19,612 - depth - INFO - Iter [15500/38400]	lr: 9.469e-05, eta: 4:05:17, time: 0.598, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0024, decode.aux_loss_depth_2: 0.1549, decode.aux_loss_ce_5: 0.0027, decode.aux_loss_depth_5: 0.3098, decode.loss_ce: 0.0028, decode.ce_acc_level_0: 91.0000, decode.ce_acc_level_1: 99.5625, decode.loss_depth: 0.6178, loss: 1.0904, grad_norm: 4.7236
2022-01-11 13:44:18,981 - depth - INFO - Iter [15600/38400]	lr: 9.442e-05, eta: 4:04:06, time: 0.594, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0028, decode.aux_loss_depth_2: 0.1572, decode.aux_loss_ce_5: 0.0028, decode.aux_loss_depth_5: 0.3147, decode.loss_ce: 0.0031, decode.ce_acc_level_0: 89.3125, decode.ce_acc_level_1: 99.3750, decode.loss_depth: 0.6274, loss: 1.1080, grad_norm: 5.1814
2022-01-11 13:45:19,126 - depth - INFO - Iter [15700/38400]	lr: 9.415e-05, eta: 4:02:56, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0038, decode.aux_loss_depth_2: 0.1564, decode.aux_loss_ce_5: 0.0039, decode.aux_loss_depth_5: 0.3140, decode.loss_ce: 0.0042, decode.ce_acc_level_0: 87.1250, decode.ce_acc_level_1: 98.9375, decode.loss_depth: 0.6254, loss: 1.1077, grad_norm: 4.6749
2022-01-11 13:46:18,955 - depth - INFO - Iter [15800/38400]	lr: 9.388e-05, eta: 4:01:45, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0024, decode.aux_loss_depth_2: 0.1534, decode.aux_loss_ce_5: 0.0027, decode.aux_loss_depth_5: 0.3074, decode.loss_ce: 0.0030, decode.ce_acc_level_0: 90.3750, decode.ce_acc_level_1: 99.1875, decode.loss_depth: 0.6135, loss: 1.0825, grad_norm: 5.4796
2022-01-11 13:47:18,354 - depth - INFO - Iter [15900/38400]	lr: 9.359e-05, eta: 4:00:34, time: 0.594, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0021, decode.aux_loss_depth_2: 0.1507, decode.aux_loss_ce_5: 0.0023, decode.aux_loss_depth_5: 0.3017, decode.loss_ce: 0.0025, decode.ce_acc_level_0: 92.4375, decode.ce_acc_level_1: 99.6250, decode.loss_depth: 0.6023, loss: 1.0615, grad_norm: 4.4921
2022-01-11 13:48:18,464 - depth - INFO - Saving checkpoint at 16000 iterations
2022-01-11 13:48:33,997 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 13:48:33,998 - depth - INFO - Iter [16000/38400]	lr: 9.330e-05, eta: 3:59:46, time: 0.756, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0022, decode.aux_loss_depth_2: 0.1589, decode.aux_loss_ce_5: 0.0026, decode.aux_loss_depth_5: 0.3177, decode.loss_ce: 0.0027, decode.ce_acc_level_0: 92.1250, decode.ce_acc_level_1: 99.4375, decode.loss_depth: 0.6341, loss: 1.1182, grad_norm: 4.9388
2022-01-11 13:48:55,593 - depth - INFO - Summary:
2022-01-11 13:48:55,594 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9144 | 0.9886 | 0.9974 |  0.1022 | 0.3488 | 0.0425 |  0.1251  | 9.9666 | 0.0516 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 13:48:55,595 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 13:48:55,595 - depth - INFO - Iter(val) [82]	a1: 0.9144, a2: 0.9886, a3: 0.9974, abs_rel: 0.10217250138521194, rmse: 0.34876254200935364, log_10: 0.0425492599606514, rmse_log: 0.12512366473674774, silog: 9.9666, sq_rel: 0.05158792436122894
2022-01-11 13:49:55,777 - depth - INFO - Iter [16100/38400]	lr: 9.301e-05, eta: 3:59:06, time: 0.817, data_time: 0.223, memory: 30431, decode.aux_loss_ce_2: 0.0027, decode.aux_loss_depth_2: 0.1537, decode.aux_loss_ce_5: 0.0028, decode.aux_loss_depth_5: 0.3074, decode.loss_ce: 0.0029, decode.ce_acc_level_0: 90.4375, decode.ce_acc_level_1: 99.5625, decode.loss_depth: 0.6140, loss: 1.0835, grad_norm: 4.2918
2022-01-11 13:50:55,153 - depth - INFO - Iter [16200/38400]	lr: 9.271e-05, eta: 3:57:55, time: 0.594, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0029, decode.aux_loss_depth_2: 0.1521, decode.aux_loss_ce_5: 0.0036, decode.aux_loss_depth_5: 0.3036, decode.loss_ce: 0.0034, decode.ce_acc_level_0: 89.3125, decode.ce_acc_level_1: 99.1875, decode.loss_depth: 0.6069, loss: 1.0725, grad_norm: 4.4343
2022-01-11 13:51:55,068 - depth - INFO - Iter [16300/38400]	lr: 9.240e-05, eta: 3:56:45, time: 0.599, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0022, decode.aux_loss_depth_2: 0.1542, decode.aux_loss_ce_5: 0.0026, decode.aux_loss_depth_5: 0.3088, decode.loss_ce: 0.0026, decode.ce_acc_level_0: 91.2500, decode.ce_acc_level_1: 99.5625, decode.loss_depth: 0.6155, loss: 1.0859, grad_norm: 4.2262
2022-01-11 13:52:54,898 - depth - INFO - Iter [16400/38400]	lr: 9.209e-05, eta: 3:55:35, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0023, decode.aux_loss_depth_2: 0.1551, decode.aux_loss_ce_5: 0.0027, decode.aux_loss_depth_5: 0.3106, decode.loss_ce: 0.0029, decode.ce_acc_level_0: 90.1875, decode.ce_acc_level_1: 99.3750, decode.loss_depth: 0.6192, loss: 1.0928, grad_norm: 4.8094
2022-01-11 13:53:54,372 - depth - INFO - Iter [16500/38400]	lr: 9.177e-05, eta: 3:54:24, time: 0.594, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0022, decode.aux_loss_depth_2: 0.1532, decode.aux_loss_ce_5: 0.0024, decode.aux_loss_depth_5: 0.3068, decode.loss_ce: 0.0026, decode.ce_acc_level_0: 91.4375, decode.ce_acc_level_1: 99.2500, decode.loss_depth: 0.6118, loss: 1.0791, grad_norm: 4.3074
2022-01-11 13:54:53,427 - depth - INFO - Iter [16600/38400]	lr: 9.145e-05, eta: 3:53:13, time: 0.590, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0021, decode.aux_loss_depth_2: 0.1518, decode.aux_loss_ce_5: 0.0026, decode.aux_loss_depth_5: 0.3040, decode.loss_ce: 0.0025, decode.ce_acc_level_0: 92.0625, decode.ce_acc_level_1: 99.5625, decode.loss_depth: 0.6075, loss: 1.0706, grad_norm: 4.3826
2022-01-11 13:55:53,023 - depth - INFO - Iter [16700/38400]	lr: 9.112e-05, eta: 3:52:03, time: 0.596, data_time: 0.010, memory: 30431, decode.aux_loss_ce_2: 0.0024, decode.aux_loss_depth_2: 0.1485, decode.aux_loss_ce_5: 0.0027, decode.aux_loss_depth_5: 0.2967, decode.loss_ce: 0.0030, decode.ce_acc_level_0: 90.0000, decode.ce_acc_level_1: 99.3750, decode.loss_depth: 0.5940, loss: 1.0473, grad_norm: 4.6643
2022-01-11 13:56:52,913 - depth - INFO - Iter [16800/38400]	lr: 9.078e-05, eta: 3:50:53, time: 0.599, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0020, decode.aux_loss_depth_2: 0.1473, decode.aux_loss_ce_5: 0.0025, decode.aux_loss_depth_5: 0.2947, decode.loss_ce: 0.0027, decode.ce_acc_level_0: 90.9375, decode.ce_acc_level_1: 99.5625, decode.loss_depth: 0.5908, loss: 1.0399, grad_norm: 4.3465
2022-01-11 13:57:14,479 - depth - INFO - Summary:
2022-01-11 13:57:14,481 - depth - INFO - 
+--------+-------+--------+---------+--------+--------+----------+-------+--------+
|   a1   |   a2  |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog | sq_rel |
+--------+-------+--------+---------+--------+--------+----------+-------+--------+
| 0.9231 | 0.989 | 0.9972 |  0.0962 | 0.3436 | 0.041  |  0.1213  | 9.831 | 0.0492 |
+--------+-------+--------+---------+--------+--------+----------+-------+--------+
2022-01-11 13:57:30,933 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_16800.pth.
2022-01-11 13:57:30,934 - depth - INFO - Best abs_rel is 0.0962 at 16800 iter.
2022-01-11 13:57:30,937 - depth - INFO - Iter(val) [82]	a1: 0.9231, a2: 0.9890, a3: 0.9972, abs_rel: 0.09621372818946838, rmse: 0.34363633394241333, log_10: 0.04102069512009621, rmse_log: 0.12125524878501892, silog: 9.8310, sq_rel: 0.04920221492648125
2022-01-11 13:58:30,960 - depth - INFO - Iter [16900/38400]	lr: 9.044e-05, eta: 3:50:32, time: 0.981, data_time: 0.389, memory: 30431, decode.aux_loss_ce_2: 0.0028, decode.aux_loss_depth_2: 0.1493, decode.aux_loss_ce_5: 0.0035, decode.aux_loss_depth_5: 0.2993, decode.loss_ce: 0.0037, decode.ce_acc_level_0: 88.0625, decode.ce_acc_level_1: 98.9375, decode.loss_depth: 0.5985, loss: 1.0572, grad_norm: 4.5058
2022-01-11 13:59:31,088 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 13:59:31,089 - depth - INFO - Iter [17000/38400]	lr: 9.009e-05, eta: 3:49:23, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0018, decode.aux_loss_depth_2: 0.1479, decode.aux_loss_ce_5: 0.0026, decode.aux_loss_depth_5: 0.2965, decode.loss_ce: 0.0025, decode.ce_acc_level_0: 91.7500, decode.ce_acc_level_1: 99.7500, decode.loss_depth: 0.5923, loss: 1.0434, grad_norm: 4.2371
2022-01-11 14:00:30,892 - depth - INFO - Iter [17100/38400]	lr: 8.974e-05, eta: 3:48:13, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0017, decode.aux_loss_depth_2: 0.1457, decode.aux_loss_ce_5: 0.0022, decode.aux_loss_depth_5: 0.2904, decode.loss_ce: 0.0023, decode.ce_acc_level_0: 93.0000, decode.ce_acc_level_1: 99.5625, decode.loss_depth: 0.5817, loss: 1.0238, grad_norm: 4.1132
2022-01-11 14:01:30,520 - depth - INFO - Iter [17200/38400]	lr: 8.939e-05, eta: 3:47:03, time: 0.597, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0022, decode.aux_loss_depth_2: 0.1430, decode.aux_loss_ce_5: 0.0025, decode.aux_loss_depth_5: 0.2859, decode.loss_ce: 0.0024, decode.ce_acc_level_0: 92.8750, decode.ce_acc_level_1: 99.2500, decode.loss_depth: 0.5714, loss: 1.0075, grad_norm: 4.4574
2022-01-11 14:02:29,613 - depth - INFO - Iter [17300/38400]	lr: 8.902e-05, eta: 3:45:52, time: 0.591, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0026, decode.aux_loss_depth_2: 0.1478, decode.aux_loss_ce_5: 0.0026, decode.aux_loss_depth_5: 0.2955, decode.loss_ce: 0.0028, decode.ce_acc_level_0: 91.5625, decode.ce_acc_level_1: 99.5625, decode.loss_depth: 0.5896, loss: 1.0409, grad_norm: 3.9510
2022-01-11 14:03:29,233 - depth - INFO - Iter [17400/38400]	lr: 8.866e-05, eta: 3:44:42, time: 0.596, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0018, decode.aux_loss_depth_2: 0.1455, decode.aux_loss_ce_5: 0.0021, decode.aux_loss_depth_5: 0.2906, decode.loss_ce: 0.0020, decode.ce_acc_level_0: 93.2500, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5811, loss: 1.0232, grad_norm: 4.2513
2022-01-11 14:04:29,066 - depth - INFO - Iter [17500/38400]	lr: 8.828e-05, eta: 3:43:33, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0018, decode.aux_loss_depth_2: 0.1451, decode.aux_loss_ce_5: 0.0022, decode.aux_loss_depth_5: 0.2910, decode.loss_ce: 0.0022, decode.ce_acc_level_0: 92.9375, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5804, loss: 1.0227, grad_norm: 4.0023
2022-01-11 14:05:28,835 - depth - INFO - Saving checkpoint at 17600 iterations
2022-01-11 14:05:44,985 - depth - INFO - Iter [17600/38400]	lr: 8.790e-05, eta: 3:42:43, time: 0.760, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0019, decode.aux_loss_depth_2: 0.1501, decode.aux_loss_ce_5: 0.0021, decode.aux_loss_depth_5: 0.3010, decode.loss_ce: 0.0022, decode.ce_acc_level_0: 93.0000, decode.ce_acc_level_1: 99.6250, decode.loss_depth: 0.5999, loss: 1.0573, grad_norm: 4.4336
2022-01-11 14:06:06,331 - depth - INFO - Summary:
2022-01-11 14:06:06,332 - depth - INFO - 
+--------+--------+--------+---------+-------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+-------+--------+----------+---------+--------+
| 0.9073 | 0.9883 | 0.9973 |  0.1036 | 0.361 | 0.0438 |  0.128   | 10.1069 | 0.0544 |
+--------+--------+--------+---------+-------+--------+----------+---------+--------+
2022-01-11 14:06:06,335 - depth - INFO - Iter(val) [82]	a1: 0.9073, a2: 0.9883, a3: 0.9973, abs_rel: 0.10363573580980301, rmse: 0.36096906661987305, log_10: 0.04380539804697037, rmse_log: 0.12801699340343475, silog: 10.1069, sq_rel: 0.05443824455142021
2022-01-11 14:07:06,051 - depth - INFO - Iter [17700/38400]	lr: 8.752e-05, eta: 3:41:58, time: 0.810, data_time: 0.220, memory: 30431, decode.aux_loss_ce_2: 0.0017, decode.aux_loss_depth_2: 0.1485, decode.aux_loss_ce_5: 0.0018, decode.aux_loss_depth_5: 0.2979, decode.loss_ce: 0.0019, decode.ce_acc_level_0: 93.9375, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5948, loss: 1.0466, grad_norm: 4.4165
2022-01-11 14:08:05,636 - depth - INFO - Iter [17800/38400]	lr: 8.713e-05, eta: 3:40:48, time: 0.596, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0015, decode.aux_loss_depth_2: 0.1453, decode.aux_loss_ce_5: 0.0018, decode.aux_loss_depth_5: 0.2912, decode.loss_ce: 0.0018, decode.ce_acc_level_0: 93.8750, decode.ce_acc_level_1: 99.7500, decode.loss_depth: 0.5809, loss: 1.0225, grad_norm: 4.3787
2022-01-11 14:09:05,765 - depth - INFO - Iter [17900/38400]	lr: 8.674e-05, eta: 3:39:39, time: 0.601, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0023, decode.aux_loss_depth_2: 0.1442, decode.aux_loss_ce_5: 0.0025, decode.aux_loss_depth_5: 0.2889, decode.loss_ce: 0.0023, decode.ce_acc_level_0: 92.5000, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5765, loss: 1.0168, grad_norm: 4.2076
2022-01-11 14:10:05,549 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 14:10:05,549 - depth - INFO - Iter [18000/38400]	lr: 8.634e-05, eta: 3:38:30, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0027, decode.aux_loss_depth_2: 0.1451, decode.aux_loss_ce_5: 0.0030, decode.aux_loss_depth_5: 0.2907, decode.loss_ce: 0.0030, decode.ce_acc_level_0: 91.3125, decode.ce_acc_level_1: 99.4375, decode.loss_depth: 0.5799, loss: 1.0245, grad_norm: 4.0543
2022-01-11 14:11:05,198 - depth - INFO - Iter [18100/38400]	lr: 8.593e-05, eta: 3:37:20, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0018, decode.aux_loss_depth_2: 0.1438, decode.aux_loss_ce_5: 0.0022, decode.aux_loss_depth_5: 0.2878, decode.loss_ce: 0.0023, decode.ce_acc_level_0: 92.5000, decode.ce_acc_level_1: 99.4375, decode.loss_depth: 0.5740, loss: 1.0119, grad_norm: 4.2548
2022-01-11 14:12:05,179 - depth - INFO - Iter [18200/38400]	lr: 8.553e-05, eta: 3:36:11, time: 0.600, data_time: 0.010, memory: 30431, decode.aux_loss_ce_2: 0.0015, decode.aux_loss_depth_2: 0.1431, decode.aux_loss_ce_5: 0.0018, decode.aux_loss_depth_5: 0.2866, decode.loss_ce: 0.0018, decode.ce_acc_level_0: 94.3750, decode.ce_acc_level_1: 99.6875, decode.loss_depth: 0.5711, loss: 1.0059, grad_norm: 3.9792
2022-01-11 14:13:05,233 - depth - INFO - Iter [18300/38400]	lr: 8.511e-05, eta: 3:35:03, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0019, decode.aux_loss_depth_2: 0.1422, decode.aux_loss_ce_5: 0.0022, decode.aux_loss_depth_5: 0.2850, decode.loss_ce: 0.0025, decode.ce_acc_level_0: 91.4375, decode.ce_acc_level_1: 99.7500, decode.loss_depth: 0.5680, loss: 1.0018, grad_norm: 3.9371
2022-01-11 14:14:05,045 - depth - INFO - Iter [18400/38400]	lr: 8.469e-05, eta: 3:33:53, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0021, decode.aux_loss_depth_2: 0.1404, decode.aux_loss_ce_5: 0.0027, decode.aux_loss_depth_5: 0.2807, decode.loss_ce: 0.0026, decode.ce_acc_level_0: 91.8750, decode.ce_acc_level_1: 99.7500, decode.loss_depth: 0.5596, loss: 0.9882, grad_norm: 4.0917
2022-01-11 14:14:26,555 - depth - INFO - Summary:
2022-01-11 14:14:26,556 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9226 | 0.9892 | 0.9975 |  0.0953 | 0.3367 | 0.0405 |  0.1202  | 9.6246 | 0.048  |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 14:14:40,518 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_18400.pth.
2022-01-11 14:14:40,518 - depth - INFO - Best abs_rel is 0.0953 at 18400 iter.
2022-01-11 14:14:40,519 - depth - INFO - Iter(val) [82]	a1: 0.9226, a2: 0.9892, a3: 0.9975, abs_rel: 0.0953008383512497, rmse: 0.33666926622390747, log_10: 0.04053341597318649, rmse_log: 0.12023086845874786, silog: 9.6246, sq_rel: 0.04804910346865654
2022-01-11 14:15:40,628 - depth - INFO - Iter [18500/38400]	lr: 8.427e-05, eta: 3:33:23, time: 0.956, data_time: 0.363, memory: 30431, decode.aux_loss_ce_2: 0.0013, decode.aux_loss_depth_2: 0.1384, decode.aux_loss_ce_5: 0.0018, decode.aux_loss_depth_5: 0.2763, decode.loss_ce: 0.0017, decode.ce_acc_level_0: 94.5625, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5513, loss: 0.9708, grad_norm: 3.8129
2022-01-11 14:16:40,151 - depth - INFO - Iter [18600/38400]	lr: 8.384e-05, eta: 3:32:14, time: 0.594, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0017, decode.aux_loss_depth_2: 0.1363, decode.aux_loss_ce_5: 0.0016, decode.aux_loss_depth_5: 0.2725, decode.loss_ce: 0.0017, decode.ce_acc_level_0: 94.1250, decode.ce_acc_level_1: 99.6875, decode.loss_depth: 0.5449, loss: 0.9588, grad_norm: 3.7782
2022-01-11 14:17:40,014 - depth - INFO - Iter [18700/38400]	lr: 8.341e-05, eta: 3:31:05, time: 0.600, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0016, decode.aux_loss_depth_2: 0.1382, decode.aux_loss_ce_5: 0.0016, decode.aux_loss_depth_5: 0.2762, decode.loss_ce: 0.0019, decode.ce_acc_level_0: 94.0000, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5513, loss: 0.9708, grad_norm: 4.5066
2022-01-11 14:18:39,763 - depth - INFO - Iter [18800/38400]	lr: 8.297e-05, eta: 3:29:56, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0011, decode.aux_loss_depth_2: 0.1391, decode.aux_loss_ce_5: 0.0014, decode.aux_loss_depth_5: 0.2781, decode.loss_ce: 0.0013, decode.ce_acc_level_0: 95.6875, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5555, loss: 0.9763, grad_norm: 3.8466
2022-01-11 14:19:39,816 - depth - INFO - Iter [18900/38400]	lr: 8.253e-05, eta: 3:28:47, time: 0.600, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0014, decode.aux_loss_depth_2: 0.1379, decode.aux_loss_ce_5: 0.0016, decode.aux_loss_depth_5: 0.2753, decode.loss_ce: 0.0019, decode.ce_acc_level_0: 93.6250, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5507, loss: 0.9689, grad_norm: 3.6624
2022-01-11 14:20:39,472 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 14:20:39,473 - depth - INFO - Iter [19000/38400]	lr: 8.209e-05, eta: 3:27:38, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0016, decode.aux_loss_depth_2: 0.1391, decode.aux_loss_ce_5: 0.0019, decode.aux_loss_depth_5: 0.2779, decode.loss_ce: 0.0018, decode.ce_acc_level_0: 94.6250, decode.ce_acc_level_1: 99.7500, decode.loss_depth: 0.5550, loss: 0.9775, grad_norm: 3.7445
2022-01-11 14:21:39,464 - depth - INFO - Iter [19100/38400]	lr: 8.164e-05, eta: 3:26:30, time: 0.599, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0013, decode.aux_loss_depth_2: 0.1391, decode.aux_loss_ce_5: 0.0013, decode.aux_loss_depth_5: 0.2784, decode.loss_ce: 0.0014, decode.ce_acc_level_0: 95.3750, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5555, loss: 0.9771, grad_norm: 3.9167
2022-01-11 14:22:39,120 - depth - INFO - Saving checkpoint at 19200 iterations
2022-01-11 14:22:55,454 - depth - INFO - Iter [19200/38400]	lr: 8.118e-05, eta: 3:25:37, time: 0.761, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0012, decode.aux_loss_depth_2: 0.1354, decode.aux_loss_ce_5: 0.0014, decode.aux_loss_depth_5: 0.2706, decode.loss_ce: 0.0016, decode.ce_acc_level_0: 94.8125, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5405, loss: 0.9506, grad_norm: 4.0454
2022-01-11 14:23:16,972 - depth - INFO - Summary:
2022-01-11 14:23:16,973 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9203 | 0.9892 | 0.9969 |  0.0966 | 0.3446 | 0.041  |  0.1212  | 9.6342 | 0.0503 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 14:23:16,973 - depth - INFO - Iter(val) [82]	a1: 0.9203, a2: 0.9892, a3: 0.9969, abs_rel: 0.09663403034210205, rmse: 0.3446217477321625, log_10: 0.04101111367344856, rmse_log: 0.12122872471809387, silog: 9.6342, sq_rel: 0.05034079775214195
2022-01-11 14:24:16,975 - depth - INFO - Iter [19300/38400]	lr: 8.072e-05, eta: 3:24:50, time: 0.815, data_time: 0.222, memory: 30431, decode.aux_loss_ce_2: 0.0019, decode.aux_loss_depth_2: 0.1346, decode.aux_loss_ce_5: 0.0021, decode.aux_loss_depth_5: 0.2685, decode.loss_ce: 0.0020, decode.ce_acc_level_0: 93.8125, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5368, loss: 0.9459, grad_norm: 4.1273
2022-01-11 14:25:16,528 - depth - INFO - Iter [19400/38400]	lr: 8.026e-05, eta: 3:23:41, time: 0.595, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0014, decode.aux_loss_depth_2: 0.1376, decode.aux_loss_ce_5: 0.0016, decode.aux_loss_depth_5: 0.2753, decode.loss_ce: 0.0015, decode.ce_acc_level_0: 95.1250, decode.ce_acc_level_1: 99.5625, decode.loss_depth: 0.5499, loss: 0.9673, grad_norm: 3.6254
2022-01-11 14:26:16,071 - depth - INFO - Iter [19500/38400]	lr: 7.979e-05, eta: 3:22:32, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0010, decode.aux_loss_depth_2: 0.1378, decode.aux_loss_ce_5: 0.0011, decode.aux_loss_depth_5: 0.2750, decode.loss_ce: 0.0012, decode.ce_acc_level_0: 96.4375, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5495, loss: 0.9656, grad_norm: 3.7301
2022-01-11 14:27:15,540 - depth - INFO - Iter [19600/38400]	lr: 7.932e-05, eta: 3:21:23, time: 0.594, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0010, decode.aux_loss_depth_2: 0.1348, decode.aux_loss_ce_5: 0.0012, decode.aux_loss_depth_5: 0.2697, decode.loss_ce: 0.0014, decode.ce_acc_level_0: 95.9375, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5379, loss: 0.9461, grad_norm: 4.0319
2022-01-11 14:28:15,052 - depth - INFO - Iter [19700/38400]	lr: 7.885e-05, eta: 3:20:14, time: 0.595, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0009, decode.aux_loss_depth_2: 0.1344, decode.aux_loss_ce_5: 0.0011, decode.aux_loss_depth_5: 0.2689, decode.loss_ce: 0.0011, decode.ce_acc_level_0: 96.7500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5364, loss: 0.9429, grad_norm: 3.9612
2022-01-11 14:29:15,270 - depth - INFO - Iter [19800/38400]	lr: 7.837e-05, eta: 3:19:06, time: 0.602, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0010, decode.aux_loss_depth_2: 0.1342, decode.aux_loss_ce_5: 0.0012, decode.aux_loss_depth_5: 0.2688, decode.loss_ce: 0.0012, decode.ce_acc_level_0: 95.7500, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5369, loss: 0.9434, grad_norm: 3.7378
2022-01-11 14:30:14,602 - depth - INFO - Iter [19900/38400]	lr: 7.788e-05, eta: 3:17:57, time: 0.594, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0013, decode.aux_loss_depth_2: 0.1328, decode.aux_loss_ce_5: 0.0015, decode.aux_loss_depth_5: 0.2655, decode.loss_ce: 0.0015, decode.ce_acc_level_0: 95.1250, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5308, loss: 0.9335, grad_norm: 3.7699
2022-01-11 14:31:14,223 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 14:31:14,224 - depth - INFO - Iter [20000/38400]	lr: 7.740e-05, eta: 3:16:49, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0008, decode.aux_loss_depth_2: 0.1331, decode.aux_loss_ce_5: 0.0009, decode.aux_loss_depth_5: 0.2666, decode.loss_ce: 0.0010, decode.ce_acc_level_0: 96.4375, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5324, loss: 0.9349, grad_norm: 4.1454
2022-01-11 14:31:35,604 - depth - INFO - Summary:
2022-01-11 14:31:35,604 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9189 | 0.9896 | 0.9974 |  0.0966 | 0.3442 | 0.0412 |  0.1217  | 9.7038 | 0.0496 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 14:31:35,605 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 14:31:35,605 - depth - INFO - Iter(val) [82]	a1: 0.9189, a2: 0.9896, a3: 0.9974, abs_rel: 0.09656200557947159, rmse: 0.3441702425479889, log_10: 0.041243694722652435, rmse_log: 0.12174089252948761, silog: 9.7038, sq_rel: 0.04961227998137474
2022-01-11 14:32:35,611 - depth - INFO - Iter [20100/38400]	lr: 7.691e-05, eta: 3:16:01, time: 0.814, data_time: 0.222, memory: 30431, decode.aux_loss_ce_2: 0.0011, decode.aux_loss_depth_2: 0.1302, decode.aux_loss_ce_5: 0.0014, decode.aux_loss_depth_5: 0.2610, decode.loss_ce: 0.0014, decode.ce_acc_level_0: 95.5625, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5200, loss: 0.9152, grad_norm: 3.3417
2022-01-11 14:33:36,045 - depth - INFO - Iter [20200/38400]	lr: 7.641e-05, eta: 3:14:53, time: 0.604, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0010, decode.aux_loss_depth_2: 0.1300, decode.aux_loss_ce_5: 0.0013, decode.aux_loss_depth_5: 0.2598, decode.loss_ce: 0.0013, decode.ce_acc_level_0: 95.7500, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5188, loss: 0.9121, grad_norm: 3.5679
2022-01-11 14:34:35,627 - depth - INFO - Iter [20300/38400]	lr: 7.591e-05, eta: 3:13:44, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0009, decode.aux_loss_depth_2: 0.1296, decode.aux_loss_ce_5: 0.0013, decode.aux_loss_depth_5: 0.2594, decode.loss_ce: 0.0012, decode.ce_acc_level_0: 96.5000, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5177, loss: 0.9102, grad_norm: 3.4107
2022-01-11 14:35:35,505 - depth - INFO - Iter [20400/38400]	lr: 7.541e-05, eta: 3:12:36, time: 0.599, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0010, decode.aux_loss_depth_2: 0.1310, decode.aux_loss_ce_5: 0.0012, decode.aux_loss_depth_5: 0.2624, decode.loss_ce: 0.0013, decode.ce_acc_level_0: 95.5000, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5241, loss: 0.9210, grad_norm: 3.5610
2022-01-11 14:36:35,083 - depth - INFO - Iter [20500/38400]	lr: 7.491e-05, eta: 3:11:28, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0012, decode.aux_loss_depth_2: 0.1308, decode.aux_loss_ce_5: 0.0011, decode.aux_loss_depth_5: 0.2618, decode.loss_ce: 0.0015, decode.ce_acc_level_0: 95.1250, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5226, loss: 0.9190, grad_norm: 3.7908
2022-01-11 14:37:34,434 - depth - INFO - Iter [20600/38400]	lr: 7.440e-05, eta: 3:10:20, time: 0.594, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0011, decode.aux_loss_depth_2: 0.1305, decode.aux_loss_ce_5: 0.0013, decode.aux_loss_depth_5: 0.2607, decode.loss_ce: 0.0015, decode.ce_acc_level_0: 95.3125, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5202, loss: 0.9153, grad_norm: 3.7067
2022-01-11 14:38:33,995 - depth - INFO - Iter [20700/38400]	lr: 7.389e-05, eta: 3:09:12, time: 0.595, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0012, decode.aux_loss_depth_2: 0.1285, decode.aux_loss_ce_5: 0.0012, decode.aux_loss_depth_5: 0.2573, decode.loss_ce: 0.0013, decode.ce_acc_level_0: 95.9375, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5143, loss: 0.9040, grad_norm: 4.1911
2022-01-11 14:39:33,494 - depth - INFO - Saving checkpoint at 20800 iterations
2022-01-11 14:39:50,279 - depth - INFO - Iter [20800/38400]	lr: 7.337e-05, eta: 3:08:18, time: 0.763, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0007, decode.aux_loss_depth_2: 0.1291, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2581, decode.loss_ce: 0.0009, decode.ce_acc_level_0: 97.4375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5159, loss: 0.9054, grad_norm: 3.5985
2022-01-11 14:40:11,641 - depth - INFO - Summary:
2022-01-11 14:40:11,642 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9172 | 0.9882 | 0.9972 |  0.0976 | 0.3403 | 0.0414 |  0.1221  | 9.6333 | 0.0505 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 14:40:11,642 - depth - INFO - Iter(val) [82]	a1: 0.9172, a2: 0.9882, a3: 0.9972, abs_rel: 0.09762567281723022, rmse: 0.340295672416687, log_10: 0.04139871150255203, rmse_log: 0.12208853662014008, silog: 9.6333, sq_rel: 0.05053040012717247
2022-01-11 14:41:11,481 - depth - INFO - Iter [20900/38400]	lr: 7.286e-05, eta: 3:07:28, time: 0.812, data_time: 0.221, memory: 30431, decode.aux_loss_ce_2: 0.0008, decode.aux_loss_depth_2: 0.1271, decode.aux_loss_ce_5: 0.0010, decode.aux_loss_depth_5: 0.2545, decode.loss_ce: 0.0009, decode.ce_acc_level_0: 96.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5077, loss: 0.8920, grad_norm: 3.6660
2022-01-11 14:42:10,875 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 14:42:10,875 - depth - INFO - Iter [21000/38400]	lr: 7.233e-05, eta: 3:06:19, time: 0.594, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0008, decode.aux_loss_depth_2: 0.1278, decode.aux_loss_ce_5: 0.0010, decode.aux_loss_depth_5: 0.2556, decode.loss_ce: 0.0010, decode.ce_acc_level_0: 97.0000, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5102, loss: 0.8964, grad_norm: 3.3170
2022-01-11 14:43:10,268 - depth - INFO - Iter [21100/38400]	lr: 7.181e-05, eta: 3:05:11, time: 0.594, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0008, decode.aux_loss_depth_2: 0.1296, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2594, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.7500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5184, loss: 0.9097, grad_norm: 3.6095
2022-01-11 14:44:10,219 - depth - INFO - Iter [21200/38400]	lr: 7.128e-05, eta: 3:04:04, time: 0.600, data_time: 0.011, memory: 30431, decode.aux_loss_ce_2: 0.0007, decode.aux_loss_depth_2: 0.1244, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2490, decode.loss_ce: 0.0009, decode.ce_acc_level_0: 97.2500, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.4974, loss: 0.8734, grad_norm: 3.1484
2022-01-11 14:45:09,884 - depth - INFO - Iter [21300/38400]	lr: 7.075e-05, eta: 3:02:56, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0007, decode.aux_loss_depth_2: 0.1267, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2533, decode.loss_ce: 0.0010, decode.ce_acc_level_0: 96.4375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5061, loss: 0.8886, grad_norm: 3.4233
2022-01-11 14:46:09,753 - depth - INFO - Iter [21400/38400]	lr: 7.022e-05, eta: 3:01:48, time: 0.599, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0009, decode.aux_loss_depth_2: 0.1246, decode.aux_loss_ce_5: 0.0012, decode.aux_loss_depth_5: 0.2498, decode.loss_ce: 0.0012, decode.ce_acc_level_0: 96.3125, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.4982, loss: 0.8760, grad_norm: 3.7787
2022-01-11 14:47:09,875 - depth - INFO - Iter [21500/38400]	lr: 6.968e-05, eta: 3:00:41, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0008, decode.aux_loss_depth_2: 0.1258, decode.aux_loss_ce_5: 0.0010, decode.aux_loss_depth_5: 0.2518, decode.loss_ce: 0.0009, decode.ce_acc_level_0: 97.1250, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5029, loss: 0.8831, grad_norm: 3.7448
2022-01-11 14:48:09,664 - depth - INFO - Iter [21600/38400]	lr: 6.915e-05, eta: 2:59:33, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0010, decode.aux_loss_depth_2: 0.1230, decode.aux_loss_ce_5: 0.0012, decode.aux_loss_depth_5: 0.2465, decode.loss_ce: 0.0010, decode.ce_acc_level_0: 96.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4914, loss: 0.8640, grad_norm: 3.5851
2022-01-11 14:48:31,199 - depth - INFO - Summary:
2022-01-11 14:48:31,200 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+--------+--------+
| 0.9161 | 0.9884 | 0.997 |  0.0997 | 0.3405 | 0.0417 |  0.1228  | 9.8254 | 0.0506 |
+--------+--------+-------+---------+--------+--------+----------+--------+--------+
2022-01-11 14:48:31,201 - depth - INFO - Iter(val) [82]	a1: 0.9161, a2: 0.9884, a3: 0.9970, abs_rel: 0.09973758459091187, rmse: 0.3405182361602783, log_10: 0.04174584150314331, rmse_log: 0.12281584739685059, silog: 9.8254, sq_rel: 0.050587721168994904
2022-01-11 14:49:30,880 - depth - INFO - Iter [21700/38400]	lr: 6.861e-05, eta: 2:58:42, time: 0.812, data_time: 0.222, memory: 30431, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1279, decode.aux_loss_ce_5: 0.0007, decode.aux_loss_depth_5: 0.2556, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5102, loss: 0.8956, grad_norm: 3.4950
2022-01-11 14:50:30,963 - depth - INFO - Iter [21800/38400]	lr: 6.806e-05, eta: 2:57:35, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1236, decode.aux_loss_ce_5: 0.0007, decode.aux_loss_depth_5: 0.2474, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 97.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4936, loss: 0.8666, grad_norm: 3.2900
2022-01-11 14:51:30,734 - depth - INFO - Iter [21900/38400]	lr: 6.752e-05, eta: 2:56:27, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0007, decode.aux_loss_depth_2: 0.1216, decode.aux_loss_ce_5: 0.0011, decode.aux_loss_depth_5: 0.2434, decode.loss_ce: 0.0009, decode.ce_acc_level_0: 96.9375, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.4858, loss: 0.8534, grad_norm: 3.5652
2022-01-11 14:52:30,605 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 14:52:30,605 - depth - INFO - Iter [22000/38400]	lr: 6.697e-05, eta: 2:55:20, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1236, decode.aux_loss_ce_5: 0.0007, decode.aux_loss_depth_5: 0.2472, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 97.4375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4937, loss: 0.8665, grad_norm: 3.8365
2022-01-11 14:53:30,135 - depth - INFO - Iter [22100/38400]	lr: 6.642e-05, eta: 2:54:12, time: 0.595, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0009, decode.aux_loss_depth_2: 0.1224, decode.aux_loss_ce_5: 0.0009, decode.aux_loss_depth_5: 0.2452, decode.loss_ce: 0.0010, decode.ce_acc_level_0: 97.2500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4893, loss: 0.8598, grad_norm: 3.1443
2022-01-11 14:54:29,486 - depth - INFO - Iter [22200/38400]	lr: 6.586e-05, eta: 2:53:05, time: 0.594, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1221, decode.aux_loss_ce_5: 0.0007, decode.aux_loss_depth_5: 0.2446, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.1250, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4878, loss: 0.8563, grad_norm: 3.1153
2022-01-11 14:55:29,307 - depth - INFO - Iter [22300/38400]	lr: 6.531e-05, eta: 2:51:58, time: 0.598, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1221, decode.aux_loss_ce_5: 0.0007, decode.aux_loss_depth_5: 0.2437, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 97.6875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4872, loss: 0.8548, grad_norm: 3.3662
2022-01-11 14:56:29,415 - depth - INFO - Saving checkpoint at 22400 iterations
2022-01-11 14:56:45,072 - depth - INFO - Iter [22400/38400]	lr: 6.475e-05, eta: 2:51:02, time: 0.758, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1247, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2493, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 98.3125, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.4976, loss: 0.8736, grad_norm: 3.5150
2022-01-11 14:57:06,423 - depth - INFO - Summary:
2022-01-11 14:57:06,424 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9235 | 0.9894 | 0.9973 |  0.0958 | 0.3367 | 0.0404 |  0.1196  | 9.5562 | 0.0485 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 14:57:06,425 - depth - INFO - Iter(val) [82]	a1: 0.9235, a2: 0.9894, a3: 0.9973, abs_rel: 0.0957789495587349, rmse: 0.33670172095298767, log_10: 0.040380485355854034, rmse_log: 0.11961360275745392, silog: 9.5562, sq_rel: 0.048513736575841904
2022-01-11 14:58:06,279 - depth - INFO - Iter [22500/38400]	lr: 6.419e-05, eta: 2:50:10, time: 0.812, data_time: 0.220, memory: 30431, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1211, decode.aux_loss_ce_5: 0.0010, decode.aux_loss_depth_5: 0.2424, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4836, loss: 0.8496, grad_norm: 3.5961
2022-01-11 14:59:06,038 - depth - INFO - Iter [22600/38400]	lr: 6.363e-05, eta: 2:49:02, time: 0.597, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0007, decode.aux_loss_depth_2: 0.1203, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2406, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.1250, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4805, loss: 0.8437, grad_norm: 3.3112
2022-01-11 15:00:05,384 - depth - INFO - Iter [22700/38400]	lr: 6.307e-05, eta: 2:47:55, time: 0.593, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0007, decode.aux_loss_depth_2: 0.1212, decode.aux_loss_ce_5: 0.0010, decode.aux_loss_depth_5: 0.2428, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.5000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4843, loss: 0.8508, grad_norm: 4.2224
2022-01-11 15:01:05,146 - depth - INFO - Iter [22800/38400]	lr: 6.250e-05, eta: 2:46:48, time: 0.598, data_time: 0.010, memory: 30431, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1180, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2360, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 97.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4711, loss: 0.8268, grad_norm: 3.0300
2022-01-11 15:02:04,605 - depth - INFO - Iter [22900/38400]	lr: 6.194e-05, eta: 2:45:40, time: 0.594, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1201, decode.aux_loss_ce_5: 0.0010, decode.aux_loss_depth_5: 0.2404, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.5625, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.4803, loss: 0.8431, grad_norm: 3.0635
2022-01-11 15:03:04,623 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 15:03:04,623 - depth - INFO - Iter [23000/38400]	lr: 6.137e-05, eta: 2:44:33, time: 0.600, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0007, decode.aux_loss_depth_2: 0.1180, decode.aux_loss_ce_5: 0.0007, decode.aux_loss_depth_5: 0.2361, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 97.8125, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.4711, loss: 0.8273, grad_norm: 3.3736
2022-01-11 15:04:04,563 - depth - INFO - Iter [23100/38400]	lr: 6.080e-05, eta: 2:43:27, time: 0.599, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1209, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2416, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 97.8750, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.4820, loss: 0.8463, grad_norm: 3.0567
2022-01-11 15:05:03,255 - depth - INFO - Iter [23200/38400]	lr: 6.023e-05, eta: 2:42:19, time: 0.587, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1168, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2336, decode.loss_ce: 0.0004, decode.ce_acc_level_0: 98.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4659, loss: 0.8177, grad_norm: 2.8199
2022-01-11 15:05:24,587 - depth - INFO - Summary:
2022-01-11 15:05:24,588 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
| 0.9261 | 0.9896 | 0.9973 |  0.0947 | 0.3312 | 0.0397 |  0.1183  | 9.373 | 0.0482 |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
2022-01-11 15:05:41,343 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_23200.pth.
2022-01-11 15:05:41,343 - depth - INFO - Best abs_rel is 0.0947 at 23200 iter.
2022-01-11 15:05:41,344 - depth - INFO - Iter(val) [82]	a1: 0.9261, a2: 0.9896, a3: 0.9973, abs_rel: 0.09467466175556183, rmse: 0.331163227558136, log_10: 0.0397491529583931, rmse_log: 0.11825767904520035, silog: 9.3730, sq_rel: 0.04817788302898407
2022-01-11 15:06:41,066 - depth - INFO - Iter [23300/38400]	lr: 5.966e-05, eta: 2:41:37, time: 0.978, data_time: 0.390, memory: 30431, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1189, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2379, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.6250, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4748, loss: 0.8332, grad_norm: 3.0920
2022-01-11 15:07:40,621 - depth - INFO - Iter [23400/38400]	lr: 5.908e-05, eta: 2:40:29, time: 0.596, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0008, decode.aux_loss_depth_2: 0.1167, decode.aux_loss_ce_5: 0.0009, decode.aux_loss_depth_5: 0.2333, decode.loss_ce: 0.0009, decode.ce_acc_level_0: 97.1875, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.4656, loss: 0.8182, grad_norm: 3.0827
2022-01-11 15:08:40,305 - depth - INFO - Iter [23500/38400]	lr: 5.851e-05, eta: 2:39:22, time: 0.597, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1179, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2357, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.0000, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.4703, loss: 0.8255, grad_norm: 3.1767
2022-01-11 15:09:40,226 - depth - INFO - Iter [23600/38400]	lr: 5.793e-05, eta: 2:38:16, time: 0.599, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1170, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2340, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4673, loss: 0.8201, grad_norm: 3.0780
2022-01-11 15:10:39,914 - depth - INFO - Iter [23700/38400]	lr: 5.735e-05, eta: 2:37:09, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1175, decode.aux_loss_ce_5: 0.0007, decode.aux_loss_depth_5: 0.2352, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 97.8750, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.4693, loss: 0.8237, grad_norm: 3.1165
2022-01-11 15:11:39,218 - depth - INFO - Iter [23800/38400]	lr: 5.678e-05, eta: 2:36:02, time: 0.593, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0007, decode.aux_loss_depth_2: 0.1179, decode.aux_loss_ce_5: 0.0009, decode.aux_loss_depth_5: 0.2359, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.3125, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.4706, loss: 0.8268, grad_norm: 3.3789
2022-01-11 15:12:39,074 - depth - INFO - Iter [23900/38400]	lr: 5.620e-05, eta: 2:34:55, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1158, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2317, decode.loss_ce: 0.0004, decode.ce_acc_level_0: 99.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4629, loss: 0.8113, grad_norm: 3.2256
2022-01-11 15:13:38,724 - depth - INFO - Saving checkpoint at 24000 iterations
2022-01-11 15:13:52,328 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 15:13:52,328 - depth - INFO - Iter [24000/38400]	lr: 5.562e-05, eta: 2:33:56, time: 0.733, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1153, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2307, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.0625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4605, loss: 0.8085, grad_norm: 3.3189
2022-01-11 15:14:13,643 - depth - INFO - Summary:
2022-01-11 15:14:13,643 - depth - INFO - 
+--------+-------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2  |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+-------+--------+---------+--------+--------+----------+--------+--------+
| 0.9237 | 0.989 | 0.9974 |  0.0949 | 0.3352 | 0.0401 |  0.1188  | 9.4369 | 0.0479 |
+--------+-------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 15:14:13,644 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 15:14:13,644 - depth - INFO - Iter(val) [82]	a1: 0.9237, a2: 0.9890, a3: 0.9974, abs_rel: 0.09488396346569061, rmse: 0.33515819907188416, log_10: 0.04008964076638222, rmse_log: 0.11880620568990707, silog: 9.4369, sq_rel: 0.047939132899045944
2022-01-11 15:15:13,435 - depth - INFO - Iter [24100/38400]	lr: 5.504e-05, eta: 2:33:02, time: 0.811, data_time: 0.221, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1159, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2319, decode.loss_ce: 0.0003, decode.ce_acc_level_0: 99.0625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4627, loss: 0.8114, grad_norm: 2.8502
2022-01-11 15:16:13,037 - depth - INFO - Iter [24200/38400]	lr: 5.445e-05, eta: 2:31:55, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1151, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2302, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4597, loss: 0.8065, grad_norm: 3.2998
2022-01-11 15:17:13,084 - depth - INFO - Iter [24300/38400]	lr: 5.387e-05, eta: 2:30:49, time: 0.600, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1131, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2260, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 97.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4509, loss: 0.7916, grad_norm: 3.1597
2022-01-11 15:18:12,656 - depth - INFO - Iter [24400/38400]	lr: 5.329e-05, eta: 2:29:42, time: 0.596, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1141, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2283, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4559, loss: 0.7993, grad_norm: 2.8550
2022-01-11 15:19:12,092 - depth - INFO - Iter [24500/38400]	lr: 5.271e-05, eta: 2:28:35, time: 0.595, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1150, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2299, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.8125, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.4597, loss: 0.8058, grad_norm: 3.0569
2022-01-11 15:20:11,730 - depth - INFO - Iter [24600/38400]	lr: 5.212e-05, eta: 2:27:28, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1127, decode.aux_loss_ce_5: 0.0007, decode.aux_loss_depth_5: 0.2249, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.5000, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.4491, loss: 0.7886, grad_norm: 2.8130
2022-01-11 15:21:11,023 - depth - INFO - Iter [24700/38400]	lr: 5.154e-05, eta: 2:26:21, time: 0.593, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1117, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2237, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4467, loss: 0.7834, grad_norm: 2.8336
2022-01-11 15:22:10,271 - depth - INFO - Iter [24800/38400]	lr: 5.095e-05, eta: 2:25:15, time: 0.593, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1138, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2276, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.5000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4546, loss: 0.7972, grad_norm: 3.0305
2022-01-11 15:22:31,874 - depth - INFO - Summary:
2022-01-11 15:22:31,875 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9227 | 0.9882 | 0.9972 |  0.0977 | 0.3372 | 0.0409 |  0.121   | 9.6209 | 0.0495 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 15:22:31,877 - depth - INFO - Iter(val) [82]	a1: 0.9227, a2: 0.9882, a3: 0.9972, abs_rel: 0.09769396483898163, rmse: 0.33722251653671265, log_10: 0.04094883054494858, rmse_log: 0.12101467698812485, silog: 9.6209, sq_rel: 0.049471672624349594
2022-01-11 15:23:31,672 - depth - INFO - Iter [24900/38400]	lr: 5.037e-05, eta: 2:24:20, time: 0.814, data_time: 0.224, memory: 30431, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1137, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2272, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 97.5000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4539, loss: 0.7966, grad_norm: 2.9390
2022-01-11 15:24:31,497 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 15:24:31,497 - depth - INFO - Iter [25000/38400]	lr: 4.979e-05, eta: 2:23:14, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1122, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2243, decode.loss_ce: 0.0004, decode.ce_acc_level_0: 98.5625, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.4471, loss: 0.7845, grad_norm: 2.9837
2022-01-11 15:25:31,080 - depth - INFO - Iter [25100/38400]	lr: 4.920e-05, eta: 2:22:07, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1095, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2192, decode.loss_ce: 0.0004, decode.ce_acc_level_0: 98.7500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4372, loss: 0.7669, grad_norm: 3.3399
2022-01-11 15:26:30,541 - depth - INFO - Iter [25200/38400]	lr: 4.862e-05, eta: 2:21:00, time: 0.595, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1115, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2226, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.2500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4446, loss: 0.7801, grad_norm: 2.8035
2022-01-11 15:27:29,965 - depth - INFO - Iter [25300/38400]	lr: 4.803e-05, eta: 2:19:54, time: 0.594, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1102, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2205, decode.loss_ce: 0.0003, decode.ce_acc_level_0: 98.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4404, loss: 0.7721, grad_norm: 3.2771
2022-01-11 15:28:30,011 - depth - INFO - Iter [25400/38400]	lr: 4.745e-05, eta: 2:18:48, time: 0.600, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1084, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2171, decode.loss_ce: 0.0003, decode.ce_acc_level_0: 99.3125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4326, loss: 0.7588, grad_norm: 3.0144
2022-01-11 15:29:29,588 - depth - INFO - Iter [25500/38400]	lr: 4.687e-05, eta: 2:17:41, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1097, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2191, decode.loss_ce: 0.0004, decode.ce_acc_level_0: 99.0625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4376, loss: 0.7674, grad_norm: 2.9887
2022-01-11 15:30:29,186 - depth - INFO - Saving checkpoint at 25600 iterations
2022-01-11 15:30:45,384 - depth - INFO - Iter [25600/38400]	lr: 4.628e-05, eta: 2:16:43, time: 0.758, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1117, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2233, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4457, loss: 0.7825, grad_norm: 2.7961
2022-01-11 15:31:06,768 - depth - INFO - Summary:
2022-01-11 15:31:06,769 - depth - INFO - 
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
| 0.9182 | 0.9882 | 0.9972 |  0.1008 | 0.338 | 0.0417 |  0.1227  | 9.5843 | 0.0507 |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
2022-01-11 15:31:06,769 - depth - INFO - Iter(val) [82]	a1: 0.9182, a2: 0.9882, a3: 0.9972, abs_rel: 0.10075194388628006, rmse: 0.33800116181373596, log_10: 0.04174697771668434, rmse_log: 0.12272816896438599, silog: 9.5843, sq_rel: 0.05065198242664337
2022-01-11 15:32:06,646 - depth - INFO - Iter [25700/38400]	lr: 4.570e-05, eta: 2:15:48, time: 0.812, data_time: 0.221, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1103, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2206, decode.loss_ce: 0.0003, decode.ce_acc_level_0: 98.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4405, loss: 0.7722, grad_norm: 2.9327
2022-01-11 15:33:06,477 - depth - INFO - Iter [25800/38400]	lr: 4.512e-05, eta: 2:14:41, time: 0.599, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1094, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2188, decode.loss_ce: 0.0002, decode.ce_acc_level_0: 99.3750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4368, loss: 0.7657, grad_norm: 2.8131
2022-01-11 15:34:06,036 - depth - INFO - Iter [25900/38400]	lr: 4.454e-05, eta: 2:13:35, time: 0.595, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1073, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2142, decode.loss_ce: 0.0003, decode.ce_acc_level_0: 99.3750, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.4278, loss: 0.7500, grad_norm: 2.6795
2022-01-11 15:35:06,399 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 15:35:06,400 - depth - INFO - Iter [26000/38400]	lr: 4.396e-05, eta: 2:12:29, time: 0.604, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1078, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2157, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.2500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4304, loss: 0.7552, grad_norm: 2.6207
2022-01-11 15:36:06,218 - depth - INFO - Iter [26100/38400]	lr: 4.338e-05, eta: 2:11:23, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1064, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2127, decode.loss_ce: 0.0004, decode.ce_acc_level_0: 98.4375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4251, loss: 0.7453, grad_norm: 2.8033
2022-01-11 15:37:06,025 - depth - INFO - Iter [26200/38400]	lr: 4.280e-05, eta: 2:10:17, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1084, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2170, decode.loss_ce: 0.0004, decode.ce_acc_level_0: 98.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4327, loss: 0.7593, grad_norm: 2.8277
2022-01-11 15:38:05,731 - depth - INFO - Iter [26300/38400]	lr: 4.222e-05, eta: 2:09:11, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1083, decode.aux_loss_ce_5: 0.0002, decode.aux_loss_depth_5: 0.2164, decode.loss_ce: 0.0002, decode.ce_acc_level_0: 99.3750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4324, loss: 0.7579, grad_norm: 3.2209
2022-01-11 15:39:05,535 - depth - INFO - Iter [26400/38400]	lr: 4.165e-05, eta: 2:08:05, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1066, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2132, decode.loss_ce: 0.0003, decode.ce_acc_level_0: 98.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4258, loss: 0.7465, grad_norm: 2.8148
2022-01-11 15:39:27,162 - depth - INFO - Summary:
2022-01-11 15:39:27,162 - depth - INFO - 
+--------+------+--------+---------+--------+--------+----------+-------+--------+
|   a1   |  a2  |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog | sq_rel |
+--------+------+--------+---------+--------+--------+----------+-------+--------+
| 0.9244 | 0.99 | 0.9977 |  0.0943 | 0.3309 | 0.0399 |  0.1181  | 9.364 | 0.0462 |
+--------+------+--------+---------+--------+--------+----------+-------+--------+
2022-01-11 15:39:44,806 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_26400.pth.
2022-01-11 15:39:44,807 - depth - INFO - Best abs_rel is 0.0943 at 26400 iter.
2022-01-11 15:39:44,809 - depth - INFO - Iter(val) [82]	a1: 0.9244, a2: 0.9900, a3: 0.9977, abs_rel: 0.09429365396499634, rmse: 0.33092001080513, log_10: 0.039896946400403976, rmse_log: 0.11813515424728394, silog: 9.3640, sq_rel: 0.046176180243492126
2022-01-11 15:40:44,848 - depth - INFO - Iter [26500/38400]	lr: 4.107e-05, eta: 2:07:17, time: 0.993, data_time: 0.402, memory: 30431, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1079, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2157, decode.loss_ce: 0.0003, decode.ce_acc_level_0: 99.0625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4302, loss: 0.7547, grad_norm: 2.5962
2022-01-11 15:41:44,124 - depth - INFO - Iter [26600/38400]	lr: 4.050e-05, eta: 2:06:10, time: 0.593, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1057, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2113, decode.loss_ce: 0.0002, decode.ce_acc_level_0: 99.0625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4222, loss: 0.7400, grad_norm: 2.6227
2022-01-11 15:42:43,684 - depth - INFO - Iter [26700/38400]	lr: 3.993e-05, eta: 2:05:04, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1071, decode.aux_loss_ce_5: 0.0002, decode.aux_loss_depth_5: 0.2139, decode.loss_ce: 0.0003, decode.ce_acc_level_0: 99.1250, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4272, loss: 0.7489, grad_norm: 2.5376
2022-01-11 15:43:43,452 - depth - INFO - Iter [26800/38400]	lr: 3.935e-05, eta: 2:03:58, time: 0.597, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1068, decode.aux_loss_ce_5: 0.0002, decode.aux_loss_depth_5: 0.2135, decode.loss_ce: 0.0002, decode.ce_acc_level_0: 99.4375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4262, loss: 0.7472, grad_norm: 2.6618
2022-01-11 15:44:43,503 - depth - INFO - Iter [26900/38400]	lr: 3.878e-05, eta: 2:02:52, time: 0.601, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1069, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2132, decode.loss_ce: 0.0003, decode.ce_acc_level_0: 99.0625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4260, loss: 0.7470, grad_norm: 2.7155
2022-01-11 15:45:43,007 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 15:45:43,007 - depth - INFO - Iter [27000/38400]	lr: 3.822e-05, eta: 2:01:46, time: 0.595, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1065, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2128, decode.loss_ce: 0.0004, decode.ce_acc_level_0: 98.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4247, loss: 0.7451, grad_norm: 2.6680
2022-01-11 15:46:42,843 - depth - INFO - Iter [27100/38400]	lr: 3.765e-05, eta: 2:00:40, time: 0.599, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.1062, decode.aux_loss_ce_5: 0.0002, decode.aux_loss_depth_5: 0.2125, decode.loss_ce: 0.0002, decode.ce_acc_level_0: 99.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4242, loss: 0.7434, grad_norm: 2.6306
2022-01-11 15:47:42,949 - depth - INFO - Saving checkpoint at 27200 iterations
2022-01-11 15:48:00,011 - depth - INFO - Iter [27200/38400]	lr: 3.708e-05, eta: 1:59:42, time: 0.772, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1043, decode.aux_loss_ce_5: 0.0002, decode.aux_loss_depth_5: 0.2083, decode.loss_ce: 0.0002, decode.ce_acc_level_0: 99.2500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4161, loss: 0.7293, grad_norm: 2.5546
2022-01-11 15:48:21,447 - depth - INFO - Summary:
2022-01-11 15:48:21,448 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9205 | 0.9883 | 0.9968 |  0.096  | 0.3353 | 0.0404 |   0.12   | 9.5632 | 0.0493 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 15:48:21,449 - depth - INFO - Iter(val) [82]	a1: 0.9205, a2: 0.9883, a3: 0.9968, abs_rel: 0.09604393690824509, rmse: 0.3353087306022644, log_10: 0.040373776108026505, rmse_log: 0.11999961733818054, silog: 9.5632, sq_rel: 0.04930770397186279
2022-01-11 15:49:21,963 - depth - INFO - Iter [27300/38400]	lr: 3.652e-05, eta: 1:58:45, time: 0.820, data_time: 0.224, memory: 30431, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1041, decode.aux_loss_ce_5: 0.0002, decode.aux_loss_depth_5: 0.2079, decode.loss_ce: 0.0003, decode.ce_acc_level_0: 99.3125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4154, loss: 0.7281, grad_norm: 2.8253
2022-01-11 15:50:21,936 - depth - INFO - Iter [27400/38400]	lr: 3.596e-05, eta: 1:57:39, time: 0.599, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1030, decode.aux_loss_ce_5: 0.0002, decode.aux_loss_depth_5: 0.2060, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.6250, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4114, loss: 0.7209, grad_norm: 2.5387
2022-01-11 15:51:21,832 - depth - INFO - Iter [27500/38400]	lr: 3.540e-05, eta: 1:56:33, time: 0.599, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.1033, decode.aux_loss_ce_5: 0.0002, decode.aux_loss_depth_5: 0.2064, decode.loss_ce: 0.0002, decode.ce_acc_level_0: 99.2500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4123, loss: 0.7225, grad_norm: 2.7874
2022-01-11 15:52:21,987 - depth - INFO - Iter [27600/38400]	lr: 3.484e-05, eta: 1:55:27, time: 0.601, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.1042, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2083, decode.loss_ce: 0.0002, decode.ce_acc_level_0: 99.3750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4161, loss: 0.7293, grad_norm: 2.7061
2022-01-11 15:53:22,074 - depth - INFO - Iter [27700/38400]	lr: 3.429e-05, eta: 1:54:21, time: 0.602, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.1023, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.2045, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.7500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4083, loss: 0.7154, grad_norm: 2.4787
2022-01-11 15:54:22,221 - depth - INFO - Iter [27800/38400]	lr: 3.373e-05, eta: 1:53:16, time: 0.601, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.1022, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.2041, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4076, loss: 0.7141, grad_norm: 2.9171
2022-01-11 15:55:21,321 - depth - INFO - Iter [27900/38400]	lr: 3.318e-05, eta: 1:52:10, time: 0.591, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1021, decode.aux_loss_ce_5: 0.0002, decode.aux_loss_depth_5: 0.2042, decode.loss_ce: 0.0002, decode.ce_acc_level_0: 99.3750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4077, loss: 0.7144, grad_norm: 2.3699
2022-01-11 15:56:21,111 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 15:56:21,112 - depth - INFO - Iter [28000/38400]	lr: 3.263e-05, eta: 1:51:04, time: 0.598, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.1007, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.2016, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4025, loss: 0.7051, grad_norm: 2.5843
2022-01-11 15:56:42,814 - depth - INFO - Summary:
2022-01-11 15:56:42,816 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9197 | 0.9892 | 0.9974 |  0.0943 | 0.3351 | 0.0401 |  0.119   | 9.4959 | 0.0473 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 15:56:42,818 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 15:56:42,818 - depth - INFO - Iter(val) [82]	a1: 0.9197, a2: 0.9892, a3: 0.9974, abs_rel: 0.09431619942188263, rmse: 0.33505383133888245, log_10: 0.040137410163879395, rmse_log: 0.11895812302827835, silog: 9.4959, sq_rel: 0.04728839173913002
2022-01-11 15:57:42,791 - depth - INFO - Iter [28100/38400]	lr: 3.209e-05, eta: 1:50:07, time: 0.817, data_time: 0.225, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1037, decode.aux_loss_ce_5: 0.0002, decode.aux_loss_depth_5: 0.2072, decode.loss_ce: 0.0002, decode.ce_acc_level_0: 99.3125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4140, loss: 0.7255, grad_norm: 2.4637
2022-01-11 15:58:42,388 - depth - INFO - Iter [28200/38400]	lr: 3.154e-05, eta: 1:49:01, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1002, decode.aux_loss_ce_5: 0.0002, decode.aux_loss_depth_5: 0.2005, decode.loss_ce: 0.0002, decode.ce_acc_level_0: 99.3750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3999, loss: 0.7011, grad_norm: 2.3408
2022-01-11 15:59:41,928 - depth - INFO - Iter [28300/38400]	lr: 3.100e-05, eta: 1:47:55, time: 0.595, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.1024, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.2045, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.6875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4083, loss: 0.7156, grad_norm: 2.5355
2022-01-11 16:00:41,936 - depth - INFO - Iter [28400/38400]	lr: 3.046e-05, eta: 1:46:49, time: 0.600, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1015, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.2029, decode.loss_ce: 0.0002, decode.ce_acc_level_0: 99.2500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4054, loss: 0.7103, grad_norm: 2.5382
2022-01-11 16:01:41,877 - depth - INFO - Iter [28500/38400]	lr: 2.993e-05, eta: 1:45:44, time: 0.600, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.1025, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.2048, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.6250, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4092, loss: 0.7170, grad_norm: 2.4630
2022-01-11 16:02:41,243 - depth - INFO - Iter [28600/38400]	lr: 2.939e-05, eta: 1:44:38, time: 0.594, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1018, decode.aux_loss_ce_5: 0.0002, decode.aux_loss_depth_5: 0.2037, decode.loss_ce: 0.0002, decode.ce_acc_level_0: 99.5000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4066, loss: 0.7127, grad_norm: 2.4939
2022-01-11 16:03:40,969 - depth - INFO - Iter [28700/38400]	lr: 2.886e-05, eta: 1:43:33, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.1022, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.2043, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.6875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4079, loss: 0.7147, grad_norm: 2.9206
2022-01-11 16:04:41,304 - depth - INFO - Saving checkpoint at 28800 iterations
2022-01-11 16:04:57,568 - depth - INFO - Iter [28800/38400]	lr: 2.833e-05, eta: 1:42:33, time: 0.766, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.1010, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.2020, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.6875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4035, loss: 0.7068, grad_norm: 2.4181
2022-01-11 16:05:18,874 - depth - INFO - Summary:
2022-01-11 16:05:18,874 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9163 | 0.9875 | 0.9971 |  0.0978 | 0.3349 | 0.0409 |  0.1213  | 9.5999 | 0.0496 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 16:05:18,875 - depth - INFO - Iter(val) [82]	a1: 0.9163, a2: 0.9875, a3: 0.9971, abs_rel: 0.09780832380056381, rmse: 0.33491209149360657, log_10: 0.04087911918759346, rmse_log: 0.12131593376398087, silog: 9.5999, sq_rel: 0.049576032906770706
2022-01-11 16:06:18,582 - depth - INFO - Iter [28900/38400]	lr: 2.781e-05, eta: 1:41:34, time: 0.810, data_time: 0.220, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0995, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1989, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3974, loss: 0.6962, grad_norm: 2.4182
2022-01-11 16:07:18,376 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 16:07:18,376 - depth - INFO - Iter [29000/38400]	lr: 2.729e-05, eta: 1:40:29, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.1009, decode.aux_loss_ce_5: 0.0002, decode.aux_loss_depth_5: 0.2017, decode.loss_ce: 0.0002, decode.ce_acc_level_0: 99.3125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4029, loss: 0.7060, grad_norm: 2.5726
2022-01-11 16:08:17,852 - depth - INFO - Iter [29100/38400]	lr: 2.677e-05, eta: 1:39:23, time: 0.595, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0995, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1988, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3970, loss: 0.6955, grad_norm: 2.5366
2022-01-11 16:09:17,434 - depth - INFO - Iter [29200/38400]	lr: 2.625e-05, eta: 1:38:17, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.1004, decode.aux_loss_ce_5: 0.0002, decode.aux_loss_depth_5: 0.2009, decode.loss_ce: 0.0002, decode.ce_acc_level_0: 99.4375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4012, loss: 0.7030, grad_norm: 2.5085
2022-01-11 16:10:17,415 - depth - INFO - Iter [29300/38400]	lr: 2.574e-05, eta: 1:37:12, time: 0.600, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0988, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1976, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.6875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3941, loss: 0.6909, grad_norm: 2.4476
2022-01-11 16:11:16,948 - depth - INFO - Iter [29400/38400]	lr: 2.523e-05, eta: 1:36:07, time: 0.595, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.1011, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.2020, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.4035, loss: 0.7069, grad_norm: 2.5851
2022-01-11 16:12:16,563 - depth - INFO - Iter [29500/38400]	lr: 2.473e-05, eta: 1:35:01, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0988, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1974, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.6875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3943, loss: 0.6907, grad_norm: 2.5505
2022-01-11 16:13:16,911 - depth - INFO - Iter [29600/38400]	lr: 2.423e-05, eta: 1:33:56, time: 0.603, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0986, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1971, decode.loss_ce: 0.0002, decode.ce_acc_level_0: 99.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3937, loss: 0.6898, grad_norm: 2.3289
2022-01-11 16:13:38,552 - depth - INFO - Summary:
2022-01-11 16:13:38,554 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
| 0.9231 | 0.9889 | 0.9973 |  0.0937 | 0.3318 | 0.0397 |  0.1182  | 9.396 | 0.0476 |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
2022-01-11 16:13:54,189 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_29600.pth.
2022-01-11 16:13:54,189 - depth - INFO - Best abs_rel is 0.0937 at 29600 iter.
2022-01-11 16:13:54,190 - depth - INFO - Iter(val) [82]	a1: 0.9231, a2: 0.9889, a3: 0.9973, abs_rel: 0.09374883025884628, rmse: 0.33183687925338745, log_10: 0.03971785679459572, rmse_log: 0.1181781068444252, silog: 9.3960, sq_rel: 0.04762553423643112
2022-01-11 16:14:54,398 - depth - INFO - Iter [29700/38400]	lr: 2.373e-05, eta: 1:33:02, time: 0.975, data_time: 0.380, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0984, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1965, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3924, loss: 0.6876, grad_norm: 2.3509
2022-01-11 16:15:54,254 - depth - INFO - Iter [29800/38400]	lr: 2.323e-05, eta: 1:31:56, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0983, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1964, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.6875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3921, loss: 0.6871, grad_norm: 2.4814
2022-01-11 16:16:54,427 - depth - INFO - Iter [29900/38400]	lr: 2.274e-05, eta: 1:30:51, time: 0.602, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0968, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1936, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3867, loss: 0.6773, grad_norm: 2.3237
2022-01-11 16:17:54,386 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 16:17:54,387 - depth - INFO - Iter [30000/38400]	lr: 2.225e-05, eta: 1:29:46, time: 0.600, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0974, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1948, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3892, loss: 0.6816, grad_norm: 2.2923
2022-01-11 16:18:54,090 - depth - INFO - Iter [30100/38400]	lr: 2.177e-05, eta: 1:28:40, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0969, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1938, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.6875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3868, loss: 0.6778, grad_norm: 2.2281
2022-01-11 16:19:53,830 - depth - INFO - Iter [30200/38400]	lr: 2.129e-05, eta: 1:27:35, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0968, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1935, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.6875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3861, loss: 0.6766, grad_norm: 2.3758
2022-01-11 16:20:54,060 - depth - INFO - Iter [30300/38400]	lr: 2.081e-05, eta: 1:26:30, time: 0.602, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0991, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1982, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.7500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3959, loss: 0.6933, grad_norm: 2.3349
2022-01-11 16:21:54,213 - depth - INFO - Saving checkpoint at 30400 iterations
2022-01-11 16:22:09,483 - depth - INFO - Iter [30400/38400]	lr: 2.034e-05, eta: 1:25:29, time: 0.754, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0964, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1927, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.7500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3848, loss: 0.6743, grad_norm: 2.3526
2022-01-11 16:22:30,952 - depth - INFO - Summary:
2022-01-11 16:22:30,953 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9247 | 0.9892 | 0.9975 |  0.094  | 0.3315 | 0.0397 |  0.1178  | 9.3494 | 0.0471 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 16:22:30,955 - depth - INFO - Iter(val) [82]	a1: 0.9247, a2: 0.9892, a3: 0.9975, abs_rel: 0.0940495952963829, rmse: 0.3315011262893677, log_10: 0.039678994566202164, rmse_log: 0.11780627071857452, silog: 9.3494, sq_rel: 0.0471000075340271
2022-01-11 16:23:31,122 - depth - INFO - Iter [30500/38400]	lr: 1.987e-05, eta: 1:24:29, time: 0.816, data_time: 0.222, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0962, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1922, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.5000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3839, loss: 0.6727, grad_norm: 2.8322
2022-01-11 16:24:31,265 - depth - INFO - Iter [30600/38400]	lr: 1.941e-05, eta: 1:23:24, time: 0.602, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0991, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1980, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.7500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3957, loss: 0.6931, grad_norm: 2.1802
2022-01-11 16:25:30,890 - depth - INFO - Iter [30700/38400]	lr: 1.895e-05, eta: 1:22:19, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0951, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1903, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.7500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3800, loss: 0.6656, grad_norm: 2.3285
2022-01-11 16:26:30,492 - depth - INFO - Iter [30800/38400]	lr: 1.849e-05, eta: 1:21:14, time: 0.596, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0970, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1938, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3871, loss: 0.6780, grad_norm: 2.3247
2022-01-11 16:27:29,846 - depth - INFO - Iter [30900/38400]	lr: 1.804e-05, eta: 1:20:08, time: 0.593, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0952, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1903, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3799, loss: 0.6656, grad_norm: 2.2552
2022-01-11 16:28:29,918 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 16:28:29,918 - depth - INFO - Iter [31000/38400]	lr: 1.760e-05, eta: 1:19:03, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0965, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1929, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.6250, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3851, loss: 0.6749, grad_norm: 2.4084
2022-01-11 16:29:30,045 - depth - INFO - Iter [31100/38400]	lr: 1.715e-05, eta: 1:17:58, time: 0.602, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0944, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1885, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3766, loss: 0.6596, grad_norm: 2.2762
2022-01-11 16:30:30,305 - depth - INFO - Iter [31200/38400]	lr: 1.672e-05, eta: 1:16:53, time: 0.602, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0938, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1876, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3749, loss: 0.6565, grad_norm: 2.1833
2022-01-11 16:30:51,892 - depth - INFO - Summary:
2022-01-11 16:30:51,892 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9261 | 0.9891 | 0.9973 |  0.0943 | 0.3293 | 0.0396 |  0.1178  | 9.3414 | 0.0477 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 16:30:51,893 - depth - INFO - Iter(val) [82]	a1: 0.9261, a2: 0.9891, a3: 0.9973, abs_rel: 0.09428393095731735, rmse: 0.3293285071849823, log_10: 0.039635345339775085, rmse_log: 0.11784465610980988, silog: 9.3414, sq_rel: 0.04768596589565277
2022-01-11 16:31:51,964 - depth - INFO - Iter [31300/38400]	lr: 1.628e-05, eta: 1:15:53, time: 0.816, data_time: 0.223, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0946, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1893, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3781, loss: 0.6621, grad_norm: 2.4126
2022-01-11 16:32:51,908 - depth - INFO - Iter [31400/38400]	lr: 1.585e-05, eta: 1:14:48, time: 0.600, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0945, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1887, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3771, loss: 0.6604, grad_norm: 2.2181
2022-01-11 16:33:51,911 - depth - INFO - Iter [31500/38400]	lr: 1.543e-05, eta: 1:13:43, time: 0.600, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0953, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1907, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3806, loss: 0.6668, grad_norm: 2.2227
2022-01-11 16:34:51,752 - depth - INFO - Iter [31600/38400]	lr: 1.501e-05, eta: 1:12:38, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0930, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1860, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3714, loss: 0.6506, grad_norm: 2.1164
2022-01-11 16:35:51,865 - depth - INFO - Iter [31700/38400]	lr: 1.460e-05, eta: 1:11:33, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0964, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1927, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3848, loss: 0.6739, grad_norm: 2.4656
2022-01-11 16:36:52,361 - depth - INFO - Iter [31800/38400]	lr: 1.419e-05, eta: 1:10:28, time: 0.605, data_time: 0.010, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0960, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1920, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.7500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3832, loss: 0.6714, grad_norm: 2.1475
2022-01-11 16:37:52,240 - depth - INFO - Iter [31900/38400]	lr: 1.378e-05, eta: 1:09:23, time: 0.599, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0922, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1843, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3682, loss: 0.6448, grad_norm: 2.1452
2022-01-11 16:38:51,825 - depth - INFO - Saving checkpoint at 32000 iterations
2022-01-11 16:39:05,969 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 16:39:05,969 - depth - INFO - Iter [32000/38400]	lr: 1.338e-05, eta: 1:08:21, time: 0.737, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0949, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1896, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3788, loss: 0.6633, grad_norm: 2.4677
2022-01-11 16:39:27,371 - depth - INFO - Summary:
2022-01-11 16:39:27,372 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9242 | 0.9893 | 0.9973 |  0.0939 | 0.3323 | 0.0397 |  0.118   | 9.3886 | 0.0476 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 16:39:27,373 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 16:39:27,373 - depth - INFO - Iter(val) [82]	a1: 0.9242, a2: 0.9893, a3: 0.9973, abs_rel: 0.09387526661157608, rmse: 0.33229687809944153, log_10: 0.03969789668917656, rmse_log: 0.11801106482744217, silog: 9.3886, sq_rel: 0.04758613556623459
2022-01-11 16:40:27,218 - depth - INFO - Iter [32100/38400]	lr: 1.299e-05, eta: 1:07:20, time: 0.812, data_time: 0.222, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0964, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1927, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3849, loss: 0.6741, grad_norm: 2.7871
2022-01-11 16:41:27,227 - depth - INFO - Iter [32200/38400]	lr: 1.260e-05, eta: 1:06:15, time: 0.600, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0934, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1867, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3730, loss: 0.6532, grad_norm: 2.2582
2022-01-11 16:42:27,571 - depth - INFO - Iter [32300/38400]	lr: 1.221e-05, eta: 1:05:11, time: 0.603, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0925, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1848, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3692, loss: 0.6465, grad_norm: 2.2727
2022-01-11 16:43:27,700 - depth - INFO - Iter [32400/38400]	lr: 1.183e-05, eta: 1:04:06, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0927, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1854, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3703, loss: 0.6486, grad_norm: 2.1640
2022-01-11 16:44:27,865 - depth - INFO - Iter [32500/38400]	lr: 1.146e-05, eta: 1:03:01, time: 0.602, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0928, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1855, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3704, loss: 0.6489, grad_norm: 2.4274
2022-01-11 16:45:27,715 - depth - INFO - Iter [32600/38400]	lr: 1.109e-05, eta: 1:01:56, time: 0.598, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0944, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1887, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3770, loss: 0.6602, grad_norm: 2.4925
2022-01-11 16:46:27,544 - depth - INFO - Iter [32700/38400]	lr: 1.073e-05, eta: 1:00:51, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0926, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1852, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3698, loss: 0.6476, grad_norm: 2.3601
2022-01-11 16:47:27,772 - depth - INFO - Iter [32800/38400]	lr: 1.037e-05, eta: 0:59:47, time: 0.603, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0942, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1885, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3763, loss: 0.6591, grad_norm: 2.1870
2022-01-11 16:47:49,655 - depth - INFO - Summary:
2022-01-11 16:47:49,656 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9223 | 0.9889 | 0.9972 |  0.0953 | 0.3305 |  0.04  |  0.1189  | 9.4388 | 0.048  |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 16:47:49,656 - depth - INFO - Iter(val) [82]	a1: 0.9223, a2: 0.9889, a3: 0.9972, abs_rel: 0.09529655426740646, rmse: 0.33047839999198914, log_10: 0.03999755159020424, rmse_log: 0.11893504858016968, silog: 9.4388, sq_rel: 0.047967083752155304
2022-01-11 16:48:49,546 - depth - INFO - Iter [32900/38400]	lr: 1.002e-05, eta: 0:58:45, time: 0.817, data_time: 0.226, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0919, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1837, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.7500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3672, loss: 0.6429, grad_norm: 2.2543
2022-01-11 16:49:49,418 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 16:49:49,418 - depth - INFO - Iter [33000/38400]	lr: 9.668e-06, eta: 0:57:41, time: 0.599, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0942, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1882, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3759, loss: 0.6584, grad_norm: 2.2151
2022-01-11 16:50:49,663 - depth - INFO - Iter [33100/38400]	lr: 9.326e-06, eta: 0:56:36, time: 0.602, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0924, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1848, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3687, loss: 0.6460, grad_norm: 2.0323
2022-01-11 16:51:49,685 - depth - INFO - Iter [33200/38400]	lr: 8.990e-06, eta: 0:55:31, time: 0.601, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0929, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1856, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3706, loss: 0.6492, grad_norm: 2.1539
2022-01-11 16:52:49,538 - depth - INFO - Iter [33300/38400]	lr: 8.659e-06, eta: 0:54:26, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0912, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1823, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3642, loss: 0.6378, grad_norm: 2.1305
2022-01-11 16:53:49,961 - depth - INFO - Iter [33400/38400]	lr: 8.334e-06, eta: 0:53:22, time: 0.605, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0919, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1837, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3670, loss: 0.6426, grad_norm: 2.3473
2022-01-11 16:54:50,107 - depth - INFO - Iter [33500/38400]	lr: 8.014e-06, eta: 0:52:17, time: 0.602, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0938, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1874, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3745, loss: 0.6558, grad_norm: 2.0502
2022-01-11 16:55:50,367 - depth - INFO - Saving checkpoint at 33600 iterations
2022-01-11 16:56:01,381 - depth - INFO - Iter [33600/38400]	lr: 7.701e-06, eta: 0:51:14, time: 0.713, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0929, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1858, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3711, loss: 0.6500, grad_norm: 2.2199
2022-01-11 16:56:22,917 - depth - INFO - Summary:
2022-01-11 16:56:22,918 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9246 | 0.9892 | 0.9973 |  0.0937 | 0.3299 | 0.0396 |  0.1177  | 9.3657 | 0.047  |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 16:56:35,972 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_33600.pth.
2022-01-11 16:56:35,972 - depth - INFO - Best abs_rel is 0.0937 at 33600 iter.
2022-01-11 16:56:35,973 - depth - INFO - Iter(val) [82]	a1: 0.9246, a2: 0.9892, a3: 0.9973, abs_rel: 0.09370718896389008, rmse: 0.3298545181751251, log_10: 0.03956110402941704, rmse_log: 0.11774034053087234, silog: 9.3657, sq_rel: 0.04703506454825401
2022-01-11 16:57:35,867 - depth - INFO - Iter [33700/38400]	lr: 7.393e-06, eta: 0:50:14, time: 0.945, data_time: 0.353, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0909, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1818, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3629, loss: 0.6357, grad_norm: 2.0992
2022-01-11 16:58:35,766 - depth - INFO - Iter [33800/38400]	lr: 7.091e-06, eta: 0:49:10, time: 0.599, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0910, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1820, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3635, loss: 0.6366, grad_norm: 2.0563
2022-01-11 16:59:35,662 - depth - INFO - Iter [33900/38400]	lr: 6.795e-06, eta: 0:48:05, time: 0.599, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0910, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1820, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3635, loss: 0.6367, grad_norm: 2.3444
2022-01-11 17:00:35,228 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 17:00:35,228 - depth - INFO - Iter [34000/38400]	lr: 6.504e-06, eta: 0:47:00, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0892, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1785, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3564, loss: 0.6242, grad_norm: 1.9987
2022-01-11 17:01:34,780 - depth - INFO - Iter [34100/38400]	lr: 6.220e-06, eta: 0:45:56, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0924, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1848, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3692, loss: 0.6466, grad_norm: 2.5479
2022-01-11 17:02:34,414 - depth - INFO - Iter [34200/38400]	lr: 5.942e-06, eta: 0:44:51, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0912, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1823, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3644, loss: 0.6380, grad_norm: 2.4887
2022-01-11 17:03:34,608 - depth - INFO - Iter [34300/38400]	lr: 5.669e-06, eta: 0:43:46, time: 0.602, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0931, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1860, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3717, loss: 0.6508, grad_norm: 2.1994
2022-01-11 17:04:34,249 - depth - INFO - Iter [34400/38400]	lr: 5.403e-06, eta: 0:42:42, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0920, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1837, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3671, loss: 0.6429, grad_norm: 2.1219
2022-01-11 17:04:55,542 - depth - INFO - Summary:
2022-01-11 17:04:55,543 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
| 0.9257 | 0.9894 | 0.9972 |  0.0935 | 0.3293 | 0.0395 |  0.1175  | 9.341 | 0.0472 |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
2022-01-11 17:05:07,938 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_34400.pth.
2022-01-11 17:05:07,939 - depth - INFO - Best abs_rel is 0.0935 at 34400 iter.
2022-01-11 17:05:07,941 - depth - INFO - Iter(val) [82]	a1: 0.9257, a2: 0.9894, a3: 0.9972, abs_rel: 0.09351009130477905, rmse: 0.32929450273513794, log_10: 0.03945710137486458, rmse_log: 0.11749029904603958, silog: 9.3410, sq_rel: 0.04724011942744255
2022-01-11 17:06:07,320 - depth - INFO - Iter [34500/38400]	lr: 5.143e-06, eta: 0:41:41, time: 0.931, data_time: 0.345, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0910, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1819, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3635, loss: 0.6365, grad_norm: 1.9839
2022-01-11 17:07:07,166 - depth - INFO - Iter [34600/38400]	lr: 4.889e-06, eta: 0:40:36, time: 0.599, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0915, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1827, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3649, loss: 0.6391, grad_norm: 2.2604
2022-01-11 17:08:07,202 - depth - INFO - Iter [34700/38400]	lr: 4.641e-06, eta: 0:39:32, time: 0.600, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0916, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1830, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3656, loss: 0.6402, grad_norm: 2.0291
2022-01-11 17:09:06,799 - depth - INFO - Iter [34800/38400]	lr: 4.399e-06, eta: 0:38:27, time: 0.596, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0915, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1828, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3653, loss: 0.6397, grad_norm: 1.9682
2022-01-11 17:10:06,541 - depth - INFO - Iter [34900/38400]	lr: 4.164e-06, eta: 0:37:23, time: 0.598, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0893, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1787, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3571, loss: 0.6252, grad_norm: 2.1659
2022-01-11 17:11:06,177 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 17:11:06,177 - depth - INFO - Iter [35000/38400]	lr: 3.934e-06, eta: 0:36:18, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0923, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1846, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3686, loss: 0.6457, grad_norm: 1.9925
2022-01-11 17:12:05,648 - depth - INFO - Iter [35100/38400]	lr: 3.712e-06, eta: 0:35:14, time: 0.595, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0911, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1822, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3640, loss: 0.6374, grad_norm: 1.9693
2022-01-11 17:13:05,273 - depth - INFO - Saving checkpoint at 35200 iterations
2022-01-11 17:13:15,833 - depth - INFO - Iter [35200/38400]	lr: 3.495e-06, eta: 0:34:10, time: 0.702, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0919, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1836, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3669, loss: 0.6426, grad_norm: 2.0502
2022-01-11 17:13:37,341 - depth - INFO - Summary:
2022-01-11 17:13:37,349 - depth - INFO - 
+-------+--------+--------+---------+------+--------+----------+--------+--------+
|   a1  |   a2   |   a3   | abs_rel | rmse | log_10 | rmse_log | silog  | sq_rel |
+-------+--------+--------+---------+------+--------+----------+--------+--------+
| 0.924 | 0.9891 | 0.9973 |  0.0943 | 0.33 | 0.0397 |  0.1182  | 9.4027 | 0.0473 |
+-------+--------+--------+---------+------+--------+----------+--------+--------+
2022-01-11 17:13:37,352 - depth - INFO - Iter(val) [82]	a1: 0.9240, a2: 0.9891, a3: 0.9973, abs_rel: 0.09427327662706375, rmse: 0.32999855279922485, log_10: 0.03973456844687462, rmse_log: 0.11819971352815628, silog: 9.4027, sq_rel: 0.0473262295126915
2022-01-11 17:14:37,467 - depth - INFO - Iter [35300/38400]	lr: 3.285e-06, eta: 0:33:08, time: 0.816, data_time: 0.223, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0896, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1790, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3576, loss: 0.6262, grad_norm: 2.0219
2022-01-11 17:15:37,166 - depth - INFO - Iter [35400/38400]	lr: 3.081e-06, eta: 0:32:03, time: 0.597, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0906, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1810, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3617, loss: 0.6334, grad_norm: 2.0382
2022-01-11 17:16:36,835 - depth - INFO - Iter [35500/38400]	lr: 2.883e-06, eta: 0:30:59, time: 0.597, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0910, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1818, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3634, loss: 0.6363, grad_norm: 2.0189
2022-01-11 17:17:36,353 - depth - INFO - Iter [35600/38400]	lr: 2.692e-06, eta: 0:29:54, time: 0.595, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0915, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1828, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3652, loss: 0.6395, grad_norm: 1.9959
2022-01-11 17:18:36,481 - depth - INFO - Iter [35700/38400]	lr: 2.508e-06, eta: 0:28:50, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0880, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1758, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3513, loss: 0.6152, grad_norm: 1.9858
2022-01-11 17:19:36,289 - depth - INFO - Iter [35800/38400]	lr: 2.330e-06, eta: 0:27:45, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0904, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1806, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3609, loss: 0.6320, grad_norm: 2.0146
2022-01-11 17:20:36,129 - depth - INFO - Iter [35900/38400]	lr: 2.158e-06, eta: 0:26:41, time: 0.599, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0903, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1805, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3607, loss: 0.6315, grad_norm: 1.9823
2022-01-11 17:21:36,276 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 17:21:36,276 - depth - INFO - Iter [36000/38400]	lr: 1.993e-06, eta: 0:25:37, time: 0.601, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0895, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1788, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3573, loss: 0.6257, grad_norm: 1.9891
2022-01-11 17:21:57,585 - depth - INFO - Summary:
2022-01-11 17:21:57,586 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9245 | 0.9893 | 0.9973 |  0.094  | 0.3303 | 0.0397 |  0.1179  | 9.3688 | 0.0474 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 17:21:57,587 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 17:21:57,587 - depth - INFO - Iter(val) [82]	a1: 0.9245, a2: 0.9893, a3: 0.9973, abs_rel: 0.09399831295013428, rmse: 0.3302622139453888, log_10: 0.03965887054800987, rmse_log: 0.11787936836481094, silog: 9.3688, sq_rel: 0.04741400107741356
2022-01-11 17:22:57,209 - depth - INFO - Iter [36100/38400]	lr: 1.835e-06, eta: 0:24:34, time: 0.810, data_time: 0.221, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0918, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1835, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3665, loss: 0.6417, grad_norm: 2.0147
2022-01-11 17:23:57,294 - depth - INFO - Iter [36200/38400]	lr: 1.683e-06, eta: 0:23:29, time: 0.601, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0894, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1787, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3569, loss: 0.6252, grad_norm: 2.3343
2022-01-11 17:24:56,717 - depth - INFO - Iter [36300/38400]	lr: 1.538e-06, eta: 0:22:25, time: 0.594, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0899, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1796, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3590, loss: 0.6285, grad_norm: 2.0122
2022-01-11 17:25:56,144 - depth - INFO - Iter [36400/38400]	lr: 1.399e-06, eta: 0:21:21, time: 0.594, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0916, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1831, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3660, loss: 0.6408, grad_norm: 2.1746
2022-01-11 17:26:55,303 - depth - INFO - Iter [36500/38400]	lr: 1.267e-06, eta: 0:20:16, time: 0.592, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0882, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1763, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3523, loss: 0.6168, grad_norm: 2.0246
2022-01-11 17:27:54,626 - depth - INFO - Iter [36600/38400]	lr: 1.142e-06, eta: 0:19:12, time: 0.593, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0905, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1810, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3616, loss: 0.6332, grad_norm: 2.0393
2022-01-11 17:28:53,715 - depth - INFO - Iter [36700/38400]	lr: 1.023e-06, eta: 0:18:08, time: 0.591, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0890, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1779, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3552, loss: 0.6223, grad_norm: 1.8490
2022-01-11 17:29:53,469 - depth - INFO - Saving checkpoint at 36800 iterations
2022-01-11 17:30:09,868 - depth - INFO - Iter [36800/38400]	lr: 9.113e-07, eta: 0:17:04, time: 0.762, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0889, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1776, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3550, loss: 0.6217, grad_norm: 2.1221
2022-01-11 17:30:31,346 - depth - INFO - Summary:
2022-01-11 17:30:31,347 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9249 | 0.9894 | 0.9973 |  0.0936 | 0.3294 | 0.0395 |  0.1176  | 9.3382 | 0.0472 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 17:30:31,348 - depth - INFO - Iter(val) [82]	a1: 0.9249, a2: 0.9894, a3: 0.9973, abs_rel: 0.09364518523216248, rmse: 0.32943257689476013, log_10: 0.03951096534729004, rmse_log: 0.11755185574293137, silog: 9.3382, sq_rel: 0.04723145067691803
2022-01-11 17:31:30,990 - depth - INFO - Iter [36900/38400]	lr: 8.061e-07, eta: 0:16:01, time: 0.811, data_time: 0.222, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0908, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1816, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3627, loss: 0.6351, grad_norm: 2.0265
2022-01-11 17:32:30,560 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 17:32:30,560 - depth - INFO - Iter [37000/38400]	lr: 7.076e-07, eta: 0:14:57, time: 0.595, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0889, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1776, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3550, loss: 0.6216, grad_norm: 2.0076
2022-01-11 17:33:30,292 - depth - INFO - Iter [37100/38400]	lr: 6.158e-07, eta: 0:13:52, time: 0.598, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0904, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1806, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3611, loss: 0.6322, grad_norm: 2.0087
2022-01-11 17:34:29,992 - depth - INFO - Iter [37200/38400]	lr: 5.307e-07, eta: 0:12:48, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0886, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1772, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3538, loss: 0.6196, grad_norm: 1.9101
2022-01-11 17:35:29,445 - depth - INFO - Iter [37300/38400]	lr: 4.525e-07, eta: 0:11:44, time: 0.595, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0902, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1803, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3602, loss: 0.6307, grad_norm: 2.0241
2022-01-11 17:36:28,923 - depth - INFO - Iter [37400/38400]	lr: 3.810e-07, eta: 0:10:40, time: 0.594, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0911, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1820, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3638, loss: 0.6370, grad_norm: 1.9625
2022-01-11 17:37:28,643 - depth - INFO - Iter [37500/38400]	lr: 3.162e-07, eta: 0:09:36, time: 0.597, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0908, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1815, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3626, loss: 0.6349, grad_norm: 2.1486
2022-01-11 17:38:28,463 - depth - INFO - Iter [37600/38400]	lr: 2.583e-07, eta: 0:08:32, time: 0.598, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0924, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1847, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3691, loss: 0.6462, grad_norm: 1.9518
2022-01-11 17:38:49,829 - depth - INFO - Summary:
2022-01-11 17:38:49,830 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9245 | 0.9893 | 0.9973 |  0.094  | 0.3294 | 0.0396 |  0.1178  | 9.3535 | 0.0473 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 17:38:49,832 - depth - INFO - Iter(val) [82]	a1: 0.9245, a2: 0.9893, a3: 0.9973, abs_rel: 0.09401784837245941, rmse: 0.3294479548931122, log_10: 0.039637934416532516, rmse_log: 0.11782953888177872, silog: 9.3535, sq_rel: 0.047319017350673676
2022-01-11 17:39:48,808 - depth - INFO - Iter [37700/38400]	lr: 2.072e-07, eta: 0:07:28, time: 0.804, data_time: 0.222, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0895, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1789, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3575, loss: 0.6260, grad_norm: 1.9191
2022-01-11 17:40:48,485 - depth - INFO - Iter [37800/38400]	lr: 1.628e-07, eta: 0:06:24, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0927, decode.aux_loss_ce_5: 0.0001, decode.aux_loss_depth_5: 0.1853, decode.loss_ce: 0.0001, decode.ce_acc_level_0: 99.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3702, loss: 0.6484, grad_norm: 2.0500
2022-01-11 17:41:48,067 - depth - INFO - Iter [37900/38400]	lr: 1.253e-07, eta: 0:05:20, time: 0.596, data_time: 0.009, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0888, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1774, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3547, loss: 0.6210, grad_norm: 1.8262
2022-01-11 17:42:47,787 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swinl_22k_nyu.py
2022-01-11 17:42:47,788 - depth - INFO - Iter [38000/38400]	lr: 9.461e-08, eta: 0:04:16, time: 0.597, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0001, decode.aux_loss_depth_2: 0.0902, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1802, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3601, loss: 0.6306, grad_norm: 1.8890
2022-01-11 17:43:47,290 - depth - INFO - Iter [38100/38400]	lr: 7.072e-08, eta: 0:03:12, time: 0.595, data_time: 0.006, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0897, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1794, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 100.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3584, loss: 0.6276, grad_norm: 2.0961
2022-01-11 17:44:46,869 - depth - INFO - Iter [38200/38400]	lr: 5.365e-08, eta: 0:02:08, time: 0.596, data_time: 0.007, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0887, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1773, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3541, loss: 0.6201, grad_norm: 1.8927
2022-01-11 17:45:46,253 - depth - INFO - Iter [38300/38400]	lr: 4.341e-08, eta: 0:01:03, time: 0.594, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0878, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1757, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3509, loss: 0.6144, grad_norm: 1.9689
2022-01-11 17:46:45,828 - depth - INFO - Saving checkpoint at 38400 iterations
2022-01-11 17:46:59,932 - depth - INFO - Iter [38400/38400]	lr: 4.000e-08, eta: 0:00:00, time: 0.737, data_time: 0.008, memory: 30431, decode.aux_loss_ce_2: 0.0000, decode.aux_loss_depth_2: 0.0913, decode.aux_loss_ce_5: 0.0000, decode.aux_loss_depth_5: 0.1825, decode.loss_ce: 0.0000, decode.ce_acc_level_0: 99.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.3647, loss: 0.6386, grad_norm: 2.2336
2022-01-11 17:47:21,091 - depth - INFO - Summary:
2022-01-11 17:47:21,092 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9248 | 0.9894 | 0.9973 |  0.0935 | 0.3297 | 0.0395 |  0.1175  | 9.3424 | 0.0471 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-11 17:47:35,043 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_38400.pth.
2022-01-11 17:47:35,043 - depth - INFO - Best abs_rel is 0.0935 at 38400 iter.
2022-01-11 17:47:35,044 - depth - INFO - Iter(val) [82]	a1: 0.9248, a2: 0.9894, a3: 0.9973, abs_rel: 0.0934920459985733, rmse: 0.3297230005264282, log_10: 0.03952482342720032, rmse_log: 0.11754993349313736, silog: 9.3424, sq_rel: 0.04711614549160004
