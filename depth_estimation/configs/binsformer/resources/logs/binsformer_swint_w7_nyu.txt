2022-01-10 16:43:34,469 - depth - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.11 (default, Jul 27 2021, 14:32:16) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: Tesla V100-SXM2-32GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.1, V10.1.243
GCC: gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
PyTorch: 1.8.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.9.0
OpenCV: 4.5.5
MMCV: 1.3.13
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.1
Depth: 0.0.0+0193246
------------------------------------------------------------

2022-01-10 16:43:34,470 - depth - INFO - Distributed training: True
2022-01-10 16:43:34,680 - depth - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
backbone_norm_cfg = dict(type='LN', requires_grad=True)
model = dict(
    type='DepthEncoderDecoder',
    pretrained='./nfs/checkpoints/swin_tiny_patch4_window7_224.pth',
    backbone=dict(
        type='SwinTransformer',
        pretrain_img_size=224,
        embed_dims=96,
        patch_size=4,
        window_size=7,
        mlp_ratio=4,
        depths=[2, 2, 6, 2],
        num_heads=[3, 6, 12, 24],
        strides=(4, 2, 2, 2),
        out_indices=(0, 1, 2, 3),
        qkv_bias=True,
        qk_scale=None,
        patch_norm=True,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.3,
        use_abs_pos_embed=False,
        act_cfg=dict(type='GELU'),
        norm_cfg=dict(type='LN', requires_grad=True),
        pretrain_style='official'),
    decode_head=dict(
        type='BinsFormerDecodeHead',
        in_channels=[96, 192, 384, 768],
        up_sample_channels=[768, 384, 192, 96],
        channels=256,
        align_corners=True,
        loss_decode=dict(type='SigLoss', valid_mask=True, loss_weight=10),
        fpn=True,
        uncertainty=False,
        class_num=25,
        min_depth=0.001,
        max_depth=10,
        n_bins=64,
        index=[0, 1, 2, 3],
        trans_index=[1, 2, 3],
        chamfer_loss=False,
        loss_chamfer=dict(type='BinsChamferLoss', loss_weight=0.1),
        classify=True,
        loss_class=dict(type='CrossEntropyLoss', loss_weight=0.01),
        norm_cfg=dict(type='BN', requires_grad=True),
        transformer_encoder=dict(
            type='PureMSDEnTransformer',
            num_feature_levels=3,
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_levels=3,
                        num_points=8),
                    feedforward_channels=1024,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding', num_feats=128, normalize=True),
        transformer_decoder=dict(
            type='PixelMSDeTransformer',
            decoder=dict(
                type='PixelTransformerDecoder',
                return_intermediate=True,
                num_layers=9,
                num_feature_levels=3,
                hidden_dim=256,
                transformerlayers=dict(
                    type='PixelTransformerDecoderLayer',
                    attn_cfgs=dict(
                        type='MultiheadAttention',
                        embed_dims=256,
                        num_heads=8,
                        dropout=0.0),
                    ffn_cfgs=dict(feedforward_channels=2048, ffn_drop=0.0),
                    operation_order=('cross_attn', 'norm', 'self_attn', 'norm',
                                     'ffn', 'norm'))))),
    train_cfg=dict(aux_loss=True, aux_index=[2, 5], aux_weight=[0.25, 0.5]),
    test_cfg=dict(mode='whole'))
dataset_type = 'NYUBinFormerDataset'
data_root = 'data/nyu/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (416, 544)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='DepthLoadAnnotations'),
    dict(type='NYUCrop', depth=True),
    dict(type='RandomRotate', prob=0.5, degree=2.5),
    dict(type='RandomFlip', prob=0.5),
    dict(type='RandomCrop', crop_size=(416, 544)),
    dict(
        type='ColorAug',
        prob=0.5,
        gamma_range=[0.9, 1.1],
        brightness_range=[0.75, 1.25],
        color_range=[0.9, 1.1]),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'depth_gt', 'class_label'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(480, 640),
        flip=True,
        flip_direction='horizontal',
        transforms=[
            dict(type='RandomFlip', direction='horizontal'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='NYUBinFormerDataset',
        data_root='data/nyu/',
        depth_scale=1000,
        split='nyu_train.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='DepthLoadAnnotations'),
            dict(type='NYUCrop', depth=True),
            dict(type='RandomRotate', prob=0.5, degree=2.5),
            dict(type='RandomFlip', prob=0.5),
            dict(type='RandomCrop', crop_size=(416, 544)),
            dict(
                type='ColorAug',
                prob=0.5,
                gamma_range=[0.9, 1.1],
                brightness_range=[0.75, 1.25],
                color_range=[0.9, 1.1]),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'depth_gt', 'class_label'])
        ],
        garg_crop=False,
        eigen_crop=True,
        min_depth=0.001,
        max_depth=10),
    val=dict(
        type='NYUBinFormerDataset',
        data_root='data/nyu/',
        depth_scale=1000,
        split='nyu_test.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(480, 640),
                flip=True,
                flip_direction='horizontal',
                transforms=[
                    dict(type='RandomFlip', direction='horizontal'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        garg_crop=False,
        eigen_crop=True,
        min_depth=0.001,
        max_depth=10),
    test=dict(
        type='NYUBinFormerDataset',
        data_root='data/nyu/',
        depth_scale=1000,
        split='nyu_test.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(480, 640),
                flip=True,
                flip_direction='horizontal',
                transforms=[
                    dict(type='RandomFlip', direction='horizontal'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        garg_crop=False,
        eigen_crop=True,
        min_depth=0.001,
        max_depth=10))
log_config = dict(
    interval=100,
    hooks=[
        dict(type='TextLoggerHook', by_epoch=False),
        dict(type='TensorboardLoggerHook')
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=0.0001,
    betas=(0.9, 0.999),
    weight_decay=0.01,
    paramwise_cfg=dict(
        custom_keys=dict(
            absolute_pos_embed=dict(decay_mult=0.0),
            relative_position_bias_table=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0),
            backbone=dict(decay_mult=0.1))))
lr_config = dict(
    policy='OneCycle',
    max_lr=0.0001,
    warmup_iters=12800,
    div_factor=25,
    final_div_factor=100,
    by_epoch=False)
optimizer_config = dict(grad_clip=dict(max_norm=0.01, norm_type=2))
runner = dict(type='IterBasedRunner', max_iters=38400)
checkpoint_config = dict(by_epoch=False, max_keep_ckpts=2, interval=1600)
evaluation = dict(
    by_epoch=False,
    start=0,
    interval=800,
    pre_eval=True,
    rule='less',
    save_best='abs_rel',
    greater_keys=('a1', 'a2', 'a3'),
    less_keys=('abs_rel', 'rmse'))
find_unused_parameters = True
work_dir = './nfs/binsformer/binsformer_full_fpn_wcls_swint_nyu_n64'
gpu_ids = range(0, 1)

2022-01-10 16:43:35,164 - depth - INFO - Use load_from_local loader
Name of parameter - Initialization information

backbone.patch_embed.projection.weight - torch.Size([96, 3, 4, 4]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.projection.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.norm.weight - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.norm.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm1.weight - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm1.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 3]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.qkv.weight - torch.Size([288, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.qkv.bias - torch.Size([288]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.proj.weight - torch.Size([96, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.proj.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm2.weight - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm2.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.0.0.weight - torch.Size([384, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.0.0.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.1.weight - torch.Size([96, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.1.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm1.weight - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm1.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 3]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.qkv.weight - torch.Size([288, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.qkv.bias - torch.Size([288]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.proj.weight - torch.Size([96, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.proj.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm2.weight - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm2.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.0.0.weight - torch.Size([384, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.0.0.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.1.weight - torch.Size([96, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.1.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.norm.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.norm.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.reduction.weight - torch.Size([192, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm1.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.qkv.bias - torch.Size([576]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.proj.weight - torch.Size([192, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.proj.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm2.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm2.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.0.0.weight - torch.Size([768, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.0.0.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.1.weight - torch.Size([192, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm1.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.qkv.bias - torch.Size([576]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.proj.weight - torch.Size([192, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.proj.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm2.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm2.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.0.0.weight - torch.Size([768, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.0.0.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.1.weight - torch.Size([192, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.norm.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.norm.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.reduction.weight - torch.Size([384, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.norm.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.norm.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.reduction.weight - torch.Size([768, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.norm0.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm0.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm1.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm3.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

backbone.norm3.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_depth.weight - torch.Size([1, 256, 3, 3]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.conv_depth.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.level_embeds - torch.Size([3, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([384, 256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([384, 256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([384, 256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([384, 256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([384, 256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([384, 256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_encoder.reference_points.weight - torch.Size([2, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_encoder.reference_points.bias - torch.Size([2]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.level_embed.weight - torch.Size([3, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.skip_proj.0.conv.weight - torch.Size([256, 192, 1, 1]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.skip_proj.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.skip_proj.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.skip_proj.1.conv.weight - torch.Size([256, 384, 1, 1]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.skip_proj.1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.skip_proj.1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.skip_proj.2.conv.weight - torch.Size([256, 768, 1, 1]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.skip_proj.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.skip_proj.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.lateral_convs.0.conv.weight - torch.Size([256, 384, 1, 1]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.lateral_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.lateral_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.lateral_convs.1.conv.weight - torch.Size([256, 192, 1, 1]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.lateral_convs.1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.lateral_convs.1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.lateral_convs.2.conv.weight - torch.Size([256, 96, 1, 1]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.lateral_convs.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.lateral_convs.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.output_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.output_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.output_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.output_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.output_convs.1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.output_convs.1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.output_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.output_convs.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.output_convs.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.query_feat.weight - torch.Size([65, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.query_embed.weight - torch.Size([65, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.1.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.1.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.2.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.2.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.3.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.3.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.4.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.4.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.5.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.5.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.6.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.6.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.6.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.6.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.6.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.6.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.6.ffns.0.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.6.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.6.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.7.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.7.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.7.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.7.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.7.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.7.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.7.ffns.0.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.7.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.7.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.8.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.8.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.8.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.8.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.8.ffns.0.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.8.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.8.ffns.0.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.layers.8.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.layers.8.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.post_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.post_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.bins_embed.weight - torch.Size([1, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.bins_embed.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.mask_embed.layers.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.mask_embed.layers.0.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.mask_embed.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.mask_embed.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.mask_embed.layers.2.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.mask_embed.layers.2.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.class_embed.layers.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.class_embed.layers.0.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.class_embed.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.class_embed.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.class_embed.layers.2.weight - torch.Size([25, 256]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.class_embed.layers.2.bias - torch.Size([25]): 
Initialized by user-defined `init_weights` in BinsFormerDecodeHead  

decode_head.transformer_decoder.decoder.decoder_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.transformer_decoder.decoder.decoder_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  
2022-01-10 16:43:35,554 - depth - INFO - DepthEncoderDecoder(
  (backbone): SwinTransformer(
    (patch_embed): PatchEmbed(
      (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (drop_after_pos): Dropout(p=0.0, inplace=False)
    (stages): ModuleList(
      (0): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=96, out_features=288, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=96, out_features=384, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=384, out_features=96, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=96, out_features=288, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=96, out_features=384, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=384, out_features=96, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (downsample): PatchMerging(
          (sampler): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=384, out_features=192, bias=False)
        )
      )
      (1): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=192, out_features=768, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=768, out_features=192, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=192, out_features=768, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=768, out_features=192, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (downsample): PatchMerging(
          (sampler): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=768, out_features=384, bias=False)
        )
      )
      (2): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=384, out_features=1536, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=1536, out_features=384, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=384, out_features=1536, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=1536, out_features=384, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (2): SwinBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=384, out_features=1536, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=1536, out_features=384, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (3): SwinBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=384, out_features=1536, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=1536, out_features=384, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (4): SwinBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=384, out_features=1536, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=1536, out_features=384, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (5): SwinBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=384, out_features=1536, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=1536, out_features=384, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
        (downsample): PatchMerging(
          (sampler): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
        )
      )
      (3): SwinBlockSequence(
        (blocks): ModuleList(
          (0): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
          (1): SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (activate): GELU()
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
            )
          )
        )
      )
    )
    (norm0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (decode_head): BinsFormerDecodeHead(
    align_corners=True
    (loss_decode): SigLoss()
    (conv_depth): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (relu): ReLU()
    (sigmoid): Sigmoid()
    (transformer_encoder): PureMSDEnTransformer(
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=384, bias=True)
                (attention_weights): Linear(in_features=256, out_features=192, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=384, bias=True)
                (attention_weights): Linear(in_features=256, out_features=192, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=384, bias=True)
                (attention_weights): Linear(in_features=256, out_features=192, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=384, bias=True)
                (attention_weights): Linear(in_features=256, out_features=192, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=384, bias=True)
                (attention_weights): Linear(in_features=256, out_features=192, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=384, bias=True)
                (attention_weights): Linear(in_features=256, out_features=192, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=2, bias=True)
    )
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (pe_layer): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (level_embed): Embedding(3, 256)
    (skip_proj): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (output_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): LeakyReLU(negative_slope=0.01, inplace=True)
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (loss_class): CrossEntropyLoss()
    (query_feat): Embedding(65, 256)
    (query_embed): Embedding(65, 256)
    (transformer_decoder): PixelMSDeTransformer(
      (decoder): PixelTransformerDecoder(
        (layers): ModuleList(
          (0): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=256, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=256, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=256, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=256, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=256, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=256, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (6): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=256, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (7): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=256, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (8): PixelTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
              (1): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.0, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=2048, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=2048, out_features=256, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (bins_embed): Linear(in_features=256, out_features=1, bias=True)
        (mask_embed): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (class_embed): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=25, bias=True)
          )
        )
        (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
2022-01-10 16:43:35,663 - depth - INFO - Loaded 24231 images. Totally 0 invalid pairs are filtered
2022-01-10 16:43:41,564 - depth - INFO - Loaded 654 images. Totally 0 invalid pairs are filtered
2022-01-10 16:43:41,564 - depth - INFO - Start running, host: lizhenyu2@lzy-dev, work_dir: /nfs/lizhenyu2/codes/DepthFormer_Release/nfs/binsformer/binsformer_full_fpn_wcls_swint_nyu_n64
2022-01-10 16:43:41,564 - depth - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) OneCycleLrUpdaterHook              
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) OneCycleLrUpdaterHook              
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) OneCycleLrUpdaterHook              
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2022-01-10 16:43:41,565 - depth - INFO - workflow: [('train', 1)], max: 38400 iters
2022-01-10 16:44:28,771 - depth - INFO - Iter [100/38400]	lr: 4.017e-06, eta: 4:05:41, time: 0.385, data_time: 0.008, memory: 15411, decode.aux_loss_ce_2: 0.0343, decode.aux_loss_depth_2: 1.1142, decode.aux_loss_ce_5: 0.0352, decode.aux_loss_depth_5: 2.2016, decode.loss_ce: 0.0326, decode.ce_acc_level_0: 4.5625, decode.ce_acc_level_1: 24.9375, decode.loss_depth: 4.5209, loss: 7.9387, grad_norm: 32.2208
2022-01-10 16:45:03,625 - depth - INFO - Iter [200/38400]	lr: 4.071e-06, eta: 3:53:28, time: 0.349, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0332, decode.aux_loss_depth_2: 0.9699, decode.aux_loss_ce_5: 0.0326, decode.aux_loss_depth_5: 1.8565, decode.loss_ce: 0.0333, decode.ce_acc_level_0: 4.3750, decode.ce_acc_level_1: 22.3750, decode.loss_depth: 3.7223, loss: 6.6478, grad_norm: 67.9968
2022-01-10 16:45:38,118 - depth - INFO - Iter [300/38400]	lr: 4.160e-06, eta: 3:48:17, time: 0.345, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0324, decode.aux_loss_depth_2: 0.8787, decode.aux_loss_ce_5: 0.0316, decode.aux_loss_depth_5: 1.7016, decode.loss_ce: 0.0321, decode.ce_acc_level_0: 6.5000, decode.ce_acc_level_1: 26.0000, decode.loss_depth: 3.3971, loss: 6.0735, grad_norm: 58.8106
2022-01-10 16:46:12,383 - depth - INFO - Iter [400/38400]	lr: 4.284e-06, eta: 3:44:59, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0315, decode.aux_loss_depth_2: 0.8187, decode.aux_loss_ce_5: 0.0302, decode.aux_loss_depth_5: 1.5756, decode.loss_ce: 0.0310, decode.ce_acc_level_0: 11.8125, decode.ce_acc_level_1: 35.5000, decode.loss_depth: 3.1398, loss: 5.6269, grad_norm: 56.5753
2022-01-10 16:46:46,622 - depth - INFO - Iter [500/38400]	lr: 4.444e-06, eta: 3:42:47, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0306, decode.aux_loss_depth_2: 0.7797, decode.aux_loss_ce_5: 0.0293, decode.aux_loss_depth_5: 1.5181, decode.loss_ce: 0.0307, decode.ce_acc_level_0: 11.7500, decode.ce_acc_level_1: 37.5000, decode.loss_depth: 3.0320, loss: 5.4204, grad_norm: 54.6950
2022-01-10 16:47:20,849 - depth - INFO - Iter [600/38400]	lr: 4.639e-06, eta: 3:41:04, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0299, decode.aux_loss_depth_2: 0.7446, decode.aux_loss_ce_5: 0.0281, decode.aux_loss_depth_5: 1.4592, decode.loss_ce: 0.0289, decode.ce_acc_level_0: 15.5625, decode.ce_acc_level_1: 54.4375, decode.loss_depth: 2.9165, loss: 5.2072, grad_norm: 51.9279
2022-01-10 16:47:55,363 - depth - INFO - Iter [700/38400]	lr: 4.870e-06, eta: 3:39:58, time: 0.345, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0290, decode.aux_loss_depth_2: 0.7338, decode.aux_loss_ce_5: 0.0277, decode.aux_loss_depth_5: 1.4427, decode.loss_ce: 0.0288, decode.ce_acc_level_0: 14.6250, decode.ce_acc_level_1: 51.2500, decode.loss_depth: 2.8795, loss: 5.1415, grad_norm: 53.2683
2022-01-10 16:48:29,369 - depth - INFO - Iter [800/38400]	lr: 5.135e-06, eta: 3:38:37, time: 0.340, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0280, decode.aux_loss_depth_2: 0.6970, decode.aux_loss_ce_5: 0.0270, decode.aux_loss_depth_5: 1.3746, decode.loss_ce: 0.0274, decode.ce_acc_level_0: 18.1250, decode.ce_acc_level_1: 59.8125, decode.loss_depth: 2.7468, loss: 4.9007, grad_norm: 51.1346
2022-01-10 16:48:54,697 - depth - INFO - Summary:
2022-01-10 16:48:54,697 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
| 0.5584 | 0.8688 | 0.965 |  0.2847 | 0.7906 | 0.1036 |  0.2861  | 20.3266 | 0.3017 |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
2022-01-10 16:48:57,463 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_800.pth.
2022-01-10 16:48:57,464 - depth - INFO - Best abs_rel is 0.2847 at 800 iter.
2022-01-10 16:48:57,465 - depth - INFO - Iter(val) [82]	a1: 0.5584, a2: 0.8688, a3: 0.9650, abs_rel: 0.28468531370162964, rmse: 0.7906111478805542, log_10: 0.10364963114261627, rmse_log: 0.28610581159591675, silog: 20.3266, sq_rel: 0.3017416000366211
2022-01-10 16:49:31,914 - depth - INFO - Iter [900/38400]	lr: 5.436e-06, eta: 3:57:14, time: 0.625, data_time: 0.287, memory: 15411, decode.aux_loss_ce_2: 0.0276, decode.aux_loss_depth_2: 0.6796, decode.aux_loss_ce_5: 0.0264, decode.aux_loss_depth_5: 1.3381, decode.loss_ce: 0.0268, decode.ce_acc_level_0: 21.1875, decode.ce_acc_level_1: 64.2500, decode.loss_depth: 2.6757, loss: 4.7742, grad_norm: 49.8815
2022-01-10 16:50:06,226 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 16:50:06,227 - depth - INFO - Iter [1000/38400]	lr: 5.771e-06, eta: 3:54:20, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0271, decode.aux_loss_depth_2: 0.6717, decode.aux_loss_ce_5: 0.0265, decode.aux_loss_depth_5: 1.3243, decode.loss_ce: 0.0270, decode.ce_acc_level_0: 21.5625, decode.ce_acc_level_1: 61.7500, decode.loss_depth: 2.6487, loss: 4.7254, grad_norm: 49.4091
2022-01-10 16:50:40,499 - depth - INFO - Iter [1100/38400]	lr: 6.140e-06, eta: 3:51:50, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0262, decode.aux_loss_depth_2: 0.6629, decode.aux_loss_ce_5: 0.0259, decode.aux_loss_depth_5: 1.3114, decode.loss_ce: 0.0262, decode.ce_acc_level_0: 19.1875, decode.ce_acc_level_1: 67.1875, decode.loss_depth: 2.6221, loss: 4.6747, grad_norm: 46.2959
2022-01-10 16:51:14,924 - depth - INFO - Iter [1200/38400]	lr: 6.544e-06, eta: 3:49:43, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0265, decode.aux_loss_depth_2: 0.6412, decode.aux_loss_ce_5: 0.0260, decode.aux_loss_depth_5: 1.2658, decode.loss_ce: 0.0268, decode.ce_acc_level_0: 16.4375, decode.ce_acc_level_1: 64.1875, decode.loss_depth: 2.5385, loss: 4.5249, grad_norm: 45.4438
2022-01-10 16:51:49,933 - depth - INFO - Iter [1300/38400]	lr: 6.981e-06, eta: 3:48:06, time: 0.349, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0261, decode.aux_loss_depth_2: 0.6409, decode.aux_loss_ce_5: 0.0256, decode.aux_loss_depth_5: 1.2662, decode.loss_ce: 0.0261, decode.ce_acc_level_0: 19.5625, decode.ce_acc_level_1: 66.4375, decode.loss_depth: 2.5403, loss: 4.5251, grad_norm: 44.5759
2022-01-10 16:52:24,179 - depth - INFO - Iter [1400/38400]	lr: 7.452e-06, eta: 3:46:21, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0255, decode.aux_loss_depth_2: 0.6341, decode.aux_loss_ce_5: 0.0252, decode.aux_loss_depth_5: 1.2525, decode.loss_ce: 0.0256, decode.ce_acc_level_0: 19.5000, decode.ce_acc_level_1: 68.5625, decode.loss_depth: 2.5069, loss: 4.4699, grad_norm: 44.0820
2022-01-10 16:52:58,600 - depth - INFO - Iter [1500/38400]	lr: 7.956e-06, eta: 3:44:48, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0251, decode.aux_loss_depth_2: 0.6054, decode.aux_loss_ce_5: 0.0249, decode.aux_loss_depth_5: 1.1978, decode.loss_ce: 0.0252, decode.ce_acc_level_0: 19.0625, decode.ce_acc_level_1: 69.7500, decode.loss_depth: 2.4017, loss: 4.2802, grad_norm: 42.4972
2022-01-10 16:53:32,715 - depth - INFO - Saving checkpoint at 1600 iterations
2022-01-10 16:53:35,795 - depth - INFO - Iter [1600/38400]	lr: 8.492e-06, eta: 3:44:26, time: 0.372, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0249, decode.aux_loss_depth_2: 0.5998, decode.aux_loss_ce_5: 0.0251, decode.aux_loss_depth_5: 1.1840, decode.loss_ce: 0.0253, decode.ce_acc_level_0: 20.5625, decode.ce_acc_level_1: 68.5625, decode.loss_depth: 2.3675, loss: 4.2267, grad_norm: 43.7169
2022-01-10 16:53:49,251 - depth - INFO - Summary:
2022-01-10 16:53:49,252 - depth - INFO - 
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1  |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.602 | 0.9067 | 0.9775 |  0.2498 | 0.7117 | 0.0938 |  0.2589  | 18.2878 | 0.2359 |
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 16:53:52,724 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_1600.pth.
2022-01-10 16:53:52,724 - depth - INFO - Best abs_rel is 0.2498 at 1600 iter.
2022-01-10 16:53:52,725 - depth - INFO - Iter(val) [82]	a1: 0.6020, a2: 0.9067, a3: 0.9775, abs_rel: 0.2498379349708557, rmse: 0.7117272019386292, log_10: 0.09375893324613571, rmse_log: 0.2589283883571625, silog: 18.2878, sq_rel: 0.23590391874313354
2022-01-10 16:54:26,905 - depth - INFO - Iter [1700/38400]	lr: 9.062e-06, eta: 3:49:03, time: 0.511, data_time: 0.174, memory: 15411, decode.aux_loss_ce_2: 0.0248, decode.aux_loss_depth_2: 0.5839, decode.aux_loss_ce_5: 0.0249, decode.aux_loss_depth_5: 1.1516, decode.loss_ce: 0.0252, decode.ce_acc_level_0: 19.4375, decode.ce_acc_level_1: 70.9375, decode.loss_depth: 2.3086, loss: 4.1191, grad_norm: 45.4576
2022-01-10 16:55:01,008 - depth - INFO - Iter [1800/38400]	lr: 9.663e-06, eta: 3:47:17, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0253, decode.aux_loss_depth_2: 0.5883, decode.aux_loss_ce_5: 0.0255, decode.aux_loss_depth_5: 1.1633, decode.loss_ce: 0.0256, decode.ce_acc_level_0: 19.2500, decode.ce_acc_level_1: 67.6250, decode.loss_depth: 2.3318, loss: 4.1598, grad_norm: 44.6025
2022-01-10 16:55:35,449 - depth - INFO - Iter [1900/38400]	lr: 1.030e-05, eta: 3:45:46, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0246, decode.aux_loss_depth_2: 0.5813, decode.aux_loss_ce_5: 0.0249, decode.aux_loss_depth_5: 1.1480, decode.loss_ce: 0.0252, decode.ce_acc_level_0: 21.9375, decode.ce_acc_level_1: 70.3125, decode.loss_depth: 2.2955, loss: 4.0995, grad_norm: 44.9519
2022-01-10 16:56:09,909 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 16:56:09,909 - depth - INFO - Iter [2000/38400]	lr: 1.096e-05, eta: 3:44:20, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0246, decode.aux_loss_depth_2: 0.5694, decode.aux_loss_ce_5: 0.0247, decode.aux_loss_depth_5: 1.1234, decode.loss_ce: 0.0249, decode.ce_acc_level_0: 19.5625, decode.ce_acc_level_1: 70.5625, decode.loss_depth: 2.2461, loss: 4.0132, grad_norm: 43.6543
2022-01-10 16:56:44,350 - depth - INFO - Iter [2100/38400]	lr: 1.165e-05, eta: 3:43:00, time: 0.345, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0248, decode.aux_loss_depth_2: 0.5657, decode.aux_loss_ce_5: 0.0251, decode.aux_loss_depth_5: 1.1224, decode.loss_ce: 0.0251, decode.ce_acc_level_0: 20.4375, decode.ce_acc_level_1: 69.3125, decode.loss_depth: 2.2472, loss: 4.0102, grad_norm: 44.8149
2022-01-10 16:57:18,622 - depth - INFO - Iter [2200/38400]	lr: 1.238e-05, eta: 3:41:40, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0236, decode.aux_loss_depth_2: 0.5464, decode.aux_loss_ce_5: 0.0240, decode.aux_loss_depth_5: 1.0822, decode.loss_ce: 0.0244, decode.ce_acc_level_0: 22.0000, decode.ce_acc_level_1: 72.6250, decode.loss_depth: 2.1729, loss: 3.8735, grad_norm: 40.3165
2022-01-10 16:57:52,753 - depth - INFO - Iter [2300/38400]	lr: 1.313e-05, eta: 3:40:23, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0239, decode.aux_loss_depth_2: 0.5383, decode.aux_loss_ce_5: 0.0241, decode.aux_loss_depth_5: 1.0654, decode.loss_ce: 0.0246, decode.ce_acc_level_0: 18.6875, decode.ce_acc_level_1: 72.0625, decode.loss_depth: 2.1325, loss: 3.8087, grad_norm: 38.3323
2022-01-10 16:58:27,184 - depth - INFO - Iter [2400/38400]	lr: 1.391e-05, eta: 3:39:12, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0237, decode.aux_loss_depth_2: 0.5380, decode.aux_loss_ce_5: 0.0241, decode.aux_loss_depth_5: 1.0692, decode.loss_ce: 0.0245, decode.ce_acc_level_0: 19.3125, decode.ce_acc_level_1: 72.5625, decode.loss_depth: 2.1393, loss: 3.8188, grad_norm: 40.6090
2022-01-10 16:58:41,047 - depth - INFO - Summary:
2022-01-10 16:58:41,048 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.7163 | 0.9417 | 0.9867 |  0.1857 | 0.5796 | 0.0758 |  0.2151  | 16.7714 | 0.1431 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 16:58:44,614 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_2400.pth.
2022-01-10 16:58:44,615 - depth - INFO - Best abs_rel is 0.1857 at 2400 iter.
2022-01-10 16:58:44,616 - depth - INFO - Iter(val) [82]	a1: 0.7163, a2: 0.9417, a3: 0.9867, abs_rel: 0.1857316792011261, rmse: 0.5796181559562683, log_10: 0.07577341049909592, rmse_log: 0.2150718867778778, silog: 16.7714, sq_rel: 0.14314447343349457
2022-01-10 16:59:18,966 - depth - INFO - Iter [2500/38400]	lr: 1.472e-05, eta: 3:42:15, time: 0.518, data_time: 0.180, memory: 15411, decode.aux_loss_ce_2: 0.0233, decode.aux_loss_depth_2: 0.5351, decode.aux_loss_ce_5: 0.0238, decode.aux_loss_depth_5: 1.0638, decode.loss_ce: 0.0241, decode.ce_acc_level_0: 22.5625, decode.ce_acc_level_1: 73.2500, decode.loss_depth: 2.1296, loss: 3.7996, grad_norm: 38.2466
2022-01-10 16:59:52,880 - depth - INFO - Iter [2600/38400]	lr: 1.556e-05, eta: 3:40:53, time: 0.339, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0233, decode.aux_loss_depth_2: 0.5109, decode.aux_loss_ce_5: 0.0239, decode.aux_loss_depth_5: 1.0204, decode.loss_ce: 0.0241, decode.ce_acc_level_0: 21.0625, decode.ce_acc_level_1: 73.0625, decode.loss_depth: 2.0443, loss: 3.6469, grad_norm: 38.1294
2022-01-10 17:00:27,555 - depth - INFO - Iter [2700/38400]	lr: 1.643e-05, eta: 3:39:45, time: 0.347, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0229, decode.aux_loss_depth_2: 0.5215, decode.aux_loss_ce_5: 0.0236, decode.aux_loss_depth_5: 1.0362, decode.loss_ce: 0.0240, decode.ce_acc_level_0: 23.6875, decode.ce_acc_level_1: 73.7500, decode.loss_depth: 2.0739, loss: 3.7020, grad_norm: 39.0508
2022-01-10 17:01:01,867 - depth - INFO - Iter [2800/38400]	lr: 1.732e-05, eta: 3:38:35, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0223, decode.aux_loss_depth_2: 0.4967, decode.aux_loss_ce_5: 0.0228, decode.aux_loss_depth_5: 0.9886, decode.loss_ce: 0.0233, decode.ce_acc_level_0: 24.7500, decode.ce_acc_level_1: 75.9375, decode.loss_depth: 1.9737, loss: 3.5273, grad_norm: 36.6445
2022-01-10 17:01:35,758 - depth - INFO - Iter [2900/38400]	lr: 1.824e-05, eta: 3:37:22, time: 0.339, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0225, decode.aux_loss_depth_2: 0.4956, decode.aux_loss_ce_5: 0.0232, decode.aux_loss_depth_5: 0.9818, decode.loss_ce: 0.0234, decode.ce_acc_level_0: 22.0625, decode.ce_acc_level_1: 75.5625, decode.loss_depth: 1.9624, loss: 3.5089, grad_norm: 37.3987
2022-01-10 17:02:09,627 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 17:02:09,627 - depth - INFO - Iter [3000/38400]	lr: 1.918e-05, eta: 3:36:11, time: 0.339, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0222, decode.aux_loss_depth_2: 0.4860, decode.aux_loss_ce_5: 0.0226, decode.aux_loss_depth_5: 0.9664, decode.loss_ce: 0.0232, decode.ce_acc_level_0: 23.1250, decode.ce_acc_level_1: 76.0000, decode.loss_depth: 1.9335, loss: 3.4539, grad_norm: 36.6559
2022-01-10 17:02:44,183 - depth - INFO - Iter [3100/38400]	lr: 2.015e-05, eta: 3:35:11, time: 0.346, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0225, decode.aux_loss_depth_2: 0.4851, decode.aux_loss_ce_5: 0.0230, decode.aux_loss_depth_5: 0.9648, decode.loss_ce: 0.0235, decode.ce_acc_level_0: 21.6875, decode.ce_acc_level_1: 75.9375, decode.loss_depth: 1.9387, loss: 3.4576, grad_norm: 36.2365
2022-01-10 17:03:18,516 - depth - INFO - Saving checkpoint at 3200 iterations
2022-01-10 17:03:21,400 - depth - INFO - Iter [3200/38400]	lr: 2.114e-05, eta: 3:34:42, time: 0.372, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0226, decode.aux_loss_depth_2: 0.4765, decode.aux_loss_ce_5: 0.0230, decode.aux_loss_depth_5: 0.9498, decode.loss_ce: 0.0238, decode.ce_acc_level_0: 26.9375, decode.ce_acc_level_1: 76.1250, decode.loss_depth: 1.9129, loss: 3.4085, grad_norm: 35.7632
2022-01-10 17:03:34,952 - depth - INFO - Summary:
2022-01-10 17:03:34,953 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
| 0.6281 | 0.9207 | 0.979 |  0.2432 | 0.6614 | 0.0898 |  0.2465  | 16.0713 | 0.2168 |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
2022-01-10 17:03:34,954 - depth - INFO - Iter(val) [82]	a1: 0.6281, a2: 0.9207, a3: 0.9790, abs_rel: 0.24323582649230957, rmse: 0.6614359617233276, log_10: 0.08982203900814056, rmse_log: 0.24651548266410828, silog: 16.0713, sq_rel: 0.21675245463848114
2022-01-10 17:04:09,309 - depth - INFO - Iter [3300/38400]	lr: 2.215e-05, eta: 3:36:05, time: 0.479, data_time: 0.141, memory: 15411, decode.aux_loss_ce_2: 0.0220, decode.aux_loss_depth_2: 0.4766, decode.aux_loss_ce_5: 0.0228, decode.aux_loss_depth_5: 0.9493, decode.loss_ce: 0.0234, decode.ce_acc_level_0: 22.2500, decode.ce_acc_level_1: 75.5000, decode.loss_depth: 1.9068, loss: 3.4009, grad_norm: 36.7189
2022-01-10 17:04:43,675 - depth - INFO - Iter [3400/38400]	lr: 2.319e-05, eta: 3:35:02, time: 0.344, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0224, decode.aux_loss_depth_2: 0.4752, decode.aux_loss_ce_5: 0.0231, decode.aux_loss_depth_5: 0.9512, decode.loss_ce: 0.0236, decode.ce_acc_level_0: 22.5625, decode.ce_acc_level_1: 74.1250, decode.loss_depth: 1.9020, loss: 3.3977, grad_norm: 35.5140
2022-01-10 17:05:18,722 - depth - INFO - Iter [3500/38400]	lr: 2.425e-05, eta: 3:34:07, time: 0.350, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0221, decode.aux_loss_depth_2: 0.4712, decode.aux_loss_ce_5: 0.0227, decode.aux_loss_depth_5: 0.9399, decode.loss_ce: 0.0232, decode.ce_acc_level_0: 28.6250, decode.ce_acc_level_1: 75.3125, decode.loss_depth: 1.8849, loss: 3.3640, grad_norm: 32.6204
2022-01-10 17:05:52,748 - depth - INFO - Iter [3600/38400]	lr: 2.533e-05, eta: 3:33:03, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0216, decode.aux_loss_depth_2: 0.4538, decode.aux_loss_ce_5: 0.0224, decode.aux_loss_depth_5: 0.9055, decode.loss_ce: 0.0227, decode.ce_acc_level_0: 28.2500, decode.ce_acc_level_1: 75.7500, decode.loss_depth: 1.8061, loss: 3.2321, grad_norm: 30.8128
2022-01-10 17:06:26,884 - depth - INFO - Iter [3700/38400]	lr: 2.642e-05, eta: 3:32:02, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0215, decode.aux_loss_depth_2: 0.4492, decode.aux_loss_ce_5: 0.0222, decode.aux_loss_depth_5: 0.8978, decode.loss_ce: 0.0229, decode.ce_acc_level_0: 28.4375, decode.ce_acc_level_1: 76.3750, decode.loss_depth: 1.7987, loss: 3.2123, grad_norm: 32.3849
2022-01-10 17:07:01,106 - depth - INFO - Iter [3800/38400]	lr: 2.754e-05, eta: 3:31:03, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0216, decode.aux_loss_depth_2: 0.4459, decode.aux_loss_ce_5: 0.0223, decode.aux_loss_depth_5: 0.8899, decode.loss_ce: 0.0229, decode.ce_acc_level_0: 27.6250, decode.ce_acc_level_1: 76.8750, decode.loss_depth: 1.7824, loss: 3.1851, grad_norm: 30.8483
2022-01-10 17:07:35,682 - depth - INFO - Iter [3900/38400]	lr: 2.868e-05, eta: 3:30:08, time: 0.345, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0217, decode.aux_loss_depth_2: 0.4456, decode.aux_loss_ce_5: 0.0223, decode.aux_loss_depth_5: 0.8907, decode.loss_ce: 0.0229, decode.ce_acc_level_0: 29.5625, decode.ce_acc_level_1: 76.5000, decode.loss_depth: 1.7830, loss: 3.1861, grad_norm: 29.1992
2022-01-10 17:08:10,218 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 17:08:10,218 - depth - INFO - Iter [4000/38400]	lr: 2.983e-05, eta: 3:29:15, time: 0.346, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0215, decode.aux_loss_depth_2: 0.4358, decode.aux_loss_ce_5: 0.0223, decode.aux_loss_depth_5: 0.8702, decode.loss_ce: 0.0224, decode.ce_acc_level_0: 30.8750, decode.ce_acc_level_1: 77.5000, decode.loss_depth: 1.7401, loss: 3.1122, grad_norm: 29.3306
2022-01-10 17:08:23,900 - depth - INFO - Summary:
2022-01-10 17:08:23,901 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
| 0.6728 | 0.9372 | 0.986 |  0.2139 | 0.7021 | 0.0817 |  0.2278  | 16.2733 | 0.2115 |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
2022-01-10 17:08:23,903 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 17:08:23,903 - depth - INFO - Iter(val) [82]	a1: 0.6728, a2: 0.9372, a3: 0.9860, abs_rel: 0.2139279842376709, rmse: 0.7021393179893494, log_10: 0.08174525201320648, rmse_log: 0.22781549394130707, silog: 16.2733, sq_rel: 0.2115345597267151
2022-01-10 17:08:58,266 - depth - INFO - Iter [4100/38400]	lr: 3.100e-05, eta: 3:30:15, time: 0.481, data_time: 0.143, memory: 15411, decode.aux_loss_ce_2: 0.0207, decode.aux_loss_depth_2: 0.4304, decode.aux_loss_ce_5: 0.0214, decode.aux_loss_depth_5: 0.8602, decode.loss_ce: 0.0221, decode.ce_acc_level_0: 31.0625, decode.ce_acc_level_1: 78.3125, decode.loss_depth: 1.7259, loss: 3.0807, grad_norm: 30.5089
2022-01-10 17:09:32,355 - depth - INFO - Iter [4200/38400]	lr: 3.218e-05, eta: 3:29:16, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0213, decode.aux_loss_depth_2: 0.4331, decode.aux_loss_ce_5: 0.0220, decode.aux_loss_depth_5: 0.8628, decode.loss_ce: 0.0225, decode.ce_acc_level_0: 29.8125, decode.ce_acc_level_1: 77.6250, decode.loss_depth: 1.7268, loss: 3.0885, grad_norm: 29.5919
2022-01-10 17:10:06,570 - depth - INFO - Iter [4300/38400]	lr: 3.338e-05, eta: 3:28:19, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0210, decode.aux_loss_depth_2: 0.4230, decode.aux_loss_ce_5: 0.0219, decode.aux_loss_depth_5: 0.8455, decode.loss_ce: 0.0225, decode.ce_acc_level_0: 27.8125, decode.ce_acc_level_1: 77.2500, decode.loss_depth: 1.6943, loss: 3.0281, grad_norm: 28.0724
2022-01-10 17:10:41,172 - depth - INFO - Iter [4400/38400]	lr: 3.460e-05, eta: 3:27:27, time: 0.346, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0210, decode.aux_loss_depth_2: 0.4202, decode.aux_loss_ce_5: 0.0220, decode.aux_loss_depth_5: 0.8415, decode.loss_ce: 0.0227, decode.ce_acc_level_0: 29.6250, decode.ce_acc_level_1: 75.8125, decode.loss_depth: 1.6860, loss: 3.0134, grad_norm: 27.5236
2022-01-10 17:11:15,504 - depth - INFO - Iter [4500/38400]	lr: 3.582e-05, eta: 3:26:33, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0207, decode.aux_loss_depth_2: 0.4159, decode.aux_loss_ce_5: 0.0215, decode.aux_loss_depth_5: 0.8332, decode.loss_ce: 0.0218, decode.ce_acc_level_0: 31.1875, decode.ce_acc_level_1: 77.4375, decode.loss_depth: 1.6618, loss: 2.9749, grad_norm: 27.6960
2022-01-10 17:11:49,781 - depth - INFO - Iter [4600/38400]	lr: 3.706e-05, eta: 3:25:40, time: 0.342, data_time: 0.008, memory: 15411, decode.aux_loss_ce_2: 0.0208, decode.aux_loss_depth_2: 0.4058, decode.aux_loss_ce_5: 0.0218, decode.aux_loss_depth_5: 0.8129, decode.loss_ce: 0.0218, decode.ce_acc_level_0: 31.4375, decode.ce_acc_level_1: 78.5625, decode.loss_depth: 1.6245, loss: 2.9076, grad_norm: 25.9066
2022-01-10 17:12:24,573 - depth - INFO - Iter [4700/38400]	lr: 3.831e-05, eta: 3:24:51, time: 0.348, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0205, decode.aux_loss_depth_2: 0.4108, decode.aux_loss_ce_5: 0.0212, decode.aux_loss_depth_5: 0.8227, decode.loss_ce: 0.0217, decode.ce_acc_level_0: 30.4375, decode.ce_acc_level_1: 79.5000, decode.loss_depth: 1.6424, loss: 2.9394, grad_norm: 25.6625
2022-01-10 17:12:58,655 - depth - INFO - Saving checkpoint at 4800 iterations
2022-01-10 17:13:02,184 - depth - INFO - Iter [4800/38400]	lr: 3.957e-05, eta: 3:24:23, time: 0.376, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0204, decode.aux_loss_depth_2: 0.4029, decode.aux_loss_ce_5: 0.0214, decode.aux_loss_depth_5: 0.8051, decode.loss_ce: 0.0217, decode.ce_acc_level_0: 31.5625, decode.ce_acc_level_1: 78.1250, decode.loss_depth: 1.6084, loss: 2.8798, grad_norm: 23.1255
2022-01-10 17:13:15,396 - depth - INFO - Summary:
2022-01-10 17:13:15,397 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.7345 | 0.9567 | 0.9912 |  0.1818 | 0.5771 | 0.0729 |  0.2031  | 14.9921 | 0.1433 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 17:13:18,459 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_4800.pth.
2022-01-10 17:13:18,459 - depth - INFO - Best abs_rel is 0.1818 at 4800 iter.
2022-01-10 17:13:18,460 - depth - INFO - Iter(val) [82]	a1: 0.7345, a2: 0.9567, a3: 0.9912, abs_rel: 0.18175987899303436, rmse: 0.5771065354347229, log_10: 0.07285025715827942, rmse_log: 0.2031080573797226, silog: 14.9921, sq_rel: 0.1432868242263794
2022-01-10 17:13:52,913 - depth - INFO - Iter [4900/38400]	lr: 4.084e-05, eta: 3:25:23, time: 0.507, data_time: 0.169, memory: 15411, decode.aux_loss_ce_2: 0.0200, decode.aux_loss_depth_2: 0.3985, decode.aux_loss_ce_5: 0.0211, decode.aux_loss_depth_5: 0.7972, decode.loss_ce: 0.0216, decode.ce_acc_level_0: 34.8125, decode.ce_acc_level_1: 78.4375, decode.loss_depth: 1.5974, loss: 2.8558, grad_norm: 25.5623
2022-01-10 17:14:26,789 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 17:14:26,789 - depth - INFO - Iter [5000/38400]	lr: 4.212e-05, eta: 3:24:27, time: 0.339, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0207, decode.aux_loss_depth_2: 0.3957, decode.aux_loss_ce_5: 0.0216, decode.aux_loss_depth_5: 0.7942, decode.loss_ce: 0.0222, decode.ce_acc_level_0: 30.0000, decode.ce_acc_level_1: 77.5000, decode.loss_depth: 1.5908, loss: 2.8451, grad_norm: 24.8734
2022-01-10 17:15:01,115 - depth - INFO - Iter [5100/38400]	lr: 4.340e-05, eta: 3:23:35, time: 0.344, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0201, decode.aux_loss_depth_2: 0.3897, decode.aux_loss_ce_5: 0.0209, decode.aux_loss_depth_5: 0.7800, decode.loss_ce: 0.0215, decode.ce_acc_level_0: 33.0625, decode.ce_acc_level_1: 79.2500, decode.loss_depth: 1.5567, loss: 2.7889, grad_norm: 24.8105
2022-01-10 17:15:35,263 - depth - INFO - Iter [5200/38400]	lr: 4.469e-05, eta: 3:22:42, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0199, decode.aux_loss_depth_2: 0.3915, decode.aux_loss_ce_5: 0.0207, decode.aux_loss_depth_5: 0.7833, decode.loss_ce: 0.0215, decode.ce_acc_level_0: 29.3125, decode.ce_acc_level_1: 78.4375, decode.loss_depth: 1.5658, loss: 2.8027, grad_norm: 24.8385
2022-01-10 17:16:09,682 - depth - INFO - Iter [5300/38400]	lr: 4.599e-05, eta: 3:21:51, time: 0.344, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0193, decode.aux_loss_depth_2: 0.3864, decode.aux_loss_ce_5: 0.0203, decode.aux_loss_depth_5: 0.7714, decode.loss_ce: 0.0211, decode.ce_acc_level_0: 34.6875, decode.ce_acc_level_1: 79.5000, decode.loss_depth: 1.5454, loss: 2.7640, grad_norm: 24.0582
2022-01-10 17:16:44,356 - depth - INFO - Iter [5400/38400]	lr: 4.729e-05, eta: 3:21:03, time: 0.347, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0196, decode.aux_loss_depth_2: 0.3831, decode.aux_loss_ce_5: 0.0207, decode.aux_loss_depth_5: 0.7668, decode.loss_ce: 0.0212, decode.ce_acc_level_0: 33.7500, decode.ce_acc_level_1: 79.4375, decode.loss_depth: 1.5372, loss: 2.7486, grad_norm: 23.4621
2022-01-10 17:17:18,553 - depth - INFO - Iter [5500/38400]	lr: 4.859e-05, eta: 3:20:12, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0194, decode.aux_loss_depth_2: 0.3751, decode.aux_loss_ce_5: 0.0202, decode.aux_loss_depth_5: 0.7500, decode.loss_ce: 0.0209, decode.ce_acc_level_0: 33.0000, decode.ce_acc_level_1: 81.0625, decode.loss_depth: 1.5047, loss: 2.6902, grad_norm: 22.3957
2022-01-10 17:17:52,725 - depth - INFO - Iter [5600/38400]	lr: 4.990e-05, eta: 3:19:22, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0197, decode.aux_loss_depth_2: 0.3736, decode.aux_loss_ce_5: 0.0206, decode.aux_loss_depth_5: 0.7479, decode.loss_ce: 0.0212, decode.ce_acc_level_0: 33.3750, decode.ce_acc_level_1: 79.4375, decode.loss_depth: 1.4923, loss: 2.6753, grad_norm: 23.0201
2022-01-10 17:18:06,092 - depth - INFO - Summary:
2022-01-10 17:18:06,092 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.7122 | 0.9503 | 0.9902 |  0.2005 | 0.5908 | 0.0765 |   0.21   | 13.9831 | 0.1608 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 17:18:06,093 - depth - INFO - Iter(val) [82]	a1: 0.7122, a2: 0.9503, a3: 0.9902, abs_rel: 0.2004801481962204, rmse: 0.5907690525054932, log_10: 0.07646478712558746, rmse_log: 0.20995932817459106, silog: 13.9831, sq_rel: 0.16081711649894714
2022-01-10 17:18:40,457 - depth - INFO - Iter [5700/38400]	lr: 5.121e-05, eta: 3:19:50, time: 0.477, data_time: 0.139, memory: 15411, decode.aux_loss_ce_2: 0.0195, decode.aux_loss_depth_2: 0.3775, decode.aux_loss_ce_5: 0.0203, decode.aux_loss_depth_5: 0.7547, decode.loss_ce: 0.0210, decode.ce_acc_level_0: 34.5625, decode.ce_acc_level_1: 78.6250, decode.loss_depth: 1.5081, loss: 2.7012, grad_norm: 22.9735
2022-01-10 17:19:14,876 - depth - INFO - Iter [5800/38400]	lr: 5.252e-05, eta: 3:19:01, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0193, decode.aux_loss_depth_2: 0.3782, decode.aux_loss_ce_5: 0.0203, decode.aux_loss_depth_5: 0.7567, decode.loss_ce: 0.0209, decode.ce_acc_level_0: 35.4375, decode.ce_acc_level_1: 79.2500, decode.loss_depth: 1.5129, loss: 2.7084, grad_norm: 23.2870
2022-01-10 17:19:49,359 - depth - INFO - Iter [5900/38400]	lr: 5.383e-05, eta: 3:18:12, time: 0.345, data_time: 0.008, memory: 15411, decode.aux_loss_ce_2: 0.0192, decode.aux_loss_depth_2: 0.3756, decode.aux_loss_ce_5: 0.0203, decode.aux_loss_depth_5: 0.7508, decode.loss_ce: 0.0209, decode.ce_acc_level_0: 37.1250, decode.ce_acc_level_1: 78.6875, decode.loss_depth: 1.4996, loss: 2.6864, grad_norm: 21.4333
2022-01-10 17:20:23,895 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 17:20:23,895 - depth - INFO - Iter [6000/38400]	lr: 5.513e-05, eta: 3:17:25, time: 0.345, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0189, decode.aux_loss_depth_2: 0.3658, decode.aux_loss_ce_5: 0.0195, decode.aux_loss_depth_5: 0.7304, decode.loss_ce: 0.0205, decode.ce_acc_level_0: 34.3125, decode.ce_acc_level_1: 82.1250, decode.loss_depth: 1.4590, loss: 2.6141, grad_norm: 22.8797
2022-01-10 17:20:58,545 - depth - INFO - Iter [6100/38400]	lr: 5.644e-05, eta: 3:16:38, time: 0.346, data_time: 0.009, memory: 15411, decode.aux_loss_ce_2: 0.0183, decode.aux_loss_depth_2: 0.3567, decode.aux_loss_ce_5: 0.0191, decode.aux_loss_depth_5: 0.7134, decode.loss_ce: 0.0199, decode.ce_acc_level_0: 37.4375, decode.ce_acc_level_1: 82.3750, decode.loss_depth: 1.4254, loss: 2.5528, grad_norm: 21.6587
2022-01-10 17:21:33,052 - depth - INFO - Iter [6200/38400]	lr: 5.774e-05, eta: 3:15:51, time: 0.345, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0190, decode.aux_loss_depth_2: 0.3556, decode.aux_loss_ce_5: 0.0198, decode.aux_loss_depth_5: 0.7109, decode.loss_ce: 0.0205, decode.ce_acc_level_0: 38.2500, decode.ce_acc_level_1: 81.1250, decode.loss_depth: 1.4194, loss: 2.5453, grad_norm: 22.2166
2022-01-10 17:22:07,507 - depth - INFO - Iter [6300/38400]	lr: 5.904e-05, eta: 3:15:04, time: 0.345, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0183, decode.aux_loss_depth_2: 0.3573, decode.aux_loss_ce_5: 0.0192, decode.aux_loss_depth_5: 0.7140, decode.loss_ce: 0.0196, decode.ce_acc_level_0: 37.6875, decode.ce_acc_level_1: 81.3750, decode.loss_depth: 1.4275, loss: 2.5559, grad_norm: 19.5569
2022-01-10 17:22:41,442 - depth - INFO - Saving checkpoint at 6400 iterations
2022-01-10 17:22:44,813 - depth - INFO - Iter [6400/38400]	lr: 6.033e-05, eta: 3:14:32, time: 0.373, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0188, decode.aux_loss_depth_2: 0.3552, decode.aux_loss_ce_5: 0.0196, decode.aux_loss_depth_5: 0.7098, decode.loss_ce: 0.0203, decode.ce_acc_level_0: 36.3125, decode.ce_acc_level_1: 82.1875, decode.loss_depth: 1.4221, loss: 2.5458, grad_norm: 20.9714
2022-01-10 17:22:58,251 - depth - INFO - Summary:
2022-01-10 17:22:58,254 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8076 | 0.9684 | 0.9941 |  0.1493 | 0.4704 | 0.0613 |  0.1754  | 13.9504 | 0.0938 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 17:23:01,595 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_6400.pth.
2022-01-10 17:23:01,595 - depth - INFO - Best abs_rel is 0.1493 at 6400 iter.
2022-01-10 17:23:01,597 - depth - INFO - Iter(val) [82]	a1: 0.8076, a2: 0.9684, a3: 0.9941, abs_rel: 0.14932376146316528, rmse: 0.4704008102416992, log_10: 0.061271097511053085, rmse_log: 0.17540565133094788, silog: 13.9504, sq_rel: 0.09380458295345306
2022-01-10 17:23:35,912 - depth - INFO - Iter [6500/38400]	lr: 6.162e-05, eta: 3:15:07, time: 0.511, data_time: 0.173, memory: 15411, decode.aux_loss_ce_2: 0.0190, decode.aux_loss_depth_2: 0.3542, decode.aux_loss_ce_5: 0.0199, decode.aux_loss_depth_5: 0.7108, decode.loss_ce: 0.0204, decode.ce_acc_level_0: 36.1250, decode.ce_acc_level_1: 80.6250, decode.loss_depth: 1.4189, loss: 2.5432, grad_norm: 20.3348
2022-01-10 17:24:10,454 - depth - INFO - Iter [6600/38400]	lr: 6.289e-05, eta: 3:14:20, time: 0.346, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0186, decode.aux_loss_depth_2: 0.3508, decode.aux_loss_ce_5: 0.0196, decode.aux_loss_depth_5: 0.7035, decode.loss_ce: 0.0200, decode.ce_acc_level_0: 37.9375, decode.ce_acc_level_1: 80.7500, decode.loss_depth: 1.4037, loss: 2.5162, grad_norm: 20.6055
2022-01-10 17:24:44,816 - depth - INFO - Iter [6700/38400]	lr: 6.416e-05, eta: 3:13:32, time: 0.344, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0185, decode.aux_loss_depth_2: 0.3532, decode.aux_loss_ce_5: 0.0193, decode.aux_loss_depth_5: 0.7077, decode.loss_ce: 0.0201, decode.ce_acc_level_0: 37.0000, decode.ce_acc_level_1: 80.9375, decode.loss_depth: 1.4146, loss: 2.5334, grad_norm: 19.8165
2022-01-10 17:25:19,532 - depth - INFO - Iter [6800/38400]	lr: 6.543e-05, eta: 3:12:47, time: 0.347, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0182, decode.aux_loss_depth_2: 0.3375, decode.aux_loss_ce_5: 0.0191, decode.aux_loss_depth_5: 0.6750, decode.loss_ce: 0.0200, decode.ce_acc_level_0: 39.4375, decode.ce_acc_level_1: 80.5000, decode.loss_depth: 1.3496, loss: 2.4193, grad_norm: 20.2971
2022-01-10 17:25:53,736 - depth - INFO - Iter [6900/38400]	lr: 6.668e-05, eta: 3:11:59, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0186, decode.aux_loss_depth_2: 0.3465, decode.aux_loss_ce_5: 0.0193, decode.aux_loss_depth_5: 0.6960, decode.loss_ce: 0.0197, decode.ce_acc_level_0: 38.3750, decode.ce_acc_level_1: 81.0000, decode.loss_depth: 1.3899, loss: 2.4900, grad_norm: 19.6527
2022-01-10 17:26:28,034 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 17:26:28,034 - depth - INFO - Iter [7000/38400]	lr: 6.792e-05, eta: 3:11:12, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0175, decode.aux_loss_depth_2: 0.3452, decode.aux_loss_ce_5: 0.0185, decode.aux_loss_depth_5: 0.6929, decode.loss_ce: 0.0188, decode.ce_acc_level_0: 40.8125, decode.ce_acc_level_1: 83.0000, decode.loss_depth: 1.3821, loss: 2.4751, grad_norm: 20.2308
2022-01-10 17:27:02,624 - depth - INFO - Iter [7100/38400]	lr: 6.915e-05, eta: 3:10:27, time: 0.346, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0184, decode.aux_loss_depth_2: 0.3496, decode.aux_loss_ce_5: 0.0192, decode.aux_loss_depth_5: 0.7022, decode.loss_ce: 0.0194, decode.ce_acc_level_0: 38.8125, decode.ce_acc_level_1: 83.5000, decode.loss_depth: 1.3980, loss: 2.5067, grad_norm: 17.9715
2022-01-10 17:27:36,745 - depth - INFO - Iter [7200/38400]	lr: 7.036e-05, eta: 3:09:40, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0180, decode.aux_loss_depth_2: 0.3435, decode.aux_loss_ce_5: 0.0190, decode.aux_loss_depth_5: 0.6889, decode.loss_ce: 0.0192, decode.ce_acc_level_0: 41.3750, decode.ce_acc_level_1: 83.3125, decode.loss_depth: 1.3718, loss: 2.4603, grad_norm: 20.0626
2022-01-10 17:27:50,226 - depth - INFO - Summary:
2022-01-10 17:27:50,227 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
| 0.8152 | 0.9706 | 0.994 |  0.1475 | 0.4724 | 0.0604 |  0.1731  | 13.8036 | 0.0935 |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
2022-01-10 17:27:53,769 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_7200.pth.
2022-01-10 17:27:53,769 - depth - INFO - Best abs_rel is 0.1475 at 7200 iter.
2022-01-10 17:27:53,772 - depth - INFO - Iter(val) [82]	a1: 0.8152, a2: 0.9706, a3: 0.9940, abs_rel: 0.14746278524398804, rmse: 0.4723922312259674, log_10: 0.06044688820838928, rmse_log: 0.17310963571071625, silog: 13.8036, sq_rel: 0.09346964955329895
2022-01-10 17:28:27,970 - depth - INFO - Iter [7300/38400]	lr: 7.157e-05, eta: 3:10:07, time: 0.512, data_time: 0.176, memory: 15411, decode.aux_loss_ce_2: 0.0170, decode.aux_loss_depth_2: 0.3455, decode.aux_loss_ce_5: 0.0179, decode.aux_loss_depth_5: 0.6966, decode.loss_ce: 0.0183, decode.ce_acc_level_0: 41.1875, decode.ce_acc_level_1: 84.3750, decode.loss_depth: 1.3844, loss: 2.4797, grad_norm: 18.7917
2022-01-10 17:29:01,988 - depth - INFO - Iter [7400/38400]	lr: 7.275e-05, eta: 3:09:19, time: 0.340, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0170, decode.aux_loss_depth_2: 0.3364, decode.aux_loss_ce_5: 0.0181, decode.aux_loss_depth_5: 0.6752, decode.loss_ce: 0.0184, decode.ce_acc_level_0: 43.7500, decode.ce_acc_level_1: 84.1250, decode.loss_depth: 1.3435, loss: 2.4086, grad_norm: 19.2067
2022-01-10 17:29:36,503 - depth - INFO - Iter [7500/38400]	lr: 7.393e-05, eta: 3:08:34, time: 0.345, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0177, decode.aux_loss_depth_2: 0.3446, decode.aux_loss_ce_5: 0.0185, decode.aux_loss_depth_5: 0.6915, decode.loss_ce: 0.0186, decode.ce_acc_level_0: 43.1875, decode.ce_acc_level_1: 82.8750, decode.loss_depth: 1.3768, loss: 2.4676, grad_norm: 19.0518
2022-01-10 17:30:10,920 - depth - INFO - Iter [7600/38400]	lr: 7.508e-05, eta: 3:07:48, time: 0.344, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0172, decode.aux_loss_depth_2: 0.3348, decode.aux_loss_ce_5: 0.0180, decode.aux_loss_depth_5: 0.6726, decode.loss_ce: 0.0181, decode.ce_acc_level_0: 42.3750, decode.ce_acc_level_1: 84.6250, decode.loss_depth: 1.3358, loss: 2.3966, grad_norm: 16.6923
2022-01-10 17:30:45,437 - depth - INFO - Iter [7700/38400]	lr: 7.622e-05, eta: 3:07:03, time: 0.345, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0171, decode.aux_loss_depth_2: 0.3224, decode.aux_loss_ce_5: 0.0179, decode.aux_loss_depth_5: 0.6467, decode.loss_ce: 0.0179, decode.ce_acc_level_0: 41.6250, decode.ce_acc_level_1: 85.2500, decode.loss_depth: 1.2858, loss: 2.3079, grad_norm: 17.4099
2022-01-10 17:31:19,736 - depth - INFO - Iter [7800/38400]	lr: 7.734e-05, eta: 3:06:18, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0173, decode.aux_loss_depth_2: 0.3281, decode.aux_loss_ce_5: 0.0183, decode.aux_loss_depth_5: 0.6559, decode.loss_ce: 0.0183, decode.ce_acc_level_0: 43.3125, decode.ce_acc_level_1: 82.9375, decode.loss_depth: 1.3070, loss: 2.3448, grad_norm: 18.4007
2022-01-10 17:31:53,797 - depth - INFO - Iter [7900/38400]	lr: 7.845e-05, eta: 3:05:32, time: 0.340, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0160, decode.aux_loss_depth_2: 0.3245, decode.aux_loss_ce_5: 0.0170, decode.aux_loss_depth_5: 0.6508, decode.loss_ce: 0.0169, decode.ce_acc_level_0: 45.1250, decode.ce_acc_level_1: 86.3125, decode.loss_depth: 1.2910, loss: 2.3161, grad_norm: 17.0823
2022-01-10 17:32:27,824 - depth - INFO - Saving checkpoint at 8000 iterations
2022-01-10 17:32:31,005 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 17:32:31,006 - depth - INFO - Iter [8000/38400]	lr: 7.953e-05, eta: 3:04:58, time: 0.372, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0176, decode.aux_loss_depth_2: 0.3322, decode.aux_loss_ce_5: 0.0184, decode.aux_loss_depth_5: 0.6657, decode.loss_ce: 0.0182, decode.ce_acc_level_0: 42.5625, decode.ce_acc_level_1: 82.9375, decode.loss_depth: 1.3251, loss: 2.3772, grad_norm: 16.9848
2022-01-10 17:32:44,449 - depth - INFO - Summary:
2022-01-10 17:32:44,450 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8267 | 0.9719 | 0.9948 |  0.1417 | 0.4615 | 0.0583 |  0.1676  | 13.2852 | 0.0907 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 17:32:47,940 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_8000.pth.
2022-01-10 17:32:47,940 - depth - INFO - Best abs_rel is 0.1417 at 8000 iter.
2022-01-10 17:32:47,942 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 17:32:47,942 - depth - INFO - Iter(val) [82]	a1: 0.8267, a2: 0.9719, a3: 0.9948, abs_rel: 0.14168605208396912, rmse: 0.461477667093277, log_10: 0.0583438016474247, rmse_log: 0.16758961975574493, silog: 13.2852, sq_rel: 0.09073854982852936
2022-01-10 17:33:22,201 - depth - INFO - Iter [8100/38400]	lr: 8.059e-05, eta: 3:05:16, time: 0.512, data_time: 0.175, memory: 15411, decode.aux_loss_ce_2: 0.0165, decode.aux_loss_depth_2: 0.3168, decode.aux_loss_ce_5: 0.0176, decode.aux_loss_depth_5: 0.6348, decode.loss_ce: 0.0175, decode.ce_acc_level_0: 44.6250, decode.ce_acc_level_1: 84.8125, decode.loss_depth: 1.2633, loss: 2.2666, grad_norm: 15.6166
2022-01-10 17:33:56,098 - depth - INFO - Iter [8200/38400]	lr: 8.163e-05, eta: 3:04:29, time: 0.339, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0167, decode.aux_loss_depth_2: 0.3218, decode.aux_loss_ce_5: 0.0175, decode.aux_loss_depth_5: 0.6424, decode.loss_ce: 0.0176, decode.ce_acc_level_0: 44.6250, decode.ce_acc_level_1: 84.3125, decode.loss_depth: 1.2785, loss: 2.2945, grad_norm: 17.3842
2022-01-10 17:34:30,324 - depth - INFO - Iter [8300/38400]	lr: 8.265e-05, eta: 3:03:44, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0175, decode.aux_loss_depth_2: 0.3199, decode.aux_loss_ce_5: 0.0182, decode.aux_loss_depth_5: 0.6400, decode.loss_ce: 0.0187, decode.ce_acc_level_0: 43.2500, decode.ce_acc_level_1: 82.3125, decode.loss_depth: 1.2756, loss: 2.2899, grad_norm: 17.4954
2022-01-10 17:35:04,959 - depth - INFO - Iter [8400/38400]	lr: 8.365e-05, eta: 3:03:00, time: 0.346, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0170, decode.aux_loss_depth_2: 0.3081, decode.aux_loss_ce_5: 0.0178, decode.aux_loss_depth_5: 0.6179, decode.loss_ce: 0.0186, decode.ce_acc_level_0: 42.7500, decode.ce_acc_level_1: 83.1875, decode.loss_depth: 1.2374, loss: 2.2168, grad_norm: 16.3993
2022-01-10 17:35:38,973 - depth - INFO - Iter [8500/38400]	lr: 8.462e-05, eta: 3:02:14, time: 0.340, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0160, decode.aux_loss_depth_2: 0.3110, decode.aux_loss_ce_5: 0.0170, decode.aux_loss_depth_5: 0.6224, decode.loss_ce: 0.0176, decode.ce_acc_level_0: 46.0000, decode.ce_acc_level_1: 85.0000, decode.loss_depth: 1.2418, loss: 2.2258, grad_norm: 15.1775
2022-01-10 17:36:13,057 - depth - INFO - Iter [8600/38400]	lr: 8.557e-05, eta: 3:01:29, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0154, decode.aux_loss_depth_2: 0.3203, decode.aux_loss_ce_5: 0.0163, decode.aux_loss_depth_5: 0.6417, decode.loss_ce: 0.0169, decode.ce_acc_level_0: 48.8750, decode.ce_acc_level_1: 86.0000, decode.loss_depth: 1.2839, loss: 2.2945, grad_norm: 16.3727
2022-01-10 17:36:47,356 - depth - INFO - Iter [8700/38400]	lr: 8.649e-05, eta: 3:00:45, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0161, decode.aux_loss_depth_2: 0.3153, decode.aux_loss_ce_5: 0.0169, decode.aux_loss_depth_5: 0.6307, decode.loss_ce: 0.0174, decode.ce_acc_level_0: 44.8750, decode.ce_acc_level_1: 84.8125, decode.loss_depth: 1.2579, loss: 2.2542, grad_norm: 16.0337
2022-01-10 17:37:21,382 - depth - INFO - Iter [8800/38400]	lr: 8.739e-05, eta: 3:00:00, time: 0.340, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0162, decode.aux_loss_depth_2: 0.3161, decode.aux_loss_ce_5: 0.0169, decode.aux_loss_depth_5: 0.6310, decode.loss_ce: 0.0173, decode.ce_acc_level_0: 44.4375, decode.ce_acc_level_1: 85.8750, decode.loss_depth: 1.2601, loss: 2.2576, grad_norm: 15.7689
2022-01-10 17:37:34,958 - depth - INFO - Summary:
2022-01-10 17:37:34,958 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.7887 | 0.9618 | 0.9911 |  0.1622 | 0.4851 | 0.0643 |  0.1832  | 13.8392 | 0.1067 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 17:37:34,963 - depth - INFO - Iter(val) [82]	a1: 0.7887, a2: 0.9618, a3: 0.9911, abs_rel: 0.16223034262657166, rmse: 0.48505479097366333, log_10: 0.06430897116661072, rmse_log: 0.18318286538124084, silog: 13.8392, sq_rel: 0.10669200867414474
2022-01-10 17:38:09,518 - depth - INFO - Iter [8900/38400]	lr: 8.826e-05, eta: 3:00:02, time: 0.481, data_time: 0.141, memory: 15411, decode.aux_loss_ce_2: 0.0154, decode.aux_loss_depth_2: 0.3069, decode.aux_loss_ce_5: 0.0162, decode.aux_loss_depth_5: 0.6133, decode.loss_ce: 0.0164, decode.ce_acc_level_0: 48.0000, decode.ce_acc_level_1: 87.5000, decode.loss_depth: 1.2232, loss: 2.1914, grad_norm: 15.5886
2022-01-10 17:38:43,818 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 17:38:43,818 - depth - INFO - Iter [9000/38400]	lr: 8.910e-05, eta: 2:59:18, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0160, decode.aux_loss_depth_2: 0.3104, decode.aux_loss_ce_5: 0.0167, decode.aux_loss_depth_5: 0.6211, decode.loss_ce: 0.0168, decode.ce_acc_level_0: 48.1250, decode.ce_acc_level_1: 85.6875, decode.loss_depth: 1.2393, loss: 2.2202, grad_norm: 15.5334
2022-01-10 17:39:18,067 - depth - INFO - Iter [9100/38400]	lr: 8.992e-05, eta: 2:58:34, time: 0.343, data_time: 0.009, memory: 15411, decode.aux_loss_ce_2: 0.0149, decode.aux_loss_depth_2: 0.3056, decode.aux_loss_ce_5: 0.0155, decode.aux_loss_depth_5: 0.6113, decode.loss_ce: 0.0158, decode.ce_acc_level_0: 49.8125, decode.ce_acc_level_1: 88.6250, decode.loss_depth: 1.2216, loss: 2.1847, grad_norm: 15.1283
2022-01-10 17:39:52,352 - depth - INFO - Iter [9200/38400]	lr: 9.071e-05, eta: 2:57:50, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0153, decode.aux_loss_depth_2: 0.3009, decode.aux_loss_ce_5: 0.0162, decode.aux_loss_depth_5: 0.6034, decode.loss_ce: 0.0164, decode.ce_acc_level_0: 47.8125, decode.ce_acc_level_1: 87.0625, decode.loss_depth: 1.2039, loss: 2.1560, grad_norm: 15.3236
2022-01-10 17:40:27,194 - depth - INFO - Iter [9300/38400]	lr: 9.147e-05, eta: 2:57:08, time: 0.348, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0155, decode.aux_loss_depth_2: 0.2972, decode.aux_loss_ce_5: 0.0162, decode.aux_loss_depth_5: 0.5943, decode.loss_ce: 0.0164, decode.ce_acc_level_0: 47.1250, decode.ce_acc_level_1: 86.2500, decode.loss_depth: 1.1869, loss: 2.1264, grad_norm: 15.6596
2022-01-10 17:41:01,513 - depth - INFO - Iter [9400/38400]	lr: 9.220e-05, eta: 2:56:25, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0156, decode.aux_loss_depth_2: 0.3009, decode.aux_loss_ce_5: 0.0162, decode.aux_loss_depth_5: 0.6016, decode.loss_ce: 0.0163, decode.ce_acc_level_0: 49.3750, decode.ce_acc_level_1: 86.5625, decode.loss_depth: 1.2008, loss: 2.1512, grad_norm: 14.0703
2022-01-10 17:41:35,817 - depth - INFO - Iter [9500/38400]	lr: 9.290e-05, eta: 2:55:42, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0150, decode.aux_loss_depth_2: 0.3024, decode.aux_loss_ce_5: 0.0159, decode.aux_loss_depth_5: 0.6032, decode.loss_ce: 0.0160, decode.ce_acc_level_0: 50.1250, decode.ce_acc_level_1: 86.8125, decode.loss_depth: 1.2034, loss: 2.1559, grad_norm: 13.4764
2022-01-10 17:42:10,006 - depth - INFO - Saving checkpoint at 9600 iterations
2022-01-10 17:42:13,574 - depth - INFO - Iter [9600/38400]	lr: 9.357e-05, eta: 2:55:09, time: 0.378, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0144, decode.aux_loss_depth_2: 0.3011, decode.aux_loss_ce_5: 0.0153, decode.aux_loss_depth_5: 0.6017, decode.loss_ce: 0.0157, decode.ce_acc_level_0: 49.3750, decode.ce_acc_level_1: 87.8125, decode.loss_depth: 1.2016, loss: 2.1498, grad_norm: 14.8298
2022-01-10 17:42:26,826 - depth - INFO - Summary:
2022-01-10 17:42:26,827 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8413 | 0.9741 | 0.9942 |  0.1336 | 0.4403 | 0.0556 |  0.1617  | 13.0426 | 0.0825 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 17:42:30,174 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_9600.pth.
2022-01-10 17:42:30,175 - depth - INFO - Best abs_rel is 0.1336 at 9600 iter.
2022-01-10 17:42:30,176 - depth - INFO - Iter(val) [82]	a1: 0.8413, a2: 0.9741, a3: 0.9942, abs_rel: 0.1336464136838913, rmse: 0.4403252601623535, log_10: 0.05555950105190277, rmse_log: 0.16166040301322937, silog: 13.0426, sq_rel: 0.08249708265066147
2022-01-10 17:43:04,303 - depth - INFO - Iter [9700/38400]	lr: 9.421e-05, eta: 2:55:15, time: 0.507, data_time: 0.171, memory: 15411, decode.aux_loss_ce_2: 0.0149, decode.aux_loss_depth_2: 0.2997, decode.aux_loss_ce_5: 0.0159, decode.aux_loss_depth_5: 0.5997, decode.loss_ce: 0.0159, decode.ce_acc_level_0: 49.2500, decode.ce_acc_level_1: 86.8125, decode.loss_depth: 1.1970, loss: 2.1432, grad_norm: 13.4182
2022-01-10 17:43:38,482 - depth - INFO - Iter [9800/38400]	lr: 9.481e-05, eta: 2:54:31, time: 0.342, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0139, decode.aux_loss_depth_2: 0.2951, decode.aux_loss_ce_5: 0.0148, decode.aux_loss_depth_5: 0.5894, decode.loss_ce: 0.0151, decode.ce_acc_level_0: 51.8750, decode.ce_acc_level_1: 89.0625, decode.loss_depth: 1.1772, loss: 2.1056, grad_norm: 14.0300
2022-01-10 17:44:12,634 - depth - INFO - Iter [9900/38400]	lr: 9.539e-05, eta: 2:53:47, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0150, decode.aux_loss_depth_2: 0.2924, decode.aux_loss_ce_5: 0.0159, decode.aux_loss_depth_5: 0.5835, decode.loss_ce: 0.0162, decode.ce_acc_level_0: 48.8125, decode.ce_acc_level_1: 87.0000, decode.loss_depth: 1.1610, loss: 2.0841, grad_norm: 12.8580
2022-01-10 17:44:46,948 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 17:44:46,948 - depth - INFO - Iter [10000/38400]	lr: 9.593e-05, eta: 2:53:04, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0148, decode.aux_loss_depth_2: 0.2912, decode.aux_loss_ce_5: 0.0157, decode.aux_loss_depth_5: 0.5826, decode.loss_ce: 0.0158, decode.ce_acc_level_0: 51.3125, decode.ce_acc_level_1: 87.3125, decode.loss_depth: 1.1612, loss: 2.0814, grad_norm: 13.2642
2022-01-10 17:45:21,481 - depth - INFO - Iter [10100/38400]	lr: 9.645e-05, eta: 2:52:22, time: 0.346, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0143, decode.aux_loss_depth_2: 0.2933, decode.aux_loss_ce_5: 0.0151, decode.aux_loss_depth_5: 0.5843, decode.loss_ce: 0.0154, decode.ce_acc_level_0: 51.3750, decode.ce_acc_level_1: 88.5000, decode.loss_depth: 1.1675, loss: 2.0899, grad_norm: 13.0488
2022-01-10 17:45:55,668 - depth - INFO - Iter [10200/38400]	lr: 9.692e-05, eta: 2:51:39, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0145, decode.aux_loss_depth_2: 0.2894, decode.aux_loss_ce_5: 0.0154, decode.aux_loss_depth_5: 0.5770, decode.loss_ce: 0.0156, decode.ce_acc_level_0: 49.4375, decode.ce_acc_level_1: 88.4375, decode.loss_depth: 1.1517, loss: 2.0636, grad_norm: 12.6917
2022-01-10 17:46:30,152 - depth - INFO - Iter [10300/38400]	lr: 9.737e-05, eta: 2:50:57, time: 0.345, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0137, decode.aux_loss_depth_2: 0.2920, decode.aux_loss_ce_5: 0.0145, decode.aux_loss_depth_5: 0.5824, decode.loss_ce: 0.0148, decode.ce_acc_level_0: 51.5000, decode.ce_acc_level_1: 89.6250, decode.loss_depth: 1.1618, loss: 2.0793, grad_norm: 13.0378
2022-01-10 17:47:05,167 - depth - INFO - Iter [10400/38400]	lr: 9.778e-05, eta: 2:50:16, time: 0.350, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0143, decode.aux_loss_depth_2: 0.2892, decode.aux_loss_ce_5: 0.0150, decode.aux_loss_depth_5: 0.5755, decode.loss_ce: 0.0155, decode.ce_acc_level_0: 53.0625, decode.ce_acc_level_1: 87.4375, decode.loss_depth: 1.1482, loss: 2.0577, grad_norm: 14.0788
2022-01-10 17:47:18,939 - depth - INFO - Summary:
2022-01-10 17:47:18,940 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
| 0.8266 | 0.9732 | 0.996 |  0.1379 | 0.4539 | 0.0571 |  0.1666  | 13.4719 | 0.0856 |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
2022-01-10 17:47:18,941 - depth - INFO - Iter(val) [82]	a1: 0.8266, a2: 0.9732, a3: 0.9960, abs_rel: 0.13793298602104187, rmse: 0.45386627316474915, log_10: 0.05712469667196274, rmse_log: 0.16661421954631805, silog: 13.4719, sq_rel: 0.0856451764702797
2022-01-10 17:47:53,431 - depth - INFO - Iter [10500/38400]	lr: 9.815e-05, eta: 2:50:11, time: 0.483, data_time: 0.143, memory: 15411, decode.aux_loss_ce_2: 0.0138, decode.aux_loss_depth_2: 0.2882, decode.aux_loss_ce_5: 0.0146, decode.aux_loss_depth_5: 0.5752, decode.loss_ce: 0.0148, decode.ce_acc_level_0: 52.8125, decode.ce_acc_level_1: 89.7500, decode.loss_depth: 1.1490, loss: 2.0556, grad_norm: 13.3440
2022-01-10 17:48:27,771 - depth - INFO - Iter [10600/38400]	lr: 9.850e-05, eta: 2:49:28, time: 0.343, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0136, decode.aux_loss_depth_2: 0.2804, decode.aux_loss_ce_5: 0.0144, decode.aux_loss_depth_5: 0.5604, decode.loss_ce: 0.0147, decode.ce_acc_level_0: 53.0000, decode.ce_acc_level_1: 89.5000, decode.loss_depth: 1.1169, loss: 2.0004, grad_norm: 12.7692
2022-01-10 17:49:02,173 - depth - INFO - Iter [10700/38400]	lr: 9.880e-05, eta: 2:48:46, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0131, decode.aux_loss_depth_2: 0.2834, decode.aux_loss_ce_5: 0.0138, decode.aux_loss_depth_5: 0.5668, decode.loss_ce: 0.0141, decode.ce_acc_level_0: 55.4375, decode.ce_acc_level_1: 89.9375, decode.loss_depth: 1.1320, loss: 2.0233, grad_norm: 12.4928
2022-01-10 17:49:36,354 - depth - INFO - Iter [10800/38400]	lr: 9.908e-05, eta: 2:48:04, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0131, decode.aux_loss_depth_2: 0.2765, decode.aux_loss_ce_5: 0.0143, decode.aux_loss_depth_5: 0.5538, decode.loss_ce: 0.0144, decode.ce_acc_level_0: 55.0000, decode.ce_acc_level_1: 89.2500, decode.loss_depth: 1.1058, loss: 1.9779, grad_norm: 11.7057
2022-01-10 17:50:10,369 - depth - INFO - Iter [10900/38400]	lr: 9.932e-05, eta: 2:47:21, time: 0.340, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0134, decode.aux_loss_depth_2: 0.2761, decode.aux_loss_ce_5: 0.0145, decode.aux_loss_depth_5: 0.5531, decode.loss_ce: 0.0145, decode.ce_acc_level_0: 53.1250, decode.ce_acc_level_1: 90.1250, decode.loss_depth: 1.1031, loss: 1.9748, grad_norm: 12.0494
2022-01-10 17:50:44,666 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 17:50:44,667 - depth - INFO - Iter [11000/38400]	lr: 9.952e-05, eta: 2:46:39, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0141, decode.aux_loss_depth_2: 0.2743, decode.aux_loss_ce_5: 0.0151, decode.aux_loss_depth_5: 0.5480, decode.loss_ce: 0.0154, decode.ce_acc_level_0: 50.9375, decode.ce_acc_level_1: 88.1875, decode.loss_depth: 1.0958, loss: 1.9625, grad_norm: 12.7296
2022-01-10 17:51:19,322 - depth - INFO - Iter [11100/38400]	lr: 9.969e-05, eta: 2:45:58, time: 0.346, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0134, decode.aux_loss_depth_2: 0.2721, decode.aux_loss_ce_5: 0.0142, decode.aux_loss_depth_5: 0.5438, decode.loss_ce: 0.0147, decode.ce_acc_level_0: 52.8750, decode.ce_acc_level_1: 89.2500, decode.loss_depth: 1.0884, loss: 1.9466, grad_norm: 11.6459
2022-01-10 17:51:53,699 - depth - INFO - Saving checkpoint at 11200 iterations
2022-01-10 17:51:56,869 - depth - INFO - Iter [11200/38400]	lr: 9.982e-05, eta: 2:45:24, time: 0.376, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0126, decode.aux_loss_depth_2: 0.2692, decode.aux_loss_ce_5: 0.0136, decode.aux_loss_depth_5: 0.5381, decode.loss_ce: 0.0142, decode.ce_acc_level_0: 54.6875, decode.ce_acc_level_1: 91.1875, decode.loss_depth: 1.0747, loss: 1.9223, grad_norm: 13.0984
2022-01-10 17:52:10,458 - depth - INFO - Summary:
2022-01-10 17:52:10,459 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8164 | 0.9708 | 0.9935 |  0.1525 | 0.4779 | 0.0606 |  0.1731  | 12.9255 | 0.1027 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 17:52:10,460 - depth - INFO - Iter(val) [82]	a1: 0.8164, a2: 0.9708, a3: 0.9935, abs_rel: 0.15254554152488708, rmse: 0.47793686389923096, log_10: 0.06056668236851692, rmse_log: 0.17310203611850739, silog: 12.9255, sq_rel: 0.10271761566400528
2022-01-10 17:52:44,421 - depth - INFO - Iter [11300/38400]	lr: 9.991e-05, eta: 2:45:14, time: 0.475, data_time: 0.141, memory: 15411, decode.aux_loss_ce_2: 0.0133, decode.aux_loss_depth_2: 0.2737, decode.aux_loss_ce_5: 0.0139, decode.aux_loss_depth_5: 0.5460, decode.loss_ce: 0.0144, decode.ce_acc_level_0: 53.1250, decode.ce_acc_level_1: 89.1250, decode.loss_depth: 1.0928, loss: 1.9541, grad_norm: 11.8920
2022-01-10 17:53:18,507 - depth - INFO - Iter [11400/38400]	lr: 9.997e-05, eta: 2:44:31, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0124, decode.aux_loss_depth_2: 0.2697, decode.aux_loss_ce_5: 0.0131, decode.aux_loss_depth_5: 0.5371, decode.loss_ce: 0.0138, decode.ce_acc_level_0: 55.7500, decode.ce_acc_level_1: 91.1875, decode.loss_depth: 1.0748, loss: 1.9210, grad_norm: 11.0499
2022-01-10 17:53:52,552 - depth - INFO - Iter [11500/38400]	lr: 1.000e-04, eta: 2:43:49, time: 0.340, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0123, decode.aux_loss_depth_2: 0.2752, decode.aux_loss_ce_5: 0.0129, decode.aux_loss_depth_5: 0.5463, decode.loss_ce: 0.0134, decode.ce_acc_level_0: 57.3125, decode.ce_acc_level_1: 89.9375, decode.loss_depth: 1.0898, loss: 1.9499, grad_norm: 11.7777
2022-01-10 17:54:27,001 - depth - INFO - Iter [11600/38400]	lr: 1.000e-04, eta: 2:43:08, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0130, decode.aux_loss_depth_2: 0.2655, decode.aux_loss_ce_5: 0.0136, decode.aux_loss_depth_5: 0.5294, decode.loss_ce: 0.0140, decode.ce_acc_level_0: 55.8125, decode.ce_acc_level_1: 89.6875, decode.loss_depth: 1.0560, loss: 1.8916, grad_norm: 11.6182
2022-01-10 17:55:01,514 - depth - INFO - Iter [11700/38400]	lr: 9.999e-05, eta: 2:42:26, time: 0.345, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0130, decode.aux_loss_depth_2: 0.2655, decode.aux_loss_ce_5: 0.0137, decode.aux_loss_depth_5: 0.5297, decode.loss_ce: 0.0141, decode.ce_acc_level_0: 55.3125, decode.ce_acc_level_1: 89.5625, decode.loss_depth: 1.0580, loss: 1.8941, grad_norm: 11.1401
2022-01-10 17:55:36,080 - depth - INFO - Iter [11800/38400]	lr: 9.997e-05, eta: 2:41:45, time: 0.346, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0128, decode.aux_loss_depth_2: 0.2602, decode.aux_loss_ce_5: 0.0139, decode.aux_loss_depth_5: 0.5196, decode.loss_ce: 0.0141, decode.ce_acc_level_0: 55.5625, decode.ce_acc_level_1: 89.1250, decode.loss_depth: 1.0363, loss: 1.8570, grad_norm: 11.3629
2022-01-10 17:56:10,174 - depth - INFO - Iter [11900/38400]	lr: 9.995e-05, eta: 2:41:04, time: 0.341, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0125, decode.aux_loss_depth_2: 0.2625, decode.aux_loss_ce_5: 0.0134, decode.aux_loss_depth_5: 0.5248, decode.loss_ce: 0.0135, decode.ce_acc_level_0: 57.5625, decode.ce_acc_level_1: 90.4375, decode.loss_depth: 1.0477, loss: 1.8745, grad_norm: 11.5597
2022-01-10 17:56:44,603 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 17:56:44,604 - depth - INFO - Iter [12000/38400]	lr: 9.992e-05, eta: 2:40:23, time: 0.344, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0119, decode.aux_loss_depth_2: 0.2649, decode.aux_loss_ce_5: 0.0126, decode.aux_loss_depth_5: 0.5282, decode.loss_ce: 0.0128, decode.ce_acc_level_0: 59.0625, decode.ce_acc_level_1: 92.1875, decode.loss_depth: 1.0546, loss: 1.8850, grad_norm: 11.5905
2022-01-10 17:56:58,076 - depth - INFO - Summary:
2022-01-10 17:56:58,077 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8477 | 0.9736 | 0.9934 |  0.1337 | 0.4281 | 0.0545 |  0.1596  | 12.7712 | 0.0824 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 17:56:58,078 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 17:56:58,078 - depth - INFO - Iter(val) [82]	a1: 0.8477, a2: 0.9736, a3: 0.9934, abs_rel: 0.13371899724006653, rmse: 0.4281170070171356, log_10: 0.054474879056215286, rmse_log: 0.15958207845687866, silog: 12.7712, sq_rel: 0.08244166523218155
2022-01-10 17:57:32,949 - depth - INFO - Iter [12100/38400]	lr: 9.989e-05, eta: 2:40:12, time: 0.484, data_time: 0.141, memory: 15411, decode.aux_loss_ce_2: 0.0119, decode.aux_loss_depth_2: 0.2655, decode.aux_loss_ce_5: 0.0126, decode.aux_loss_depth_5: 0.5309, decode.loss_ce: 0.0133, decode.ce_acc_level_0: 56.3750, decode.ce_acc_level_1: 91.6875, decode.loss_depth: 1.0598, loss: 1.8940, grad_norm: 11.5411
2022-01-10 17:58:07,247 - depth - INFO - Iter [12200/38400]	lr: 9.984e-05, eta: 2:39:31, time: 0.343, data_time: 0.009, memory: 15411, decode.aux_loss_ce_2: 0.0113, decode.aux_loss_depth_2: 0.2549, decode.aux_loss_ce_5: 0.0126, decode.aux_loss_depth_5: 0.5108, decode.loss_ce: 0.0127, decode.ce_acc_level_0: 57.8750, decode.ce_acc_level_1: 91.6250, decode.loss_depth: 1.0192, loss: 1.8215, grad_norm: 10.8438
2022-01-10 17:58:41,294 - depth - INFO - Iter [12300/38400]	lr: 9.979e-05, eta: 2:38:49, time: 0.340, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0114, decode.aux_loss_depth_2: 0.2592, decode.aux_loss_ce_5: 0.0124, decode.aux_loss_depth_5: 0.5178, decode.loss_ce: 0.0127, decode.ce_acc_level_0: 59.5625, decode.ce_acc_level_1: 90.8750, decode.loss_depth: 1.0330, loss: 1.8465, grad_norm: 11.0040
2022-01-10 17:59:15,933 - depth - INFO - Iter [12400/38400]	lr: 9.974e-05, eta: 2:38:08, time: 0.346, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0119, decode.aux_loss_depth_2: 0.2585, decode.aux_loss_ce_5: 0.0131, decode.aux_loss_depth_5: 0.5160, decode.loss_ce: 0.0130, decode.ce_acc_level_0: 57.5000, decode.ce_acc_level_1: 91.7500, decode.loss_depth: 1.0300, loss: 1.8425, grad_norm: 10.6900
2022-01-10 17:59:50,664 - depth - INFO - Iter [12500/38400]	lr: 9.967e-05, eta: 2:37:28, time: 0.347, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0111, decode.aux_loss_depth_2: 0.2553, decode.aux_loss_ce_5: 0.0123, decode.aux_loss_depth_5: 0.5107, decode.loss_ce: 0.0126, decode.ce_acc_level_0: 59.8125, decode.ce_acc_level_1: 92.3125, decode.loss_depth: 1.0177, loss: 1.8196, grad_norm: 10.4578
2022-01-10 18:00:25,186 - depth - INFO - Iter [12600/38400]	lr: 9.960e-05, eta: 2:36:48, time: 0.345, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0106, decode.aux_loss_depth_2: 0.2525, decode.aux_loss_ce_5: 0.0115, decode.aux_loss_depth_5: 0.5045, decode.loss_ce: 0.0117, decode.ce_acc_level_0: 63.8750, decode.ce_acc_level_1: 93.3750, decode.loss_depth: 1.0065, loss: 1.7972, grad_norm: 10.7418
2022-01-10 18:00:59,444 - depth - INFO - Iter [12700/38400]	lr: 9.953e-05, eta: 2:36:07, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0107, decode.aux_loss_depth_2: 0.2545, decode.aux_loss_ce_5: 0.0117, decode.aux_loss_depth_5: 0.5091, decode.loss_ce: 0.0123, decode.ce_acc_level_0: 61.5000, decode.ce_acc_level_1: 92.3125, decode.loss_depth: 1.0155, loss: 1.8139, grad_norm: 9.9984
2022-01-10 18:01:34,165 - depth - INFO - Saving checkpoint at 12800 iterations
2022-01-10 18:01:37,043 - depth - INFO - Iter [12800/38400]	lr: 9.944e-05, eta: 2:35:33, time: 0.376, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0113, decode.aux_loss_depth_2: 0.2487, decode.aux_loss_ce_5: 0.0121, decode.aux_loss_depth_5: 0.4957, decode.loss_ce: 0.0125, decode.ce_acc_level_0: 60.8125, decode.ce_acc_level_1: 92.0000, decode.loss_depth: 0.9884, loss: 1.7687, grad_norm: 10.0542
2022-01-10 18:01:50,441 - depth - INFO - Summary:
2022-01-10 18:01:50,442 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8505 | 0.9751 | 0.9941 |  0.1333 | 0.4312 | 0.0542 |  0.1583  | 12.4401 | 0.0852 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 18:01:53,690 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_12800.pth.
2022-01-10 18:01:53,691 - depth - INFO - Best abs_rel is 0.1333 at 12800 iter.
2022-01-10 18:01:53,692 - depth - INFO - Iter(val) [82]	a1: 0.8505, a2: 0.9751, a3: 0.9941, abs_rel: 0.13334140181541443, rmse: 0.43120232224464417, log_10: 0.0541551448404789, rmse_log: 0.15834398567676544, silog: 12.4401, sq_rel: 0.08521848171949387
2022-01-10 18:02:27,918 - depth - INFO - Iter [12900/38400]	lr: 9.935e-05, eta: 2:35:25, time: 0.509, data_time: 0.172, memory: 15411, decode.aux_loss_ce_2: 0.0120, decode.aux_loss_depth_2: 0.2491, decode.aux_loss_ce_5: 0.0131, decode.aux_loss_depth_5: 0.4966, decode.loss_ce: 0.0130, decode.ce_acc_level_0: 58.0625, decode.ce_acc_level_1: 91.3125, decode.loss_depth: 0.9906, loss: 1.7743, grad_norm: 10.9186
2022-01-10 18:03:02,222 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 18:03:02,222 - depth - INFO - Iter [13000/38400]	lr: 9.925e-05, eta: 2:34:44, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0106, decode.aux_loss_depth_2: 0.2457, decode.aux_loss_ce_5: 0.0114, decode.aux_loss_depth_5: 0.4913, decode.loss_ce: 0.0119, decode.ce_acc_level_0: 63.4375, decode.ce_acc_level_1: 91.9375, decode.loss_depth: 0.9776, loss: 1.7485, grad_norm: 10.0540
2022-01-10 18:03:36,657 - depth - INFO - Iter [13100/38400]	lr: 9.915e-05, eta: 2:34:03, time: 0.344, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0110, decode.aux_loss_depth_2: 0.2455, decode.aux_loss_ce_5: 0.0116, decode.aux_loss_depth_5: 0.4902, decode.loss_ce: 0.0119, decode.ce_acc_level_0: 61.8125, decode.ce_acc_level_1: 92.6250, decode.loss_depth: 0.9787, loss: 1.7491, grad_norm: 10.3146
2022-01-10 18:04:10,870 - depth - INFO - Iter [13200/38400]	lr: 9.904e-05, eta: 2:33:22, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0108, decode.aux_loss_depth_2: 0.2446, decode.aux_loss_ce_5: 0.0119, decode.aux_loss_depth_5: 0.4891, decode.loss_ce: 0.0117, decode.ce_acc_level_0: 62.8125, decode.ce_acc_level_1: 93.1875, decode.loss_depth: 0.9762, loss: 1.7443, grad_norm: 9.8021
2022-01-10 18:04:45,326 - depth - INFO - Iter [13300/38400]	lr: 9.892e-05, eta: 2:32:42, time: 0.345, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0107, decode.aux_loss_depth_2: 0.2500, decode.aux_loss_ce_5: 0.0117, decode.aux_loss_depth_5: 0.4996, decode.loss_ce: 0.0119, decode.ce_acc_level_0: 62.5000, decode.ce_acc_level_1: 92.1250, decode.loss_depth: 0.9968, loss: 1.7808, grad_norm: 10.5707
2022-01-10 18:05:19,821 - depth - INFO - Iter [13400/38400]	lr: 9.880e-05, eta: 2:32:02, time: 0.345, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0115, decode.aux_loss_depth_2: 0.2478, decode.aux_loss_ce_5: 0.0123, decode.aux_loss_depth_5: 0.4949, decode.loss_ce: 0.0123, decode.ce_acc_level_0: 61.5625, decode.ce_acc_level_1: 91.8125, decode.loss_depth: 0.9867, loss: 1.7655, grad_norm: 9.8045
2022-01-10 18:05:54,135 - depth - INFO - Iter [13500/38400]	lr: 9.867e-05, eta: 2:31:21, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0102, decode.aux_loss_depth_2: 0.2456, decode.aux_loss_ce_5: 0.0111, decode.aux_loss_depth_5: 0.4909, decode.loss_ce: 0.0113, decode.ce_acc_level_0: 65.6250, decode.ce_acc_level_1: 93.3125, decode.loss_depth: 0.9793, loss: 1.7485, grad_norm: 10.2889
2022-01-10 18:06:28,325 - depth - INFO - Iter [13600/38400]	lr: 9.853e-05, eta: 2:30:40, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0101, decode.aux_loss_depth_2: 0.2509, decode.aux_loss_ce_5: 0.0112, decode.aux_loss_depth_5: 0.5014, decode.loss_ce: 0.0114, decode.ce_acc_level_0: 63.8125, decode.ce_acc_level_1: 93.1250, decode.loss_depth: 0.9992, loss: 1.7844, grad_norm: 9.7822
2022-01-10 18:06:41,912 - depth - INFO - Summary:
2022-01-10 18:06:41,912 - depth - INFO - 
+--------+-------+-------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2  |   a3  | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+-------+-------+---------+--------+--------+----------+---------+--------+
| 0.8488 | 0.977 | 0.995 |  0.1324 | 0.4326 | 0.0539 |  0.1573  | 12.3607 | 0.0821 |
+--------+-------+-------+---------+--------+--------+----------+---------+--------+
2022-01-10 18:06:45,069 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_13600.pth.
2022-01-10 18:06:45,070 - depth - INFO - Best abs_rel is 0.1324 at 13600 iter.
2022-01-10 18:06:45,071 - depth - INFO - Iter(val) [82]	a1: 0.8488, a2: 0.9770, a3: 0.9950, abs_rel: 0.1323826164007187, rmse: 0.4325923025608063, log_10: 0.053897298872470856, rmse_log: 0.1573082059621811, silog: 12.3607, sq_rel: 0.08209366351366043
2022-01-10 18:07:19,307 - depth - INFO - Iter [13700/38400]	lr: 9.839e-05, eta: 2:30:30, time: 0.510, data_time: 0.175, memory: 15411, decode.aux_loss_ce_2: 0.0100, decode.aux_loss_depth_2: 0.2446, decode.aux_loss_ce_5: 0.0108, decode.aux_loss_depth_5: 0.4881, decode.loss_ce: 0.0112, decode.ce_acc_level_0: 65.3750, decode.ce_acc_level_1: 93.2500, decode.loss_depth: 0.9742, loss: 1.7390, grad_norm: 10.2734
2022-01-10 18:07:53,403 - depth - INFO - Iter [13800/38400]	lr: 9.824e-05, eta: 2:29:49, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0100, decode.aux_loss_depth_2: 0.2413, decode.aux_loss_ce_5: 0.0107, decode.aux_loss_depth_5: 0.4818, decode.loss_ce: 0.0110, decode.ce_acc_level_0: 65.9375, decode.ce_acc_level_1: 93.4375, decode.loss_depth: 0.9616, loss: 1.7163, grad_norm: 8.9282
2022-01-10 18:08:27,857 - depth - INFO - Iter [13900/38400]	lr: 9.808e-05, eta: 2:29:09, time: 0.345, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0097, decode.aux_loss_depth_2: 0.2342, decode.aux_loss_ce_5: 0.0104, decode.aux_loss_depth_5: 0.4669, decode.loss_ce: 0.0109, decode.ce_acc_level_0: 64.8750, decode.ce_acc_level_1: 94.1875, decode.loss_depth: 0.9331, loss: 1.6653, grad_norm: 9.2297
2022-01-10 18:09:01,488 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 18:09:01,488 - depth - INFO - Iter [14000/38400]	lr: 9.792e-05, eta: 2:28:27, time: 0.336, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0097, decode.aux_loss_depth_2: 0.2375, decode.aux_loss_ce_5: 0.0106, decode.aux_loss_depth_5: 0.4742, decode.loss_ce: 0.0108, decode.ce_acc_level_0: 66.0625, decode.ce_acc_level_1: 94.1250, decode.loss_depth: 0.9452, loss: 1.6879, grad_norm: 9.5477
2022-01-10 18:09:35,961 - depth - INFO - Iter [14100/38400]	lr: 9.774e-05, eta: 2:27:47, time: 0.345, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0096, decode.aux_loss_depth_2: 0.2286, decode.aux_loss_ce_5: 0.0105, decode.aux_loss_depth_5: 0.4564, decode.loss_ce: 0.0110, decode.ce_acc_level_0: 65.6875, decode.ce_acc_level_1: 93.0625, decode.loss_depth: 0.9103, loss: 1.6265, grad_norm: 10.2667
2022-01-10 18:10:10,304 - depth - INFO - Iter [14200/38400]	lr: 9.757e-05, eta: 2:27:07, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0098, decode.aux_loss_depth_2: 0.2313, decode.aux_loss_ce_5: 0.0104, decode.aux_loss_depth_5: 0.4613, decode.loss_ce: 0.0109, decode.ce_acc_level_0: 66.7500, decode.ce_acc_level_1: 93.2500, decode.loss_depth: 0.9227, loss: 1.6464, grad_norm: 8.6460
2022-01-10 18:10:44,365 - depth - INFO - Iter [14300/38400]	lr: 9.738e-05, eta: 2:26:27, time: 0.340, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0093, decode.aux_loss_depth_2: 0.2348, decode.aux_loss_ce_5: 0.0103, decode.aux_loss_depth_5: 0.4676, decode.loss_ce: 0.0109, decode.ce_acc_level_0: 66.0000, decode.ce_acc_level_1: 93.8750, decode.loss_depth: 0.9400, loss: 1.6728, grad_norm: 8.9056
2022-01-10 18:11:18,989 - depth - INFO - Saving checkpoint at 14400 iterations
2022-01-10 18:11:21,317 - depth - INFO - Iter [14400/38400]	lr: 9.720e-05, eta: 2:25:51, time: 0.370, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0091, decode.aux_loss_depth_2: 0.2323, decode.aux_loss_ce_5: 0.0100, decode.aux_loss_depth_5: 0.4649, decode.loss_ce: 0.0106, decode.ce_acc_level_0: 65.1250, decode.ce_acc_level_1: 93.8125, decode.loss_depth: 0.9304, loss: 1.6573, grad_norm: 8.7679
2022-01-10 18:11:34,468 - depth - INFO - Summary:
2022-01-10 18:11:34,468 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8562 | 0.9781 | 0.9944 |  0.1286 | 0.4233 | 0.0527 |  0.1548  | 12.4468 | 0.0804 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 18:11:36,810 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_14400.pth.
2022-01-10 18:11:36,810 - depth - INFO - Best abs_rel is 0.1286 at 14400 iter.
2022-01-10 18:11:36,811 - depth - INFO - Iter(val) [82]	a1: 0.8562, a2: 0.9781, a3: 0.9944, abs_rel: 0.1286267787218094, rmse: 0.4232959747314453, log_10: 0.05265890806913376, rmse_log: 0.15475961565971375, silog: 12.4468, sq_rel: 0.08041874319314957
2022-01-10 18:12:10,948 - depth - INFO - Iter [14500/38400]	lr: 9.700e-05, eta: 2:25:36, time: 0.496, data_time: 0.160, memory: 15411, decode.aux_loss_ce_2: 0.0092, decode.aux_loss_depth_2: 0.2318, decode.aux_loss_ce_5: 0.0101, decode.aux_loss_depth_5: 0.4633, decode.loss_ce: 0.0106, decode.ce_acc_level_0: 66.5625, decode.ce_acc_level_1: 95.1250, decode.loss_depth: 0.9265, loss: 1.6515, grad_norm: 9.0786
2022-01-10 18:12:44,870 - depth - INFO - Iter [14600/38400]	lr: 9.680e-05, eta: 2:24:55, time: 0.339, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0093, decode.aux_loss_depth_2: 0.2318, decode.aux_loss_ce_5: 0.0106, decode.aux_loss_depth_5: 0.4631, decode.loss_ce: 0.0108, decode.ce_acc_level_0: 66.4375, decode.ce_acc_level_1: 94.4375, decode.loss_depth: 0.9268, loss: 1.6523, grad_norm: 8.8706
2022-01-10 18:13:19,599 - depth - INFO - Iter [14700/38400]	lr: 9.659e-05, eta: 2:24:16, time: 0.347, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0096, decode.aux_loss_depth_2: 0.2262, decode.aux_loss_ce_5: 0.0106, decode.aux_loss_depth_5: 0.4509, decode.loss_ce: 0.0110, decode.ce_acc_level_0: 65.5625, decode.ce_acc_level_1: 92.8750, decode.loss_depth: 0.9025, loss: 1.6108, grad_norm: 8.5006
2022-01-10 18:13:53,784 - depth - INFO - Iter [14800/38400]	lr: 9.637e-05, eta: 2:23:36, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0090, decode.aux_loss_depth_2: 0.2353, decode.aux_loss_ce_5: 0.0100, decode.aux_loss_depth_5: 0.4681, decode.loss_ce: 0.0102, decode.ce_acc_level_0: 67.7500, decode.ce_acc_level_1: 95.2500, decode.loss_depth: 0.9352, loss: 1.6678, grad_norm: 9.0297
2022-01-10 18:14:27,894 - depth - INFO - Iter [14900/38400]	lr: 9.615e-05, eta: 2:22:55, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0084, decode.aux_loss_depth_2: 0.2291, decode.aux_loss_ce_5: 0.0095, decode.aux_loss_depth_5: 0.4574, decode.loss_ce: 0.0099, decode.ce_acc_level_0: 69.3750, decode.ce_acc_level_1: 94.5625, decode.loss_depth: 0.9132, loss: 1.6276, grad_norm: 9.1196
2022-01-10 18:15:02,205 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 18:15:02,206 - depth - INFO - Iter [15000/38400]	lr: 9.592e-05, eta: 2:22:15, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0083, decode.aux_loss_depth_2: 0.2277, decode.aux_loss_ce_5: 0.0094, decode.aux_loss_depth_5: 0.4552, decode.loss_ce: 0.0095, decode.ce_acc_level_0: 69.9375, decode.ce_acc_level_1: 94.9375, decode.loss_depth: 0.9080, loss: 1.6180, grad_norm: 9.4436
2022-01-10 18:15:36,498 - depth - INFO - Iter [15100/38400]	lr: 9.569e-05, eta: 2:21:36, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0078, decode.aux_loss_depth_2: 0.2241, decode.aux_loss_ce_5: 0.0088, decode.aux_loss_depth_5: 0.4467, decode.loss_ce: 0.0093, decode.ce_acc_level_0: 71.1250, decode.ce_acc_level_1: 95.5000, decode.loss_depth: 0.8914, loss: 1.5881, grad_norm: 8.2906
2022-01-10 18:16:11,279 - depth - INFO - Iter [15200/38400]	lr: 9.545e-05, eta: 2:20:57, time: 0.348, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0084, decode.aux_loss_depth_2: 0.2283, decode.aux_loss_ce_5: 0.0094, decode.aux_loss_depth_5: 0.4554, decode.loss_ce: 0.0099, decode.ce_acc_level_0: 67.3750, decode.ce_acc_level_1: 95.9375, decode.loss_depth: 0.9086, loss: 1.6200, grad_norm: 8.5854
2022-01-10 18:16:24,826 - depth - INFO - Summary:
2022-01-10 18:16:24,826 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8509 | 0.9747 | 0.9939 |  0.1373 | 0.4315 | 0.0553 |  0.1595  | 12.4708 | 0.0878 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 18:16:24,831 - depth - INFO - Iter(val) [82]	a1: 0.8509, a2: 0.9747, a3: 0.9939, abs_rel: 0.13725419342517853, rmse: 0.4315209686756134, log_10: 0.05527808889746666, rmse_log: 0.15953242778778076, silog: 12.4708, sq_rel: 0.08776000142097473
2022-01-10 18:16:59,664 - depth - INFO - Iter [15300/38400]	lr: 9.520e-05, eta: 2:20:38, time: 0.484, data_time: 0.141, memory: 15411, decode.aux_loss_ce_2: 0.0079, decode.aux_loss_depth_2: 0.2174, decode.aux_loss_ce_5: 0.0088, decode.aux_loss_depth_5: 0.4331, decode.loss_ce: 0.0093, decode.ce_acc_level_0: 71.3750, decode.ce_acc_level_1: 95.7500, decode.loss_depth: 0.8639, loss: 1.5404, grad_norm: 7.9292
2022-01-10 18:17:33,913 - depth - INFO - Iter [15400/38400]	lr: 9.495e-05, eta: 2:19:58, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0082, decode.aux_loss_depth_2: 0.2213, decode.aux_loss_ce_5: 0.0092, decode.aux_loss_depth_5: 0.4403, decode.loss_ce: 0.0095, decode.ce_acc_level_0: 69.3750, decode.ce_acc_level_1: 95.6250, decode.loss_depth: 0.8792, loss: 1.5677, grad_norm: 8.9103
2022-01-10 18:18:08,227 - depth - INFO - Iter [15500/38400]	lr: 9.469e-05, eta: 2:19:18, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0073, decode.aux_loss_depth_2: 0.2204, decode.aux_loss_ce_5: 0.0085, decode.aux_loss_depth_5: 0.4404, decode.loss_ce: 0.0087, decode.ce_acc_level_0: 73.0000, decode.ce_acc_level_1: 96.9375, decode.loss_depth: 0.8800, loss: 1.5654, grad_norm: 9.2284
2022-01-10 18:18:42,537 - depth - INFO - Iter [15600/38400]	lr: 9.442e-05, eta: 2:18:39, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0080, decode.aux_loss_depth_2: 0.2215, decode.aux_loss_ce_5: 0.0090, decode.aux_loss_depth_5: 0.4415, decode.loss_ce: 0.0095, decode.ce_acc_level_0: 70.0625, decode.ce_acc_level_1: 94.8750, decode.loss_depth: 0.8822, loss: 1.5718, grad_norm: 8.9971
2022-01-10 18:19:16,899 - depth - INFO - Iter [15700/38400]	lr: 9.415e-05, eta: 2:17:59, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0078, decode.aux_loss_depth_2: 0.2196, decode.aux_loss_ce_5: 0.0090, decode.aux_loss_depth_5: 0.4377, decode.loss_ce: 0.0090, decode.ce_acc_level_0: 72.1250, decode.ce_acc_level_1: 94.8125, decode.loss_depth: 0.8744, loss: 1.5575, grad_norm: 8.3399
2022-01-10 18:19:51,067 - depth - INFO - Iter [15800/38400]	lr: 9.388e-05, eta: 2:17:19, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0070, decode.aux_loss_depth_2: 0.2162, decode.aux_loss_ce_5: 0.0081, decode.aux_loss_depth_5: 0.4307, decode.loss_ce: 0.0082, decode.ce_acc_level_0: 74.6250, decode.ce_acc_level_1: 96.4375, decode.loss_depth: 0.8591, loss: 1.5293, grad_norm: 8.1818
2022-01-10 18:20:25,111 - depth - INFO - Iter [15900/38400]	lr: 9.359e-05, eta: 2:16:39, time: 0.340, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0071, decode.aux_loss_depth_2: 0.2150, decode.aux_loss_ce_5: 0.0081, decode.aux_loss_depth_5: 0.4290, decode.loss_ce: 0.0084, decode.ce_acc_level_0: 73.8125, decode.ce_acc_level_1: 96.2500, decode.loss_depth: 0.8554, loss: 1.5230, grad_norm: 7.9409
2022-01-10 18:20:59,164 - depth - INFO - Saving checkpoint at 16000 iterations
2022-01-10 18:21:01,459 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 18:21:01,459 - depth - INFO - Iter [16000/38400]	lr: 9.330e-05, eta: 2:16:03, time: 0.364, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0079, decode.aux_loss_depth_2: 0.2212, decode.aux_loss_ce_5: 0.0090, decode.aux_loss_depth_5: 0.4414, decode.loss_ce: 0.0091, decode.ce_acc_level_0: 70.8750, decode.ce_acc_level_1: 96.4375, decode.loss_depth: 0.8819, loss: 1.5705, grad_norm: 8.5423
2022-01-10 18:21:14,969 - depth - INFO - Summary:
2022-01-10 18:21:14,969 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8741 | 0.9803 | 0.9953 |  0.1184 | 0.4032 | 0.0495 |  0.147   | 11.9984 | 0.0703 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 18:21:17,169 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_16000.pth.
2022-01-10 18:21:17,170 - depth - INFO - Best abs_rel is 0.1184 at 16000 iter.
2022-01-10 18:21:17,171 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 18:21:17,171 - depth - INFO - Iter(val) [82]	a1: 0.8741, a2: 0.9803, a3: 0.9953, abs_rel: 0.11836576461791992, rmse: 0.40318024158477783, log_10: 0.049515996128320694, rmse_log: 0.14698652923107147, silog: 11.9984, sq_rel: 0.0703202486038208
2022-01-10 18:21:51,343 - depth - INFO - Iter [16100/38400]	lr: 9.301e-05, eta: 2:15:45, time: 0.499, data_time: 0.163, memory: 15411, decode.aux_loss_ce_2: 0.0077, decode.aux_loss_depth_2: 0.2131, decode.aux_loss_ce_5: 0.0087, decode.aux_loss_depth_5: 0.4250, decode.loss_ce: 0.0089, decode.ce_acc_level_0: 70.9375, decode.ce_acc_level_1: 96.0625, decode.loss_depth: 0.8479, loss: 1.5113, grad_norm: 8.0377
2022-01-10 18:22:25,393 - depth - INFO - Iter [16200/38400]	lr: 9.271e-05, eta: 2:15:05, time: 0.340, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0071, decode.aux_loss_depth_2: 0.2138, decode.aux_loss_ce_5: 0.0082, decode.aux_loss_depth_5: 0.4275, decode.loss_ce: 0.0084, decode.ce_acc_level_0: 74.5625, decode.ce_acc_level_1: 96.9375, decode.loss_depth: 0.8537, loss: 1.5186, grad_norm: 8.8221
2022-01-10 18:22:59,423 - depth - INFO - Iter [16300/38400]	lr: 9.240e-05, eta: 2:14:25, time: 0.340, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0077, decode.aux_loss_depth_2: 0.2152, decode.aux_loss_ce_5: 0.0087, decode.aux_loss_depth_5: 0.4291, decode.loss_ce: 0.0091, decode.ce_acc_level_0: 71.3750, decode.ce_acc_level_1: 95.6875, decode.loss_depth: 0.8558, loss: 1.5255, grad_norm: 7.8202
2022-01-10 18:23:33,738 - depth - INFO - Iter [16400/38400]	lr: 9.209e-05, eta: 2:13:46, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0069, decode.aux_loss_depth_2: 0.2144, decode.aux_loss_ce_5: 0.0081, decode.aux_loss_depth_5: 0.4280, decode.loss_ce: 0.0082, decode.ce_acc_level_0: 73.2500, decode.ce_acc_level_1: 97.1250, decode.loss_depth: 0.8543, loss: 1.5198, grad_norm: 8.9329
2022-01-10 18:24:08,111 - depth - INFO - Iter [16500/38400]	lr: 9.177e-05, eta: 2:13:07, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0068, decode.aux_loss_depth_2: 0.2140, decode.aux_loss_ce_5: 0.0082, decode.aux_loss_depth_5: 0.4276, decode.loss_ce: 0.0083, decode.ce_acc_level_0: 73.3750, decode.ce_acc_level_1: 96.4375, decode.loss_depth: 0.8520, loss: 1.5169, grad_norm: 7.8937
2022-01-10 18:24:42,366 - depth - INFO - Iter [16600/38400]	lr: 9.145e-05, eta: 2:12:27, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0065, decode.aux_loss_depth_2: 0.2123, decode.aux_loss_ce_5: 0.0075, decode.aux_loss_depth_5: 0.4242, decode.loss_ce: 0.0079, decode.ce_acc_level_0: 75.9375, decode.ce_acc_level_1: 96.9375, decode.loss_depth: 0.8466, loss: 1.5050, grad_norm: 7.7609
2022-01-10 18:25:16,804 - depth - INFO - Iter [16700/38400]	lr: 9.112e-05, eta: 2:11:48, time: 0.345, data_time: 0.008, memory: 15411, decode.aux_loss_ce_2: 0.0065, decode.aux_loss_depth_2: 0.2118, decode.aux_loss_ce_5: 0.0074, decode.aux_loss_depth_5: 0.4225, decode.loss_ce: 0.0077, decode.ce_acc_level_0: 75.8750, decode.ce_acc_level_1: 97.1875, decode.loss_depth: 0.8431, loss: 1.4990, grad_norm: 7.5131
2022-01-10 18:25:50,903 - depth - INFO - Iter [16800/38400]	lr: 9.078e-05, eta: 2:11:09, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0068, decode.aux_loss_depth_2: 0.2066, decode.aux_loss_ce_5: 0.0078, decode.aux_loss_depth_5: 0.4124, decode.loss_ce: 0.0079, decode.ce_acc_level_0: 75.6875, decode.ce_acc_level_1: 96.4375, decode.loss_depth: 0.8244, loss: 1.4659, grad_norm: 8.1602
2022-01-10 18:26:04,372 - depth - INFO - Summary:
2022-01-10 18:26:04,372 - depth - INFO - 
+-------+--------+--------+---------+-------+--------+----------+---------+--------+
|   a1  |   a2   |   a3   | abs_rel |  rmse | log_10 | rmse_log |  silog  | sq_rel |
+-------+--------+--------+---------+-------+--------+----------+---------+--------+
| 0.869 | 0.9776 | 0.9949 |  0.1251 | 0.411 | 0.0511 |   0.15   | 11.9412 | 0.0752 |
+-------+--------+--------+---------+-------+--------+----------+---------+--------+
2022-01-10 18:26:04,373 - depth - INFO - Iter(val) [82]	a1: 0.8690, a2: 0.9776, a3: 0.9949, abs_rel: 0.1250918209552765, rmse: 0.4110431671142578, log_10: 0.05112744867801666, rmse_log: 0.1500338613986969, silog: 11.9412, sq_rel: 0.07515135407447815
2022-01-10 18:26:38,566 - depth - INFO - Iter [16900/38400]	lr: 9.044e-05, eta: 2:10:46, time: 0.476, data_time: 0.140, memory: 15411, decode.aux_loss_ce_2: 0.0066, decode.aux_loss_depth_2: 0.2092, decode.aux_loss_ce_5: 0.0074, decode.aux_loss_depth_5: 0.4169, decode.loss_ce: 0.0076, decode.ce_acc_level_0: 76.0000, decode.ce_acc_level_1: 96.8125, decode.loss_depth: 0.8313, loss: 1.4791, grad_norm: 8.3584
2022-01-10 18:27:12,600 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 18:27:12,600 - depth - INFO - Iter [17000/38400]	lr: 9.009e-05, eta: 2:10:07, time: 0.340, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0064, decode.aux_loss_depth_2: 0.2045, decode.aux_loss_ce_5: 0.0073, decode.aux_loss_depth_5: 0.4085, decode.loss_ce: 0.0076, decode.ce_acc_level_0: 75.1250, decode.ce_acc_level_1: 96.8125, decode.loss_depth: 0.8151, loss: 1.4496, grad_norm: 7.6032
2022-01-10 18:27:46,818 - depth - INFO - Iter [17100/38400]	lr: 8.974e-05, eta: 2:09:28, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0061, decode.aux_loss_depth_2: 0.2050, decode.aux_loss_ce_5: 0.0067, decode.aux_loss_depth_5: 0.4091, decode.loss_ce: 0.0069, decode.ce_acc_level_0: 79.2500, decode.ce_acc_level_1: 97.0000, decode.loss_depth: 0.8158, loss: 1.4496, grad_norm: 7.5865
2022-01-10 18:28:20,757 - depth - INFO - Iter [17200/38400]	lr: 8.939e-05, eta: 2:08:48, time: 0.339, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0060, decode.aux_loss_depth_2: 0.2053, decode.aux_loss_ce_5: 0.0067, decode.aux_loss_depth_5: 0.4091, decode.loss_ce: 0.0070, decode.ce_acc_level_0: 77.0000, decode.ce_acc_level_1: 97.5625, decode.loss_depth: 0.8157, loss: 1.4500, grad_norm: 8.2669
2022-01-10 18:28:54,983 - depth - INFO - Iter [17300/38400]	lr: 8.902e-05, eta: 2:08:09, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0058, decode.aux_loss_depth_2: 0.2048, decode.aux_loss_ce_5: 0.0065, decode.aux_loss_depth_5: 0.4080, decode.loss_ce: 0.0071, decode.ce_acc_level_0: 78.5000, decode.ce_acc_level_1: 97.3125, decode.loss_depth: 0.8147, loss: 1.4469, grad_norm: 7.8468
2022-01-10 18:29:29,456 - depth - INFO - Iter [17400/38400]	lr: 8.866e-05, eta: 2:07:30, time: 0.345, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0056, decode.aux_loss_depth_2: 0.2029, decode.aux_loss_ce_5: 0.0067, decode.aux_loss_depth_5: 0.4054, decode.loss_ce: 0.0069, decode.ce_acc_level_0: 77.6250, decode.ce_acc_level_1: 97.5625, decode.loss_depth: 0.8099, loss: 1.4374, grad_norm: 7.8639
2022-01-10 18:30:03,403 - depth - INFO - Iter [17500/38400]	lr: 8.828e-05, eta: 2:06:51, time: 0.340, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0062, decode.aux_loss_depth_2: 0.2046, decode.aux_loss_ce_5: 0.0070, decode.aux_loss_depth_5: 0.4076, decode.loss_ce: 0.0074, decode.ce_acc_level_0: 77.8125, decode.ce_acc_level_1: 97.1875, decode.loss_depth: 0.8144, loss: 1.4471, grad_norm: 7.5775
2022-01-10 18:30:37,499 - depth - INFO - Saving checkpoint at 17600 iterations
2022-01-10 18:30:39,780 - depth - INFO - Iter [17600/38400]	lr: 8.790e-05, eta: 2:06:14, time: 0.364, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0059, decode.aux_loss_depth_2: 0.2100, decode.aux_loss_ce_5: 0.0065, decode.aux_loss_depth_5: 0.4187, decode.loss_ce: 0.0069, decode.ce_acc_level_0: 78.1250, decode.ce_acc_level_1: 98.0625, decode.loss_depth: 0.8367, loss: 1.4847, grad_norm: 7.5292
2022-01-10 18:30:53,068 - depth - INFO - Summary:
2022-01-10 18:30:53,069 - depth - INFO - 
+-------+--------+--------+---------+-------+--------+----------+--------+--------+
|   a1  |   a2   |   a3   | abs_rel |  rmse | log_10 | rmse_log | silog  | sq_rel |
+-------+--------+--------+---------+-------+--------+----------+--------+--------+
| 0.863 | 0.9767 | 0.9945 |  0.1268 | 0.414 | 0.0519 |  0.1526  | 12.112 | 0.0782 |
+-------+--------+--------+---------+-------+--------+----------+--------+--------+
2022-01-10 18:30:53,070 - depth - INFO - Iter(val) [82]	a1: 0.8630, a2: 0.9767, a3: 0.9945, abs_rel: 0.12684206664562225, rmse: 0.41400378942489624, log_10: 0.051910609006881714, rmse_log: 0.1525999754667282, silog: 12.1120, sq_rel: 0.0782114639878273
2022-01-10 18:31:27,298 - depth - INFO - Iter [17700/38400]	lr: 8.752e-05, eta: 2:05:51, time: 0.475, data_time: 0.139, memory: 15411, decode.aux_loss_ce_2: 0.0053, decode.aux_loss_depth_2: 0.2055, decode.aux_loss_ce_5: 0.0063, decode.aux_loss_depth_5: 0.4098, decode.loss_ce: 0.0066, decode.ce_acc_level_0: 78.3750, decode.ce_acc_level_1: 98.0625, decode.loss_depth: 0.8188, loss: 1.4523, grad_norm: 8.5771
2022-01-10 18:32:01,558 - depth - INFO - Iter [17800/38400]	lr: 8.713e-05, eta: 2:05:12, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0055, decode.aux_loss_depth_2: 0.2054, decode.aux_loss_ce_5: 0.0066, decode.aux_loss_depth_5: 0.4103, decode.loss_ce: 0.0068, decode.ce_acc_level_0: 79.1250, decode.ce_acc_level_1: 97.5000, decode.loss_depth: 0.8194, loss: 1.4540, grad_norm: 8.1871
2022-01-10 18:32:35,501 - depth - INFO - Iter [17900/38400]	lr: 8.674e-05, eta: 2:04:32, time: 0.339, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0062, decode.aux_loss_depth_2: 0.2039, decode.aux_loss_ce_5: 0.0071, decode.aux_loss_depth_5: 0.4073, decode.loss_ce: 0.0075, decode.ce_acc_level_0: 76.9375, decode.ce_acc_level_1: 97.3750, decode.loss_depth: 0.8131, loss: 1.4450, grad_norm: 7.6180
2022-01-10 18:33:09,572 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 18:33:09,573 - depth - INFO - Iter [18000/38400]	lr: 8.634e-05, eta: 2:03:53, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0056, decode.aux_loss_depth_2: 0.2003, decode.aux_loss_ce_5: 0.0065, decode.aux_loss_depth_5: 0.4005, decode.loss_ce: 0.0068, decode.ce_acc_level_0: 77.8750, decode.ce_acc_level_1: 98.0625, decode.loss_depth: 0.7995, loss: 1.4193, grad_norm: 7.5270
2022-01-10 18:33:43,945 - depth - INFO - Iter [18100/38400]	lr: 8.593e-05, eta: 2:03:14, time: 0.344, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0055, decode.aux_loss_depth_2: 0.1982, decode.aux_loss_ce_5: 0.0067, decode.aux_loss_depth_5: 0.3953, decode.loss_ce: 0.0073, decode.ce_acc_level_0: 77.0000, decode.ce_acc_level_1: 97.1875, decode.loss_depth: 0.7896, loss: 1.4025, grad_norm: 7.0265
2022-01-10 18:34:17,991 - depth - INFO - Iter [18200/38400]	lr: 8.553e-05, eta: 2:02:35, time: 0.341, data_time: 0.009, memory: 15411, decode.aux_loss_ce_2: 0.0054, decode.aux_loss_depth_2: 0.1988, decode.aux_loss_ce_5: 0.0060, decode.aux_loss_depth_5: 0.3972, decode.loss_ce: 0.0065, decode.ce_acc_level_0: 79.5000, decode.ce_acc_level_1: 97.6250, decode.loss_depth: 0.7936, loss: 1.4076, grad_norm: 7.4873
2022-01-10 18:34:51,861 - depth - INFO - Iter [18300/38400]	lr: 8.511e-05, eta: 2:01:56, time: 0.338, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0057, decode.aux_loss_depth_2: 0.1987, decode.aux_loss_ce_5: 0.0067, decode.aux_loss_depth_5: 0.3963, decode.loss_ce: 0.0072, decode.ce_acc_level_0: 77.3125, decode.ce_acc_level_1: 97.4375, decode.loss_depth: 0.7920, loss: 1.4066, grad_norm: 7.3855
2022-01-10 18:35:25,922 - depth - INFO - Iter [18400/38400]	lr: 8.469e-05, eta: 2:01:17, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0057, decode.aux_loss_depth_2: 0.1949, decode.aux_loss_ce_5: 0.0067, decode.aux_loss_depth_5: 0.3892, decode.loss_ce: 0.0072, decode.ce_acc_level_0: 78.1875, decode.ce_acc_level_1: 97.0000, decode.loss_depth: 0.7782, loss: 1.3818, grad_norm: 7.2838
2022-01-10 18:35:39,286 - depth - INFO - Summary:
2022-01-10 18:35:39,287 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.8761 | 0.9793 | 0.9951 |  0.1178 | 0.4022 | 0.0493 |  0.146   | 11.848 | 0.0695 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-01-10 18:35:41,677 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_18400.pth.
2022-01-10 18:35:41,677 - depth - INFO - Best abs_rel is 0.1178 at 18400 iter.
2022-01-10 18:35:41,678 - depth - INFO - Iter(val) [82]	a1: 0.8761, a2: 0.9793, a3: 0.9951, abs_rel: 0.11778290569782257, rmse: 0.4022494852542877, log_10: 0.04925171285867691, rmse_log: 0.14600637555122375, silog: 11.8480, sq_rel: 0.06950189173221588
2022-01-10 18:36:16,077 - depth - INFO - Iter [18500/38400]	lr: 8.427e-05, eta: 2:00:56, time: 0.501, data_time: 0.163, memory: 15411, decode.aux_loss_ce_2: 0.0048, decode.aux_loss_depth_2: 0.1917, decode.aux_loss_ce_5: 0.0055, decode.aux_loss_depth_5: 0.3820, decode.loss_ce: 0.0063, decode.ce_acc_level_0: 80.1250, decode.ce_acc_level_1: 98.0625, decode.loss_depth: 0.7634, loss: 1.3537, grad_norm: 7.1413
2022-01-10 18:36:50,286 - depth - INFO - Iter [18600/38400]	lr: 8.384e-05, eta: 2:00:17, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0053, decode.aux_loss_depth_2: 0.1941, decode.aux_loss_ce_5: 0.0063, decode.aux_loss_depth_5: 0.3872, decode.loss_ce: 0.0067, decode.ce_acc_level_0: 78.5000, decode.ce_acc_level_1: 97.3125, decode.loss_depth: 0.7729, loss: 1.3726, grad_norm: 7.0963
2022-01-10 18:37:24,602 - depth - INFO - Iter [18700/38400]	lr: 8.341e-05, eta: 1:59:38, time: 0.344, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0049, decode.aux_loss_depth_2: 0.1939, decode.aux_loss_ce_5: 0.0056, decode.aux_loss_depth_5: 0.3874, decode.loss_ce: 0.0064, decode.ce_acc_level_0: 79.5000, decode.ce_acc_level_1: 98.0000, decode.loss_depth: 0.7728, loss: 1.3710, grad_norm: 7.7345
2022-01-10 18:37:59,300 - depth - INFO - Iter [18800/38400]	lr: 8.297e-05, eta: 1:59:00, time: 0.347, data_time: 0.009, memory: 15411, decode.aux_loss_ce_2: 0.0049, decode.aux_loss_depth_2: 0.1938, decode.aux_loss_ce_5: 0.0056, decode.aux_loss_depth_5: 0.3861, decode.loss_ce: 0.0061, decode.ce_acc_level_0: 80.5000, decode.ce_acc_level_1: 98.6250, decode.loss_depth: 0.7716, loss: 1.3682, grad_norm: 6.9935
2022-01-10 18:38:33,609 - depth - INFO - Iter [18900/38400]	lr: 8.253e-05, eta: 1:58:21, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0046, decode.aux_loss_depth_2: 0.1932, decode.aux_loss_ce_5: 0.0055, decode.aux_loss_depth_5: 0.3859, decode.loss_ce: 0.0061, decode.ce_acc_level_0: 80.8125, decode.ce_acc_level_1: 98.0000, decode.loss_depth: 0.7696, loss: 1.3650, grad_norm: 7.1638
2022-01-10 18:39:07,989 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 18:39:07,990 - depth - INFO - Iter [19000/38400]	lr: 8.209e-05, eta: 1:57:43, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0042, decode.aux_loss_depth_2: 0.1928, decode.aux_loss_ce_5: 0.0049, decode.aux_loss_depth_5: 0.3846, decode.loss_ce: 0.0055, decode.ce_acc_level_0: 82.8750, decode.ce_acc_level_1: 98.2500, decode.loss_depth: 0.7697, loss: 1.3618, grad_norm: 6.9178
2022-01-10 18:39:42,213 - depth - INFO - Iter [19100/38400]	lr: 8.164e-05, eta: 1:57:04, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0047, decode.aux_loss_depth_2: 0.1920, decode.aux_loss_ce_5: 0.0055, decode.aux_loss_depth_5: 0.3828, decode.loss_ce: 0.0063, decode.ce_acc_level_0: 79.2500, decode.ce_acc_level_1: 98.2500, decode.loss_depth: 0.7662, loss: 1.3574, grad_norm: 6.6520
2022-01-10 18:40:16,628 - depth - INFO - Saving checkpoint at 19200 iterations
2022-01-10 18:40:19,283 - depth - INFO - Iter [19200/38400]	lr: 8.118e-05, eta: 1:56:28, time: 0.371, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0048, decode.aux_loss_depth_2: 0.1903, decode.aux_loss_ce_5: 0.0056, decode.aux_loss_depth_5: 0.3803, decode.loss_ce: 0.0063, decode.ce_acc_level_0: 80.1875, decode.ce_acc_level_1: 98.0000, decode.loss_depth: 0.7598, loss: 1.3470, grad_norm: 6.8380
2022-01-10 18:40:32,472 - depth - INFO - Summary:
2022-01-10 18:40:32,477 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8787 | 0.9808 | 0.9951 |  0.1162 | 0.4019 | 0.0487 |  0.1445  | 11.7714 | 0.0704 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 18:40:34,753 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_19200.pth.
2022-01-10 18:40:34,753 - depth - INFO - Best abs_rel is 0.1162 at 19200 iter.
2022-01-10 18:40:34,755 - depth - INFO - Iter(val) [82]	a1: 0.8787, a2: 0.9808, a3: 0.9951, abs_rel: 0.11624632030725479, rmse: 0.4018772542476654, log_10: 0.04870914667844772, rmse_log: 0.1445368081331253, silog: 11.7714, sq_rel: 0.07036357372999191
2022-01-10 18:41:08,796 - depth - INFO - Iter [19300/38400]	lr: 8.072e-05, eta: 1:56:05, time: 0.495, data_time: 0.161, memory: 15411, decode.aux_loss_ce_2: 0.0045, decode.aux_loss_depth_2: 0.1879, decode.aux_loss_ce_5: 0.0051, decode.aux_loss_depth_5: 0.3744, decode.loss_ce: 0.0058, decode.ce_acc_level_0: 81.8125, decode.ce_acc_level_1: 98.2500, decode.loss_depth: 0.7497, loss: 1.3275, grad_norm: 7.4541
2022-01-10 18:41:43,113 - depth - INFO - Iter [19400/38400]	lr: 8.026e-05, eta: 1:55:26, time: 0.343, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0048, decode.aux_loss_depth_2: 0.1925, decode.aux_loss_ce_5: 0.0057, decode.aux_loss_depth_5: 0.3843, decode.loss_ce: 0.0063, decode.ce_acc_level_0: 81.7500, decode.ce_acc_level_1: 97.9375, decode.loss_depth: 0.7672, loss: 1.3608, grad_norm: 7.2446
2022-01-10 18:42:17,301 - depth - INFO - Iter [19500/38400]	lr: 7.979e-05, eta: 1:54:48, time: 0.342, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0043, decode.aux_loss_depth_2: 0.1888, decode.aux_loss_ce_5: 0.0049, decode.aux_loss_depth_5: 0.3769, decode.loss_ce: 0.0055, decode.ce_acc_level_0: 83.5000, decode.ce_acc_level_1: 98.3750, decode.loss_depth: 0.7527, loss: 1.3332, grad_norm: 6.7698
2022-01-10 18:42:51,432 - depth - INFO - Iter [19600/38400]	lr: 7.932e-05, eta: 1:54:09, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0037, decode.aux_loss_depth_2: 0.1885, decode.aux_loss_ce_5: 0.0046, decode.aux_loss_depth_5: 0.3763, decode.loss_ce: 0.0050, decode.ce_acc_level_0: 84.5000, decode.ce_acc_level_1: 98.8750, decode.loss_depth: 0.7518, loss: 1.3299, grad_norm: 7.1550
2022-01-10 18:43:26,288 - depth - INFO - Iter [19700/38400]	lr: 7.885e-05, eta: 1:53:31, time: 0.349, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0043, decode.aux_loss_depth_2: 0.1879, decode.aux_loss_ce_5: 0.0049, decode.aux_loss_depth_5: 0.3752, decode.loss_ce: 0.0053, decode.ce_acc_level_0: 83.8125, decode.ce_acc_level_1: 98.5625, decode.loss_depth: 0.7487, loss: 1.3262, grad_norm: 7.0960
2022-01-10 18:44:00,914 - depth - INFO - Iter [19800/38400]	lr: 7.837e-05, eta: 1:52:53, time: 0.346, data_time: 0.010, memory: 15411, decode.aux_loss_ce_2: 0.0044, decode.aux_loss_depth_2: 0.1879, decode.aux_loss_ce_5: 0.0051, decode.aux_loss_depth_5: 0.3747, decode.loss_ce: 0.0055, decode.ce_acc_level_0: 83.1875, decode.ce_acc_level_1: 98.3125, decode.loss_depth: 0.7477, loss: 1.3254, grad_norm: 7.2021
2022-01-10 18:44:35,275 - depth - INFO - Iter [19900/38400]	lr: 7.788e-05, eta: 1:52:15, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0043, decode.aux_loss_depth_2: 0.1868, decode.aux_loss_ce_5: 0.0049, decode.aux_loss_depth_5: 0.3729, decode.loss_ce: 0.0053, decode.ce_acc_level_0: 83.7500, decode.ce_acc_level_1: 98.6250, decode.loss_depth: 0.7435, loss: 1.3177, grad_norm: 6.5989
2022-01-10 18:45:09,945 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 18:45:09,945 - depth - INFO - Iter [20000/38400]	lr: 7.740e-05, eta: 1:51:37, time: 0.347, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0031, decode.aux_loss_depth_2: 0.1861, decode.aux_loss_ce_5: 0.0038, decode.aux_loss_depth_5: 0.3718, decode.loss_ce: 0.0043, decode.ce_acc_level_0: 87.0625, decode.ce_acc_level_1: 99.3750, decode.loss_depth: 0.7411, loss: 1.3103, grad_norm: 6.8552
2022-01-10 18:45:23,159 - depth - INFO - Summary:
2022-01-10 18:45:23,159 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8691 | 0.9797 | 0.9951 |  0.1231 | 0.4042 | 0.0503 |  0.1477  | 11.6212 | 0.0748 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 18:45:23,160 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 18:45:23,160 - depth - INFO - Iter(val) [82]	a1: 0.8691, a2: 0.9797, a3: 0.9951, abs_rel: 0.1230786144733429, rmse: 0.4042254388332367, log_10: 0.050305843353271484, rmse_log: 0.14766322076320648, silog: 11.6212, sq_rel: 0.07479964196681976
2022-01-10 18:45:57,661 - depth - INFO - Iter [20100/38400]	lr: 7.691e-05, eta: 1:51:11, time: 0.477, data_time: 0.137, memory: 15411, decode.aux_loss_ce_2: 0.0039, decode.aux_loss_depth_2: 0.1818, decode.aux_loss_ce_5: 0.0046, decode.aux_loss_depth_5: 0.3630, decode.loss_ce: 0.0050, decode.ce_acc_level_0: 84.9375, decode.ce_acc_level_1: 98.9375, decode.loss_depth: 0.7242, loss: 1.2824, grad_norm: 6.6799
2022-01-10 18:46:32,023 - depth - INFO - Iter [20200/38400]	lr: 7.641e-05, eta: 1:50:32, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0042, decode.aux_loss_depth_2: 0.1842, decode.aux_loss_ce_5: 0.0051, decode.aux_loss_depth_5: 0.3674, decode.loss_ce: 0.0053, decode.ce_acc_level_0: 83.0625, decode.ce_acc_level_1: 98.1250, decode.loss_depth: 0.7343, loss: 1.3005, grad_norm: 7.2732
2022-01-10 18:47:06,179 - depth - INFO - Iter [20300/38400]	lr: 7.591e-05, eta: 1:49:54, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0038, decode.aux_loss_depth_2: 0.1817, decode.aux_loss_ce_5: 0.0045, decode.aux_loss_depth_5: 0.3626, decode.loss_ce: 0.0048, decode.ce_acc_level_0: 85.3750, decode.ce_acc_level_1: 98.8125, decode.loss_depth: 0.7241, loss: 1.2814, grad_norm: 6.7914
2022-01-10 18:47:40,172 - depth - INFO - Iter [20400/38400]	lr: 7.541e-05, eta: 1:49:15, time: 0.340, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0038, decode.aux_loss_depth_2: 0.1841, decode.aux_loss_ce_5: 0.0045, decode.aux_loss_depth_5: 0.3681, decode.loss_ce: 0.0049, decode.ce_acc_level_0: 85.0625, decode.ce_acc_level_1: 98.8750, decode.loss_depth: 0.7345, loss: 1.2998, grad_norm: 6.5441
2022-01-10 18:48:14,335 - depth - INFO - Iter [20500/38400]	lr: 7.491e-05, eta: 1:48:37, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0035, decode.aux_loss_depth_2: 0.1857, decode.aux_loss_ce_5: 0.0042, decode.aux_loss_depth_5: 0.3704, decode.loss_ce: 0.0042, decode.ce_acc_level_0: 86.9375, decode.ce_acc_level_1: 99.1250, decode.loss_depth: 0.7398, loss: 1.3078, grad_norm: 6.9410
2022-01-10 18:48:48,399 - depth - INFO - Iter [20600/38400]	lr: 7.440e-05, eta: 1:47:58, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0035, decode.aux_loss_depth_2: 0.1820, decode.aux_loss_ce_5: 0.0040, decode.aux_loss_depth_5: 0.3633, decode.loss_ce: 0.0044, decode.ce_acc_level_0: 86.1250, decode.ce_acc_level_1: 99.0000, decode.loss_depth: 0.7250, loss: 1.2822, grad_norm: 7.5071
2022-01-10 18:49:22,999 - depth - INFO - Iter [20700/38400]	lr: 7.389e-05, eta: 1:47:20, time: 0.346, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0035, decode.aux_loss_depth_2: 0.1782, decode.aux_loss_ce_5: 0.0040, decode.aux_loss_depth_5: 0.3555, decode.loss_ce: 0.0044, decode.ce_acc_level_0: 85.6250, decode.ce_acc_level_1: 98.5625, decode.loss_depth: 0.7099, loss: 1.2556, grad_norm: 6.4073
2022-01-10 18:49:57,546 - depth - INFO - Saving checkpoint at 20800 iterations
2022-01-10 18:50:00,261 - depth - INFO - Iter [20800/38400]	lr: 7.337e-05, eta: 1:46:45, time: 0.373, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0039, decode.aux_loss_depth_2: 0.1819, decode.aux_loss_ce_5: 0.0046, decode.aux_loss_depth_5: 0.3633, decode.loss_ce: 0.0049, decode.ce_acc_level_0: 83.7500, decode.ce_acc_level_1: 99.1250, decode.loss_depth: 0.7261, loss: 1.2847, grad_norm: 6.8305
2022-01-10 18:50:13,631 - depth - INFO - Summary:
2022-01-10 18:50:13,632 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8756 | 0.9807 | 0.9955 |  0.1157 | 0.3937 | 0.0485 |  0.1436  | 11.5405 | 0.0684 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 18:50:16,189 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_20800.pth.
2022-01-10 18:50:16,189 - depth - INFO - Best abs_rel is 0.1157 at 20800 iter.
2022-01-10 18:50:16,190 - depth - INFO - Iter(val) [82]	a1: 0.8756, a2: 0.9807, a3: 0.9955, abs_rel: 0.11574148386716843, rmse: 0.3936806917190552, log_10: 0.0484602116048336, rmse_log: 0.14359372854232788, silog: 11.5405, sq_rel: 0.06839391589164734
2022-01-10 18:50:50,325 - depth - INFO - Iter [20900/38400]	lr: 7.286e-05, eta: 1:46:20, time: 0.500, data_time: 0.165, memory: 15411, decode.aux_loss_ce_2: 0.0031, decode.aux_loss_depth_2: 0.1791, decode.aux_loss_ce_5: 0.0037, decode.aux_loss_depth_5: 0.3577, decode.loss_ce: 0.0041, decode.ce_acc_level_0: 86.6875, decode.ce_acc_level_1: 98.8750, decode.loss_depth: 0.7150, loss: 1.2626, grad_norm: 6.9884
2022-01-10 18:51:24,588 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 18:51:24,589 - depth - INFO - Iter [21000/38400]	lr: 7.233e-05, eta: 1:45:41, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0035, decode.aux_loss_depth_2: 0.1782, decode.aux_loss_ce_5: 0.0040, decode.aux_loss_depth_5: 0.3556, decode.loss_ce: 0.0046, decode.ce_acc_level_0: 85.9375, decode.ce_acc_level_1: 98.8750, decode.loss_depth: 0.7091, loss: 1.2549, grad_norm: 6.8010
2022-01-10 18:51:58,803 - depth - INFO - Iter [21100/38400]	lr: 7.181e-05, eta: 1:45:03, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0030, decode.aux_loss_depth_2: 0.1836, decode.aux_loss_ce_5: 0.0035, decode.aux_loss_depth_5: 0.3661, decode.loss_ce: 0.0039, decode.ce_acc_level_0: 87.4375, decode.ce_acc_level_1: 99.3750, decode.loss_depth: 0.7311, loss: 1.2912, grad_norm: 6.6025
2022-01-10 18:52:33,433 - depth - INFO - Iter [21200/38400]	lr: 7.128e-05, eta: 1:44:25, time: 0.346, data_time: 0.008, memory: 15411, decode.aux_loss_ce_2: 0.0035, decode.aux_loss_depth_2: 0.1755, decode.aux_loss_ce_5: 0.0041, decode.aux_loss_depth_5: 0.3494, decode.loss_ce: 0.0047, decode.ce_acc_level_0: 85.8750, decode.ce_acc_level_1: 98.4375, decode.loss_depth: 0.6971, loss: 1.2343, grad_norm: 6.3549
2022-01-10 18:53:07,580 - depth - INFO - Iter [21300/38400]	lr: 7.075e-05, eta: 1:43:47, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0035, decode.aux_loss_depth_2: 0.1787, decode.aux_loss_ce_5: 0.0040, decode.aux_loss_depth_5: 0.3568, decode.loss_ce: 0.0045, decode.ce_acc_level_0: 86.6250, decode.ce_acc_level_1: 99.0625, decode.loss_depth: 0.7131, loss: 1.2605, grad_norm: 6.3405
2022-01-10 18:53:41,666 - depth - INFO - Iter [21400/38400]	lr: 7.022e-05, eta: 1:43:09, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0034, decode.aux_loss_depth_2: 0.1744, decode.aux_loss_ce_5: 0.0038, decode.aux_loss_depth_5: 0.3482, decode.loss_ce: 0.0041, decode.ce_acc_level_0: 87.8750, decode.ce_acc_level_1: 99.1875, decode.loss_depth: 0.6957, loss: 1.2297, grad_norm: 6.5175
2022-01-10 18:54:15,791 - depth - INFO - Iter [21500/38400]	lr: 6.968e-05, eta: 1:42:31, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0029, decode.aux_loss_depth_2: 0.1778, decode.aux_loss_ce_5: 0.0035, decode.aux_loss_depth_5: 0.3550, decode.loss_ce: 0.0038, decode.ce_acc_level_0: 87.5625, decode.ce_acc_level_1: 99.5625, decode.loss_depth: 0.7088, loss: 1.2517, grad_norm: 6.2666
2022-01-10 18:54:49,884 - depth - INFO - Iter [21600/38400]	lr: 6.915e-05, eta: 1:41:52, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0029, decode.aux_loss_depth_2: 0.1721, decode.aux_loss_ce_5: 0.0035, decode.aux_loss_depth_5: 0.3430, decode.loss_ce: 0.0039, decode.ce_acc_level_0: 87.0625, decode.ce_acc_level_1: 99.3125, decode.loss_depth: 0.6851, loss: 1.2104, grad_norm: 6.5322
2022-01-10 18:55:03,331 - depth - INFO - Summary:
2022-01-10 18:55:03,332 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8763 | 0.9799 | 0.9953 |  0.1209 | 0.3965 | 0.0495 |  0.1455  | 11.5049 | 0.0715 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 18:55:03,333 - depth - INFO - Iter(val) [82]	a1: 0.8763, a2: 0.9799, a3: 0.9953, abs_rel: 0.12091031670570374, rmse: 0.3965153694152832, log_10: 0.049515802413225174, rmse_log: 0.14552167057991028, silog: 11.5049, sq_rel: 0.07145944237709045
2022-01-10 18:55:37,975 - depth - INFO - Iter [21700/38400]	lr: 6.861e-05, eta: 1:41:25, time: 0.481, data_time: 0.140, memory: 15411, decode.aux_loss_ce_2: 0.0026, decode.aux_loss_depth_2: 0.1785, decode.aux_loss_ce_5: 0.0030, decode.aux_loss_depth_5: 0.3563, decode.loss_ce: 0.0034, decode.ce_acc_level_0: 90.2500, decode.ce_acc_level_1: 99.5625, decode.loss_depth: 0.7108, loss: 1.2546, grad_norm: 6.1654
2022-01-10 18:56:12,038 - depth - INFO - Iter [21800/38400]	lr: 6.806e-05, eta: 1:40:47, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0029, decode.aux_loss_depth_2: 0.1722, decode.aux_loss_ce_5: 0.0036, decode.aux_loss_depth_5: 0.3434, decode.loss_ce: 0.0038, decode.ce_acc_level_0: 88.3750, decode.ce_acc_level_1: 99.3125, decode.loss_depth: 0.6860, loss: 1.2119, grad_norm: 6.0123
2022-01-10 18:56:45,899 - depth - INFO - Iter [21900/38400]	lr: 6.752e-05, eta: 1:40:08, time: 0.339, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0029, decode.aux_loss_depth_2: 0.1703, decode.aux_loss_ce_5: 0.0033, decode.aux_loss_depth_5: 0.3399, decode.loss_ce: 0.0037, decode.ce_acc_level_0: 89.0625, decode.ce_acc_level_1: 98.8750, decode.loss_depth: 0.6796, loss: 1.1997, grad_norm: 6.0851
2022-01-10 18:57:20,147 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 18:57:20,147 - depth - INFO - Iter [22000/38400]	lr: 6.697e-05, eta: 1:39:30, time: 0.342, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0029, decode.aux_loss_depth_2: 0.1728, decode.aux_loss_ce_5: 0.0034, decode.aux_loss_depth_5: 0.3451, decode.loss_ce: 0.0037, decode.ce_acc_level_0: 87.6250, decode.ce_acc_level_1: 99.2500, decode.loss_depth: 0.6893, loss: 1.2172, grad_norm: 6.2268
2022-01-10 18:57:54,269 - depth - INFO - Iter [22100/38400]	lr: 6.642e-05, eta: 1:38:52, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0031, decode.aux_loss_depth_2: 0.1718, decode.aux_loss_ce_5: 0.0034, decode.aux_loss_depth_5: 0.3421, decode.loss_ce: 0.0037, decode.ce_acc_level_0: 87.7500, decode.ce_acc_level_1: 99.5000, decode.loss_depth: 0.6830, loss: 1.2071, grad_norm: 6.4878
2022-01-10 18:58:28,830 - depth - INFO - Iter [22200/38400]	lr: 6.586e-05, eta: 1:38:14, time: 0.346, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0032, decode.aux_loss_depth_2: 0.1722, decode.aux_loss_ce_5: 0.0037, decode.aux_loss_depth_5: 0.3428, decode.loss_ce: 0.0039, decode.ce_acc_level_0: 87.1250, decode.ce_acc_level_1: 99.0625, decode.loss_depth: 0.6854, loss: 1.2113, grad_norm: 6.2404
2022-01-10 18:59:02,796 - depth - INFO - Iter [22300/38400]	lr: 6.531e-05, eta: 1:37:36, time: 0.339, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0028, decode.aux_loss_depth_2: 0.1719, decode.aux_loss_ce_5: 0.0031, decode.aux_loss_depth_5: 0.3438, decode.loss_ce: 0.0034, decode.ce_acc_level_0: 88.8125, decode.ce_acc_level_1: 99.5000, decode.loss_depth: 0.6868, loss: 1.2119, grad_norm: 6.2127
2022-01-10 18:59:36,686 - depth - INFO - Saving checkpoint at 22400 iterations
2022-01-10 18:59:39,044 - depth - INFO - Iter [22400/38400]	lr: 6.475e-05, eta: 1:37:00, time: 0.363, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0024, decode.aux_loss_depth_2: 0.1751, decode.aux_loss_ce_5: 0.0029, decode.aux_loss_depth_5: 0.3488, decode.loss_ce: 0.0033, decode.ce_acc_level_0: 89.9375, decode.ce_acc_level_1: 99.7500, decode.loss_depth: 0.6976, loss: 1.2301, grad_norm: 5.9406
2022-01-10 18:59:52,462 - depth - INFO - Summary:
2022-01-10 18:59:52,463 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8822 | 0.9815 | 0.9953 |  0.1172 | 0.3955 | 0.0485 |  0.1437  | 11.5908 | 0.0713 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 18:59:52,464 - depth - INFO - Iter(val) [82]	a1: 0.8822, a2: 0.9815, a3: 0.9953, abs_rel: 0.11722546070814133, rmse: 0.3955334424972534, log_10: 0.04854363948106766, rmse_log: 0.14365045726299286, silog: 11.5908, sq_rel: 0.0712728351354599
2022-01-10 19:00:26,726 - depth - INFO - Iter [22500/38400]	lr: 6.419e-05, eta: 1:36:31, time: 0.477, data_time: 0.141, memory: 15411, decode.aux_loss_ce_2: 0.0026, decode.aux_loss_depth_2: 0.1690, decode.aux_loss_ce_5: 0.0031, decode.aux_loss_depth_5: 0.3376, decode.loss_ce: 0.0035, decode.ce_acc_level_0: 88.1250, decode.ce_acc_level_1: 99.6250, decode.loss_depth: 0.6741, loss: 1.1898, grad_norm: 5.8803
2022-01-10 19:01:01,202 - depth - INFO - Iter [22600/38400]	lr: 6.363e-05, eta: 1:35:54, time: 0.345, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0027, decode.aux_loss_depth_2: 0.1684, decode.aux_loss_ce_5: 0.0030, decode.aux_loss_depth_5: 0.3364, decode.loss_ce: 0.0034, decode.ce_acc_level_0: 89.1250, decode.ce_acc_level_1: 99.3750, decode.loss_depth: 0.6717, loss: 1.1856, grad_norm: 5.7538
2022-01-10 19:01:35,478 - depth - INFO - Iter [22700/38400]	lr: 6.307e-05, eta: 1:35:16, time: 0.343, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0028, decode.aux_loss_depth_2: 0.1708, decode.aux_loss_ce_5: 0.0035, decode.aux_loss_depth_5: 0.3408, decode.loss_ce: 0.0036, decode.ce_acc_level_0: 88.6875, decode.ce_acc_level_1: 99.3125, decode.loss_depth: 0.6805, loss: 1.2020, grad_norm: 6.2377
2022-01-10 19:02:09,777 - depth - INFO - Iter [22800/38400]	lr: 6.250e-05, eta: 1:34:38, time: 0.343, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0023, decode.aux_loss_depth_2: 0.1673, decode.aux_loss_ce_5: 0.0028, decode.aux_loss_depth_5: 0.3338, decode.loss_ce: 0.0030, decode.ce_acc_level_0: 91.0000, decode.ce_acc_level_1: 99.5000, decode.loss_depth: 0.6660, loss: 1.1752, grad_norm: 5.8293
2022-01-10 19:02:43,888 - depth - INFO - Iter [22900/38400]	lr: 6.194e-05, eta: 1:34:00, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0025, decode.aux_loss_depth_2: 0.1690, decode.aux_loss_ce_5: 0.0029, decode.aux_loss_depth_5: 0.3368, decode.loss_ce: 0.0034, decode.ce_acc_level_0: 89.8125, decode.ce_acc_level_1: 99.6250, decode.loss_depth: 0.6721, loss: 1.1868, grad_norm: 6.0088
2022-01-10 19:03:17,797 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 19:03:17,797 - depth - INFO - Iter [23000/38400]	lr: 6.137e-05, eta: 1:33:22, time: 0.339, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0026, decode.aux_loss_depth_2: 0.1648, decode.aux_loss_ce_5: 0.0031, decode.aux_loss_depth_5: 0.3291, decode.loss_ce: 0.0033, decode.ce_acc_level_0: 89.1250, decode.ce_acc_level_1: 99.3125, decode.loss_depth: 0.6577, loss: 1.1606, grad_norm: 6.4018
2022-01-10 19:03:52,940 - depth - INFO - Iter [23100/38400]	lr: 6.080e-05, eta: 1:32:45, time: 0.351, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0021, decode.aux_loss_depth_2: 0.1685, decode.aux_loss_ce_5: 0.0025, decode.aux_loss_depth_5: 0.3365, decode.loss_ce: 0.0026, decode.ce_acc_level_0: 91.2500, decode.ce_acc_level_1: 99.6250, decode.loss_depth: 0.6717, loss: 1.1840, grad_norm: 5.9977
2022-01-10 19:04:27,050 - depth - INFO - Iter [23200/38400]	lr: 6.023e-05, eta: 1:32:07, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0022, decode.aux_loss_depth_2: 0.1639, decode.aux_loss_ce_5: 0.0025, decode.aux_loss_depth_5: 0.3273, decode.loss_ce: 0.0028, decode.ce_acc_level_0: 91.3125, decode.ce_acc_level_1: 99.5000, decode.loss_depth: 0.6533, loss: 1.1519, grad_norm: 5.5536
2022-01-10 19:04:40,685 - depth - INFO - Summary:
2022-01-10 19:04:40,685 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
| 0.8801 | 0.9799 | 0.9948 |  0.1193 | 0.3961 | 0.0491 |  0.1446  | 11.59 | 0.0723 |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
2022-01-10 19:04:40,687 - depth - INFO - Iter(val) [82]	a1: 0.8801, a2: 0.9799, a3: 0.9948, abs_rel: 0.11928447335958481, rmse: 0.3961227238178253, log_10: 0.04905974119901657, rmse_log: 0.14464904367923737, silog: 11.5900, sq_rel: 0.07225145399570465
2022-01-10 19:05:15,167 - depth - INFO - Iter [23300/38400]	lr: 5.966e-05, eta: 1:31:38, time: 0.481, data_time: 0.142, memory: 15411, decode.aux_loss_ce_2: 0.0019, decode.aux_loss_depth_2: 0.1686, decode.aux_loss_ce_5: 0.0022, decode.aux_loss_depth_5: 0.3370, decode.loss_ce: 0.0026, decode.ce_acc_level_0: 92.3750, decode.ce_acc_level_1: 99.6875, decode.loss_depth: 0.6725, loss: 1.1848, grad_norm: 6.2393
2022-01-10 19:05:49,188 - depth - INFO - Iter [23400/38400]	lr: 5.908e-05, eta: 1:31:00, time: 0.340, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0021, decode.aux_loss_depth_2: 0.1635, decode.aux_loss_ce_5: 0.0024, decode.aux_loss_depth_5: 0.3264, decode.loss_ce: 0.0028, decode.ce_acc_level_0: 91.0625, decode.ce_acc_level_1: 99.4375, decode.loss_depth: 0.6513, loss: 1.1484, grad_norm: 6.1538
2022-01-10 19:06:23,591 - depth - INFO - Iter [23500/38400]	lr: 5.851e-05, eta: 1:30:22, time: 0.344, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0024, decode.aux_loss_depth_2: 0.1638, decode.aux_loss_ce_5: 0.0028, decode.aux_loss_depth_5: 0.3275, decode.loss_ce: 0.0031, decode.ce_acc_level_0: 90.1250, decode.ce_acc_level_1: 99.5625, decode.loss_depth: 0.6543, loss: 1.1540, grad_norm: 5.6550
2022-01-10 19:06:57,753 - depth - INFO - Iter [23600/38400]	lr: 5.793e-05, eta: 1:29:45, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0024, decode.aux_loss_depth_2: 0.1636, decode.aux_loss_ce_5: 0.0027, decode.aux_loss_depth_5: 0.3270, decode.loss_ce: 0.0030, decode.ce_acc_level_0: 90.0625, decode.ce_acc_level_1: 99.6250, decode.loss_depth: 0.6529, loss: 1.1515, grad_norm: 5.7624
2022-01-10 19:07:32,124 - depth - INFO - Iter [23700/38400]	lr: 5.735e-05, eta: 1:29:07, time: 0.344, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0021, decode.aux_loss_depth_2: 0.1636, decode.aux_loss_ce_5: 0.0026, decode.aux_loss_depth_5: 0.3269, decode.loss_ce: 0.0028, decode.ce_acc_level_0: 90.9375, decode.ce_acc_level_1: 99.6875, decode.loss_depth: 0.6526, loss: 1.1507, grad_norm: 5.4657
2022-01-10 19:08:06,286 - depth - INFO - Iter [23800/38400]	lr: 5.678e-05, eta: 1:28:29, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0020, decode.aux_loss_depth_2: 0.1624, decode.aux_loss_ce_5: 0.0024, decode.aux_loss_depth_5: 0.3242, decode.loss_ce: 0.0026, decode.ce_acc_level_0: 91.4375, decode.ce_acc_level_1: 99.6250, decode.loss_depth: 0.6477, loss: 1.1411, grad_norm: 5.4349
2022-01-10 19:08:40,397 - depth - INFO - Iter [23900/38400]	lr: 5.620e-05, eta: 1:27:52, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0019, decode.aux_loss_depth_2: 0.1628, decode.aux_loss_ce_5: 0.0023, decode.aux_loss_depth_5: 0.3251, decode.loss_ce: 0.0025, decode.ce_acc_level_0: 91.3125, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.6485, loss: 1.1431, grad_norm: 5.6781
2022-01-10 19:09:14,611 - depth - INFO - Saving checkpoint at 24000 iterations
2022-01-10 19:09:16,797 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 19:09:16,798 - depth - INFO - Iter [24000/38400]	lr: 5.562e-05, eta: 1:27:15, time: 0.364, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0018, decode.aux_loss_depth_2: 0.1617, decode.aux_loss_ce_5: 0.0022, decode.aux_loss_depth_5: 0.3229, decode.loss_ce: 0.0025, decode.ce_acc_level_0: 92.3125, decode.ce_acc_level_1: 99.6875, decode.loss_depth: 0.6437, loss: 1.1348, grad_norm: 5.3967
2022-01-10 19:09:30,257 - depth - INFO - Summary:
2022-01-10 19:09:30,258 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8825 | 0.9825 | 0.9956 |  0.1149 | 0.3874 | 0.0477 |  0.1415  | 11.4273 | 0.0683 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 19:09:32,628 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_24000.pth.
2022-01-10 19:09:32,629 - depth - INFO - Best abs_rel is 0.1149 at 24000 iter.
2022-01-10 19:09:32,630 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 19:09:32,630 - depth - INFO - Iter(val) [82]	a1: 0.8825, a2: 0.9825, a3: 0.9956, abs_rel: 0.11491783708333969, rmse: 0.3874036371707916, log_10: 0.04769079014658928, rmse_log: 0.1414911299943924, silog: 11.4273, sq_rel: 0.06827247142791748
2022-01-10 19:10:06,751 - depth - INFO - Iter [24100/38400]	lr: 5.504e-05, eta: 1:26:47, time: 0.499, data_time: 0.164, memory: 15411, decode.aux_loss_ce_2: 0.0018, decode.aux_loss_depth_2: 0.1631, decode.aux_loss_ce_5: 0.0021, decode.aux_loss_depth_5: 0.3260, decode.loss_ce: 0.0024, decode.ce_acc_level_0: 92.6250, decode.ce_acc_level_1: 99.6875, decode.loss_depth: 0.6502, loss: 1.1457, grad_norm: 5.5971
2022-01-10 19:10:41,027 - depth - INFO - Iter [24200/38400]	lr: 5.445e-05, eta: 1:26:09, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0020, decode.aux_loss_depth_2: 0.1630, decode.aux_loss_ce_5: 0.0024, decode.aux_loss_depth_5: 0.3256, decode.loss_ce: 0.0026, decode.ce_acc_level_0: 92.4375, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.6501, loss: 1.1457, grad_norm: 6.0914
2022-01-10 19:11:15,651 - depth - INFO - Iter [24300/38400]	lr: 5.387e-05, eta: 1:25:32, time: 0.346, data_time: 0.008, memory: 15411, decode.aux_loss_ce_2: 0.0018, decode.aux_loss_depth_2: 0.1585, decode.aux_loss_ce_5: 0.0021, decode.aux_loss_depth_5: 0.3162, decode.loss_ce: 0.0025, decode.ce_acc_level_0: 91.4375, decode.ce_acc_level_1: 99.6875, decode.loss_depth: 0.6316, loss: 1.1129, grad_norm: 5.6539
2022-01-10 19:11:50,021 - depth - INFO - Iter [24400/38400]	lr: 5.329e-05, eta: 1:24:54, time: 0.344, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0019, decode.aux_loss_depth_2: 0.1596, decode.aux_loss_ce_5: 0.0023, decode.aux_loss_depth_5: 0.3188, decode.loss_ce: 0.0024, decode.ce_acc_level_0: 92.1250, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.6365, loss: 1.1216, grad_norm: 5.2391
2022-01-10 19:12:24,134 - depth - INFO - Iter [24500/38400]	lr: 5.271e-05, eta: 1:24:17, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0023, decode.aux_loss_depth_2: 0.1621, decode.aux_loss_ce_5: 0.0026, decode.aux_loss_depth_5: 0.3237, decode.loss_ce: 0.0029, decode.ce_acc_level_0: 90.9375, decode.ce_acc_level_1: 99.7500, decode.loss_depth: 0.6467, loss: 1.1402, grad_norm: 6.0266
2022-01-10 19:12:58,449 - depth - INFO - Iter [24600/38400]	lr: 5.212e-05, eta: 1:23:39, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0017, decode.aux_loss_depth_2: 0.1596, decode.aux_loss_ce_5: 0.0021, decode.aux_loss_depth_5: 0.3181, decode.loss_ce: 0.0023, decode.ce_acc_level_0: 92.8750, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.6362, loss: 1.1200, grad_norm: 5.6259
2022-01-10 19:13:32,717 - depth - INFO - Iter [24700/38400]	lr: 5.154e-05, eta: 1:23:02, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0017, decode.aux_loss_depth_2: 0.1562, decode.aux_loss_ce_5: 0.0020, decode.aux_loss_depth_5: 0.3120, decode.loss_ce: 0.0022, decode.ce_acc_level_0: 92.8750, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.6227, loss: 1.0967, grad_norm: 5.5959
2022-01-10 19:14:06,961 - depth - INFO - Iter [24800/38400]	lr: 5.095e-05, eta: 1:22:24, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0021, decode.aux_loss_depth_2: 0.1581, decode.aux_loss_ce_5: 0.0024, decode.aux_loss_depth_5: 0.3157, decode.loss_ce: 0.0029, decode.ce_acc_level_0: 90.2500, decode.ce_acc_level_1: 99.6250, decode.loss_depth: 0.6298, loss: 1.1109, grad_norm: 5.5228
2022-01-10 19:14:20,430 - depth - INFO - Summary:
2022-01-10 19:14:20,431 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8843 | 0.9808 | 0.9952 |  0.1149 | 0.3903 | 0.0479 |  0.1425  | 11.6113 | 0.0688 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 19:14:23,166 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_24800.pth.
2022-01-10 19:14:23,167 - depth - INFO - Best abs_rel is 0.1149 at 24800 iter.
2022-01-10 19:14:23,168 - depth - INFO - Iter(val) [82]	a1: 0.8843, a2: 0.9808, a3: 0.9952, abs_rel: 0.11491359025239944, rmse: 0.3903387784957886, log_10: 0.04789641872048378, rmse_log: 0.1425008326768875, silog: 11.6113, sq_rel: 0.06884965300559998
2022-01-10 19:14:57,353 - depth - INFO - Iter [24900/38400]	lr: 5.037e-05, eta: 1:21:55, time: 0.504, data_time: 0.168, memory: 15411, decode.aux_loss_ce_2: 0.0018, decode.aux_loss_depth_2: 0.1611, decode.aux_loss_ce_5: 0.0021, decode.aux_loss_depth_5: 0.3223, decode.loss_ce: 0.0024, decode.ce_acc_level_0: 92.1875, decode.ce_acc_level_1: 99.6875, decode.loss_depth: 0.6432, loss: 1.1329, grad_norm: 6.0141
2022-01-10 19:15:31,966 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 19:15:31,967 - depth - INFO - Iter [25000/38400]	lr: 4.979e-05, eta: 1:21:18, time: 0.346, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0015, decode.aux_loss_depth_2: 0.1574, decode.aux_loss_ce_5: 0.0016, decode.aux_loss_depth_5: 0.3142, decode.loss_ce: 0.0019, decode.ce_acc_level_0: 93.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.6281, loss: 1.1048, grad_norm: 5.5053
2022-01-10 19:16:06,122 - depth - INFO - Iter [25100/38400]	lr: 4.920e-05, eta: 1:20:40, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0020, decode.aux_loss_depth_2: 0.1571, decode.aux_loss_ce_5: 0.0024, decode.aux_loss_depth_5: 0.3133, decode.loss_ce: 0.0026, decode.ce_acc_level_0: 90.9375, decode.ce_acc_level_1: 99.6875, decode.loss_depth: 0.6261, loss: 1.1035, grad_norm: 5.8863
2022-01-10 19:16:40,466 - depth - INFO - Iter [25200/38400]	lr: 4.862e-05, eta: 1:20:03, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0015, decode.aux_loss_depth_2: 0.1567, decode.aux_loss_ce_5: 0.0017, decode.aux_loss_depth_5: 0.3134, decode.loss_ce: 0.0021, decode.ce_acc_level_0: 93.1875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.6255, loss: 1.1009, grad_norm: 5.6520
2022-01-10 19:17:14,777 - depth - INFO - Iter [25300/38400]	lr: 4.803e-05, eta: 1:19:25, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0016, decode.aux_loss_depth_2: 0.1540, decode.aux_loss_ce_5: 0.0018, decode.aux_loss_depth_5: 0.3076, decode.loss_ce: 0.0021, decode.ce_acc_level_0: 93.6875, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.6131, loss: 1.0802, grad_norm: 5.1717
2022-01-10 19:17:49,294 - depth - INFO - Iter [25400/38400]	lr: 4.745e-05, eta: 1:18:48, time: 0.345, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0018, decode.aux_loss_depth_2: 0.1533, decode.aux_loss_ce_5: 0.0020, decode.aux_loss_depth_5: 0.3057, decode.loss_ce: 0.0022, decode.ce_acc_level_0: 92.5000, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.6109, loss: 1.0759, grad_norm: 5.7800
2022-01-10 19:18:23,653 - depth - INFO - Iter [25500/38400]	lr: 4.687e-05, eta: 1:18:11, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0014, decode.aux_loss_depth_2: 0.1558, decode.aux_loss_ce_5: 0.0017, decode.aux_loss_depth_5: 0.3109, decode.loss_ce: 0.0020, decode.ce_acc_level_0: 93.4375, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.6204, loss: 1.0921, grad_norm: 5.6227
2022-01-10 19:18:57,405 - depth - INFO - Saving checkpoint at 25600 iterations
2022-01-10 19:18:59,942 - depth - INFO - Iter [25600/38400]	lr: 4.628e-05, eta: 1:17:34, time: 0.363, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0015, decode.aux_loss_depth_2: 0.1570, decode.aux_loss_ce_5: 0.0018, decode.aux_loss_depth_5: 0.3132, decode.loss_ce: 0.0019, decode.ce_acc_level_0: 94.3125, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.6262, loss: 1.1016, grad_norm: 5.4176
2022-01-10 19:19:13,250 - depth - INFO - Summary:
2022-01-10 19:19:13,251 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8842 | 0.9828 | 0.9954 |  0.1155 | 0.3896 | 0.0479 |  0.1415  | 11.3612 | 0.0688 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 19:19:13,252 - depth - INFO - Iter(val) [82]	a1: 0.8842, a2: 0.9828, a3: 0.9954, abs_rel: 0.11548899114131927, rmse: 0.38964083790779114, log_10: 0.047904424369335175, rmse_log: 0.14154736697673798, silog: 11.3612, sq_rel: 0.06879211217164993
2022-01-10 19:19:47,640 - depth - INFO - Iter [25700/38400]	lr: 4.570e-05, eta: 1:17:03, time: 0.477, data_time: 0.139, memory: 15411, decode.aux_loss_ce_2: 0.0016, decode.aux_loss_depth_2: 0.1562, decode.aux_loss_ce_5: 0.0018, decode.aux_loss_depth_5: 0.3112, decode.loss_ce: 0.0021, decode.ce_acc_level_0: 93.5625, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.6221, loss: 1.0950, grad_norm: 5.5717
2022-01-10 19:20:22,250 - depth - INFO - Iter [25800/38400]	lr: 4.512e-05, eta: 1:16:26, time: 0.347, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0012, decode.aux_loss_depth_2: 0.1531, decode.aux_loss_ce_5: 0.0016, decode.aux_loss_depth_5: 0.3051, decode.loss_ce: 0.0018, decode.ce_acc_level_0: 94.3125, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.6096, loss: 1.0723, grad_norm: 5.3998
2022-01-10 19:20:56,798 - depth - INFO - Iter [25900/38400]	lr: 4.454e-05, eta: 1:15:49, time: 0.345, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0014, decode.aux_loss_depth_2: 0.1525, decode.aux_loss_ce_5: 0.0018, decode.aux_loss_depth_5: 0.3039, decode.loss_ce: 0.0021, decode.ce_acc_level_0: 93.8750, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.6072, loss: 1.0688, grad_norm: 5.4185
2022-01-10 19:21:30,994 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 19:21:30,994 - depth - INFO - Iter [26000/38400]	lr: 4.396e-05, eta: 1:15:11, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0013, decode.aux_loss_depth_2: 0.1526, decode.aux_loss_ce_5: 0.0017, decode.aux_loss_depth_5: 0.3044, decode.loss_ce: 0.0019, decode.ce_acc_level_0: 94.0000, decode.ce_acc_level_1: 99.7500, decode.loss_depth: 0.6083, loss: 1.0703, grad_norm: 5.4888
2022-01-10 19:22:05,469 - depth - INFO - Iter [26100/38400]	lr: 4.338e-05, eta: 1:14:34, time: 0.345, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0014, decode.aux_loss_depth_2: 0.1519, decode.aux_loss_ce_5: 0.0017, decode.aux_loss_depth_5: 0.3032, decode.loss_ce: 0.0019, decode.ce_acc_level_0: 93.8125, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.6049, loss: 1.0651, grad_norm: 5.7505
2022-01-10 19:22:39,754 - depth - INFO - Iter [26200/38400]	lr: 4.280e-05, eta: 1:13:57, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0012, decode.aux_loss_depth_2: 0.1528, decode.aux_loss_ce_5: 0.0015, decode.aux_loss_depth_5: 0.3049, decode.loss_ce: 0.0017, decode.ce_acc_level_0: 94.5000, decode.ce_acc_level_1: 99.7500, decode.loss_depth: 0.6094, loss: 1.0715, grad_norm: 5.5289
2022-01-10 19:23:13,866 - depth - INFO - Iter [26300/38400]	lr: 4.222e-05, eta: 1:13:19, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0019, decode.aux_loss_depth_2: 0.1518, decode.aux_loss_ce_5: 0.0021, decode.aux_loss_depth_5: 0.3032, decode.loss_ce: 0.0023, decode.ce_acc_level_0: 93.1875, decode.ce_acc_level_1: 99.6250, decode.loss_depth: 0.6056, loss: 1.0670, grad_norm: 5.6121
2022-01-10 19:23:47,828 - depth - INFO - Iter [26400/38400]	lr: 4.165e-05, eta: 1:12:42, time: 0.339, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0014, decode.aux_loss_depth_2: 0.1498, decode.aux_loss_ce_5: 0.0017, decode.aux_loss_depth_5: 0.2992, decode.loss_ce: 0.0020, decode.ce_acc_level_0: 93.5625, decode.ce_acc_level_1: 99.7500, decode.loss_depth: 0.5965, loss: 1.0508, grad_norm: 5.4324
2022-01-10 19:24:01,326 - depth - INFO - Summary:
2022-01-10 19:24:01,327 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8881 | 0.9828 | 0.9955 |  0.1115 | 0.3821 | 0.0466 |  0.1392  | 11.3056 | 0.0667 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 19:24:03,626 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_26400.pth.
2022-01-10 19:24:03,627 - depth - INFO - Best abs_rel is 0.1115 at 26400 iter.
2022-01-10 19:24:03,628 - depth - INFO - Iter(val) [82]	a1: 0.8881, a2: 0.9828, a3: 0.9955, abs_rel: 0.11146928369998932, rmse: 0.3820531964302063, log_10: 0.046618446707725525, rmse_log: 0.139166921377182, silog: 11.3056, sq_rel: 0.06672688573598862
2022-01-10 19:24:37,739 - depth - INFO - Iter [26500/38400]	lr: 4.107e-05, eta: 1:12:12, time: 0.499, data_time: 0.164, memory: 15411, decode.aux_loss_ce_2: 0.0014, decode.aux_loss_depth_2: 0.1515, decode.aux_loss_ce_5: 0.0018, decode.aux_loss_depth_5: 0.3028, decode.loss_ce: 0.0019, decode.ce_acc_level_0: 93.7500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.6043, loss: 1.0637, grad_norm: 5.1650
2022-01-10 19:25:12,033 - depth - INFO - Iter [26600/38400]	lr: 4.050e-05, eta: 1:11:34, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0012, decode.aux_loss_depth_2: 0.1498, decode.aux_loss_ce_5: 0.0015, decode.aux_loss_depth_5: 0.2991, decode.loss_ce: 0.0016, decode.ce_acc_level_0: 94.5000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5966, loss: 1.0499, grad_norm: 5.2935
2022-01-10 19:25:46,304 - depth - INFO - Iter [26700/38400]	lr: 3.993e-05, eta: 1:10:57, time: 0.343, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0011, decode.aux_loss_depth_2: 0.1500, decode.aux_loss_ce_5: 0.0014, decode.aux_loss_depth_5: 0.2998, decode.loss_ce: 0.0016, decode.ce_acc_level_0: 94.6250, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5994, loss: 1.0533, grad_norm: 5.2159
2022-01-10 19:26:20,646 - depth - INFO - Iter [26800/38400]	lr: 3.935e-05, eta: 1:10:20, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0011, decode.aux_loss_depth_2: 0.1506, decode.aux_loss_ce_5: 0.0013, decode.aux_loss_depth_5: 0.3006, decode.loss_ce: 0.0016, decode.ce_acc_level_0: 94.8750, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.6003, loss: 1.0556, grad_norm: 5.4362
2022-01-10 19:26:54,911 - depth - INFO - Iter [26900/38400]	lr: 3.878e-05, eta: 1:09:42, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0013, decode.aux_loss_depth_2: 0.1494, decode.aux_loss_ce_5: 0.0015, decode.aux_loss_depth_5: 0.2983, decode.loss_ce: 0.0018, decode.ce_acc_level_0: 93.9375, decode.ce_acc_level_1: 99.7500, decode.loss_depth: 0.5954, loss: 1.0478, grad_norm: 5.2853
2022-01-10 19:27:29,089 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 19:27:29,089 - depth - INFO - Iter [27000/38400]	lr: 3.822e-05, eta: 1:09:05, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0011, decode.aux_loss_depth_2: 0.1507, decode.aux_loss_ce_5: 0.0014, decode.aux_loss_depth_5: 0.3012, decode.loss_ce: 0.0016, decode.ce_acc_level_0: 94.8125, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.6007, loss: 1.0568, grad_norm: 5.7373
2022-01-10 19:28:03,455 - depth - INFO - Iter [27100/38400]	lr: 3.765e-05, eta: 1:08:28, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0010, decode.aux_loss_depth_2: 0.1490, decode.aux_loss_ce_5: 0.0013, decode.aux_loss_depth_5: 0.2975, decode.loss_ce: 0.0015, decode.ce_acc_level_0: 95.0625, decode.ce_acc_level_1: 99.7500, decode.loss_depth: 0.5941, loss: 1.0443, grad_norm: 4.9946
2022-01-10 19:28:37,446 - depth - INFO - Saving checkpoint at 27200 iterations
2022-01-10 19:28:39,752 - depth - INFO - Iter [27200/38400]	lr: 3.708e-05, eta: 1:07:52, time: 0.363, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0011, decode.aux_loss_depth_2: 0.1469, decode.aux_loss_ce_5: 0.0013, decode.aux_loss_depth_5: 0.2929, decode.loss_ce: 0.0016, decode.ce_acc_level_0: 95.0625, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5853, loss: 1.0292, grad_norm: 5.2246
2022-01-10 19:28:53,018 - depth - INFO - Summary:
2022-01-10 19:28:53,018 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8842 | 0.9826 | 0.9953 |  0.1158 | 0.3864 | 0.0476 |  0.1413  | 11.3763 | 0.071  |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 19:28:53,019 - depth - INFO - Iter(val) [82]	a1: 0.8842, a2: 0.9826, a3: 0.9953, abs_rel: 0.11584363132715225, rmse: 0.3863806128501892, log_10: 0.04759388789534569, rmse_log: 0.14131297171115875, silog: 11.3763, sq_rel: 0.07099618017673492
2022-01-10 19:29:27,404 - depth - INFO - Iter [27300/38400]	lr: 3.652e-05, eta: 1:07:20, time: 0.476, data_time: 0.140, memory: 15411, decode.aux_loss_ce_2: 0.0009, decode.aux_loss_depth_2: 0.1470, decode.aux_loss_ce_5: 0.0012, decode.aux_loss_depth_5: 0.2938, decode.loss_ce: 0.0013, decode.ce_acc_level_0: 95.7500, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5862, loss: 1.0305, grad_norm: 5.1860
2022-01-10 19:30:01,547 - depth - INFO - Iter [27400/38400]	lr: 3.596e-05, eta: 1:06:42, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0011, decode.aux_loss_depth_2: 0.1461, decode.aux_loss_ce_5: 0.0013, decode.aux_loss_depth_5: 0.2914, decode.loss_ce: 0.0016, decode.ce_acc_level_0: 95.5000, decode.ce_acc_level_1: 99.7500, decode.loss_depth: 0.5828, loss: 1.0243, grad_norm: 4.9889
2022-01-10 19:30:35,774 - depth - INFO - Iter [27500/38400]	lr: 3.540e-05, eta: 1:06:05, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0010, decode.aux_loss_depth_2: 0.1467, decode.aux_loss_ce_5: 0.0012, decode.aux_loss_depth_5: 0.2930, decode.loss_ce: 0.0014, decode.ce_acc_level_0: 95.6250, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5846, loss: 1.0278, grad_norm: 5.0237
2022-01-10 19:31:10,123 - depth - INFO - Iter [27600/38400]	lr: 3.484e-05, eta: 1:05:28, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0007, decode.aux_loss_depth_2: 0.1461, decode.aux_loss_ce_5: 0.0011, decode.aux_loss_depth_5: 0.2915, decode.loss_ce: 0.0012, decode.ce_acc_level_0: 96.5000, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5812, loss: 1.0218, grad_norm: 5.0325
2022-01-10 19:31:43,859 - depth - INFO - Iter [27700/38400]	lr: 3.429e-05, eta: 1:04:51, time: 0.337, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0010, decode.aux_loss_depth_2: 0.1436, decode.aux_loss_ce_5: 0.0012, decode.aux_loss_depth_5: 0.2868, decode.loss_ce: 0.0013, decode.ce_acc_level_0: 95.6875, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5724, loss: 1.0063, grad_norm: 4.9349
2022-01-10 19:32:18,593 - depth - INFO - Iter [27800/38400]	lr: 3.373e-05, eta: 1:04:14, time: 0.347, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0009, decode.aux_loss_depth_2: 0.1449, decode.aux_loss_ce_5: 0.0011, decode.aux_loss_depth_5: 0.2895, decode.loss_ce: 0.0013, decode.ce_acc_level_0: 95.8125, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5781, loss: 1.0158, grad_norm: 5.5579
2022-01-10 19:32:52,972 - depth - INFO - Iter [27900/38400]	lr: 3.318e-05, eta: 1:03:37, time: 0.344, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0009, decode.aux_loss_depth_2: 0.1471, decode.aux_loss_ce_5: 0.0012, decode.aux_loss_depth_5: 0.2939, decode.loss_ce: 0.0015, decode.ce_acc_level_0: 95.0625, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5864, loss: 1.0310, grad_norm: 5.1898
2022-01-10 19:33:27,738 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 19:33:27,739 - depth - INFO - Iter [28000/38400]	lr: 3.263e-05, eta: 1:03:00, time: 0.348, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0009, decode.aux_loss_depth_2: 0.1439, decode.aux_loss_ce_5: 0.0011, decode.aux_loss_depth_5: 0.2876, decode.loss_ce: 0.0013, decode.ce_acc_level_0: 96.1250, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5736, loss: 1.0085, grad_norm: 5.5028
2022-01-10 19:33:41,028 - depth - INFO - Summary:
2022-01-10 19:33:41,028 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8829 | 0.9812 | 0.9953 |  0.1162 | 0.3857 | 0.0478 |  0.1417  | 11.3428 | 0.0701 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 19:33:41,029 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 19:33:41,029 - depth - INFO - Iter(val) [82]	a1: 0.8829, a2: 0.9812, a3: 0.9953, abs_rel: 0.11616045981645584, rmse: 0.38570913672447205, log_10: 0.04776604473590851, rmse_log: 0.14165179431438446, silog: 11.3428, sq_rel: 0.0700540840625763
2022-01-10 19:34:15,019 - depth - INFO - Iter [28100/38400]	lr: 3.209e-05, eta: 1:02:27, time: 0.473, data_time: 0.138, memory: 15411, decode.aux_loss_ce_2: 0.0009, decode.aux_loss_depth_2: 0.1465, decode.aux_loss_ce_5: 0.0011, decode.aux_loss_depth_5: 0.2918, decode.loss_ce: 0.0014, decode.ce_acc_level_0: 95.5000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5822, loss: 1.0239, grad_norm: 5.0136
2022-01-10 19:34:49,115 - depth - INFO - Iter [28200/38400]	lr: 3.154e-05, eta: 1:01:50, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0008, decode.aux_loss_depth_2: 0.1433, decode.aux_loss_ce_5: 0.0010, decode.aux_loss_depth_5: 0.2863, decode.loss_ce: 0.0013, decode.ce_acc_level_0: 96.3750, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5716, loss: 1.0042, grad_norm: 5.0246
2022-01-10 19:35:23,509 - depth - INFO - Iter [28300/38400]	lr: 3.100e-05, eta: 1:01:13, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0009, decode.aux_loss_depth_2: 0.1433, decode.aux_loss_ce_5: 0.0011, decode.aux_loss_depth_5: 0.2857, decode.loss_ce: 0.0013, decode.ce_acc_level_0: 96.1875, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5701, loss: 1.0025, grad_norm: 5.0421
2022-01-10 19:35:58,111 - depth - INFO - Iter [28400/38400]	lr: 3.046e-05, eta: 1:00:36, time: 0.346, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0009, decode.aux_loss_depth_2: 0.1449, decode.aux_loss_ce_5: 0.0011, decode.aux_loss_depth_5: 0.2892, decode.loss_ce: 0.0014, decode.ce_acc_level_0: 95.2500, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5777, loss: 1.0153, grad_norm: 5.3453
2022-01-10 19:36:32,287 - depth - INFO - Iter [28500/38400]	lr: 2.993e-05, eta: 0:59:59, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0008, decode.aux_loss_depth_2: 0.1449, decode.aux_loss_ce_5: 0.0010, decode.aux_loss_depth_5: 0.2892, decode.loss_ce: 0.0011, decode.ce_acc_level_0: 96.6875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5769, loss: 1.0139, grad_norm: 5.2457
2022-01-10 19:37:06,400 - depth - INFO - Iter [28600/38400]	lr: 2.939e-05, eta: 0:59:22, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0009, decode.aux_loss_depth_2: 0.1441, decode.aux_loss_ce_5: 0.0010, decode.aux_loss_depth_5: 0.2876, decode.loss_ce: 0.0012, decode.ce_acc_level_0: 96.1250, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5745, loss: 1.0093, grad_norm: 5.4595
2022-01-10 19:37:40,230 - depth - INFO - Iter [28700/38400]	lr: 2.886e-05, eta: 0:58:44, time: 0.338, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0009, decode.aux_loss_depth_2: 0.1449, decode.aux_loss_ce_5: 0.0011, decode.aux_loss_depth_5: 0.2887, decode.loss_ce: 0.0013, decode.ce_acc_level_0: 95.8750, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5766, loss: 1.0135, grad_norm: 5.0370
2022-01-10 19:38:14,847 - depth - INFO - Saving checkpoint at 28800 iterations
2022-01-10 19:38:17,743 - depth - INFO - Iter [28800/38400]	lr: 2.833e-05, eta: 0:58:09, time: 0.375, data_time: 0.008, memory: 15411, decode.aux_loss_ce_2: 0.0008, decode.aux_loss_depth_2: 0.1430, decode.aux_loss_ce_5: 0.0010, decode.aux_loss_depth_5: 0.2853, decode.loss_ce: 0.0012, decode.ce_acc_level_0: 96.3750, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5701, loss: 1.0012, grad_norm: 4.9698
2022-01-10 19:38:31,222 - depth - INFO - Summary:
2022-01-10 19:38:31,223 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8869 | 0.9821 | 0.9956 |  0.1138 | 0.3823 | 0.047  |   0.14   | 11.2683 | 0.0694 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 19:38:31,225 - depth - INFO - Iter(val) [82]	a1: 0.8869, a2: 0.9821, a3: 0.9956, abs_rel: 0.11379994451999664, rmse: 0.38232821226119995, log_10: 0.04695083945989609, rmse_log: 0.1400056779384613, silog: 11.2683, sq_rel: 0.06941855698823929
2022-01-10 19:39:05,163 - depth - INFO - Iter [28900/38400]	lr: 2.781e-05, eta: 0:57:36, time: 0.474, data_time: 0.141, memory: 15411, decode.aux_loss_ce_2: 0.0012, decode.aux_loss_depth_2: 0.1404, decode.aux_loss_ce_5: 0.0014, decode.aux_loss_depth_5: 0.2798, decode.loss_ce: 0.0014, decode.ce_acc_level_0: 95.4375, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5594, loss: 0.9835, grad_norm: 5.0623
2022-01-10 19:39:39,249 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 19:39:39,249 - depth - INFO - Iter [29000/38400]	lr: 2.729e-05, eta: 0:56:59, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0008, decode.aux_loss_depth_2: 0.1431, decode.aux_loss_ce_5: 0.0010, decode.aux_loss_depth_5: 0.2856, decode.loss_ce: 0.0012, decode.ce_acc_level_0: 95.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5706, loss: 1.0023, grad_norm: 4.8461
2022-01-10 19:40:13,596 - depth - INFO - Iter [29100/38400]	lr: 2.677e-05, eta: 0:56:22, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0009, decode.aux_loss_depth_2: 0.1411, decode.aux_loss_ce_5: 0.0010, decode.aux_loss_depth_5: 0.2815, decode.loss_ce: 0.0011, decode.ce_acc_level_0: 95.8750, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5624, loss: 0.9880, grad_norm: 4.8771
2022-01-10 19:40:47,854 - depth - INFO - Iter [29200/38400]	lr: 2.625e-05, eta: 0:55:45, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0008, decode.aux_loss_depth_2: 0.1428, decode.aux_loss_ce_5: 0.0010, decode.aux_loss_depth_5: 0.2847, decode.loss_ce: 0.0011, decode.ce_acc_level_0: 97.0000, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5684, loss: 0.9987, grad_norm: 5.1028
2022-01-10 19:41:22,176 - depth - INFO - Iter [29300/38400]	lr: 2.574e-05, eta: 0:55:08, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0008, decode.aux_loss_depth_2: 0.1400, decode.aux_loss_ce_5: 0.0011, decode.aux_loss_depth_5: 0.2795, decode.loss_ce: 0.0012, decode.ce_acc_level_0: 96.3125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5579, loss: 0.9805, grad_norm: 4.8424
2022-01-10 19:41:56,556 - depth - INFO - Iter [29400/38400]	lr: 2.523e-05, eta: 0:54:31, time: 0.344, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0008, decode.aux_loss_depth_2: 0.1425, decode.aux_loss_ce_5: 0.0010, decode.aux_loss_depth_5: 0.2843, decode.loss_ce: 0.0011, decode.ce_acc_level_0: 96.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5677, loss: 0.9974, grad_norm: 5.2098
2022-01-10 19:42:30,642 - depth - INFO - Iter [29500/38400]	lr: 2.473e-05, eta: 0:53:54, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0007, decode.aux_loss_depth_2: 0.1425, decode.aux_loss_ce_5: 0.0009, decode.aux_loss_depth_5: 0.2845, decode.loss_ce: 0.0010, decode.ce_acc_level_0: 96.8750, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5680, loss: 0.9976, grad_norm: 4.8964
2022-01-10 19:43:04,985 - depth - INFO - Iter [29600/38400]	lr: 2.423e-05, eta: 0:53:17, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0007, decode.aux_loss_depth_2: 0.1406, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2806, decode.loss_ce: 0.0009, decode.ce_acc_level_0: 97.2500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5603, loss: 0.9839, grad_norm: 5.1190
2022-01-10 19:43:18,593 - depth - INFO - Summary:
2022-01-10 19:43:18,594 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8859 | 0.9824 | 0.9956 |  0.1135 | 0.3834 | 0.047  |  0.1398  | 11.2543 | 0.0687 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 19:43:18,596 - depth - INFO - Iter(val) [82]	a1: 0.8859, a2: 0.9824, a3: 0.9956, abs_rel: 0.11345458030700684, rmse: 0.3834346532821655, log_10: 0.04702359065413475, rmse_log: 0.1398157775402069, silog: 11.2543, sq_rel: 0.06865891069173813
2022-01-10 19:43:52,675 - depth - INFO - Iter [29700/38400]	lr: 2.373e-05, eta: 0:52:44, time: 0.477, data_time: 0.142, memory: 15411, decode.aux_loss_ce_2: 0.0007, decode.aux_loss_depth_2: 0.1402, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2797, decode.loss_ce: 0.0009, decode.ce_acc_level_0: 97.3125, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5587, loss: 0.9810, grad_norm: 4.8600
2022-01-10 19:44:27,019 - depth - INFO - Iter [29800/38400]	lr: 2.323e-05, eta: 0:52:07, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1379, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2754, decode.loss_ce: 0.0010, decode.ce_acc_level_0: 96.8750, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5496, loss: 0.9652, grad_norm: 4.7966
2022-01-10 19:45:01,618 - depth - INFO - Iter [29900/38400]	lr: 2.274e-05, eta: 0:51:30, time: 0.346, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1380, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2754, decode.loss_ce: 0.0009, decode.ce_acc_level_0: 97.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5500, loss: 0.9656, grad_norm: 5.0340
2022-01-10 19:45:35,706 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 19:45:35,707 - depth - INFO - Iter [30000/38400]	lr: 2.225e-05, eta: 0:50:53, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0007, decode.aux_loss_depth_2: 0.1385, decode.aux_loss_ce_5: 0.0009, decode.aux_loss_depth_5: 0.2763, decode.loss_ce: 0.0011, decode.ce_acc_level_0: 96.1250, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5510, loss: 0.9685, grad_norm: 4.7721
2022-01-10 19:46:09,829 - depth - INFO - Iter [30100/38400]	lr: 2.177e-05, eta: 0:50:16, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1384, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2767, decode.loss_ce: 0.0010, decode.ce_acc_level_0: 96.7500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5526, loss: 0.9701, grad_norm: 4.9787
2022-01-10 19:46:43,880 - depth - INFO - Iter [30200/38400]	lr: 2.129e-05, eta: 0:49:39, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1376, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2748, decode.loss_ce: 0.0010, decode.ce_acc_level_0: 96.6875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5492, loss: 0.9639, grad_norm: 4.9157
2022-01-10 19:47:18,365 - depth - INFO - Iter [30300/38400]	lr: 2.081e-05, eta: 0:49:02, time: 0.345, data_time: 0.008, memory: 15411, decode.aux_loss_ce_2: 0.0007, decode.aux_loss_depth_2: 0.1403, decode.aux_loss_ce_5: 0.0009, decode.aux_loss_depth_5: 0.2799, decode.loss_ce: 0.0011, decode.ce_acc_level_0: 96.7500, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5595, loss: 0.9824, grad_norm: 5.0103
2022-01-10 19:47:52,318 - depth - INFO - Saving checkpoint at 30400 iterations
2022-01-10 19:47:54,673 - depth - INFO - Iter [30400/38400]	lr: 2.034e-05, eta: 0:48:26, time: 0.363, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0008, decode.aux_loss_depth_2: 0.1383, decode.aux_loss_ce_5: 0.0009, decode.aux_loss_depth_5: 0.2761, decode.loss_ce: 0.0012, decode.ce_acc_level_0: 96.8125, decode.ce_acc_level_1: 99.7500, decode.loss_depth: 0.5514, loss: 0.9687, grad_norm: 4.5864
2022-01-10 19:48:07,877 - depth - INFO - Summary:
2022-01-10 19:48:07,877 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8882 | 0.9826 | 0.9954 |  0.1141 | 0.3816 | 0.047  |  0.1395  | 11.1557 | 0.0683 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 19:48:07,878 - depth - INFO - Iter(val) [82]	a1: 0.8882, a2: 0.9826, a3: 0.9954, abs_rel: 0.11409605294466019, rmse: 0.381614089012146, log_10: 0.04700373485684395, rmse_log: 0.13954131305217743, silog: 11.1557, sq_rel: 0.06828014552593231
2022-01-10 19:48:42,171 - depth - INFO - Iter [30500/38400]	lr: 1.987e-05, eta: 0:47:52, time: 0.475, data_time: 0.137, memory: 15411, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1355, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2708, decode.loss_ce: 0.0011, decode.ce_acc_level_0: 96.1875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5401, loss: 0.9490, grad_norm: 4.7156
2022-01-10 19:49:16,457 - depth - INFO - Iter [30600/38400]	lr: 1.941e-05, eta: 0:47:16, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1390, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2773, decode.loss_ce: 0.0010, decode.ce_acc_level_0: 96.7500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5542, loss: 0.9729, grad_norm: 4.7016
2022-01-10 19:49:50,394 - depth - INFO - Iter [30700/38400]	lr: 1.895e-05, eta: 0:46:39, time: 0.339, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1352, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2697, decode.loss_ce: 0.0010, decode.ce_acc_level_0: 96.7500, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5387, loss: 0.9459, grad_norm: 4.6619
2022-01-10 19:50:24,594 - depth - INFO - Iter [30800/38400]	lr: 1.849e-05, eta: 0:46:02, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1380, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2755, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 97.6875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5501, loss: 0.9653, grad_norm: 4.6541
2022-01-10 19:50:59,621 - depth - INFO - Iter [30900/38400]	lr: 1.804e-05, eta: 0:45:25, time: 0.350, data_time: 0.010, memory: 15411, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1366, decode.aux_loss_ce_5: 0.0007, decode.aux_loss_depth_5: 0.2728, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.3750, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5445, loss: 0.9560, grad_norm: 4.7374
2022-01-10 19:51:33,645 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 19:51:33,646 - depth - INFO - Iter [31000/38400]	lr: 1.760e-05, eta: 0:44:48, time: 0.340, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1357, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2715, decode.loss_ce: 0.0010, decode.ce_acc_level_0: 97.4375, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5418, loss: 0.9513, grad_norm: 4.8348
2022-01-10 19:52:07,540 - depth - INFO - Iter [31100/38400]	lr: 1.715e-05, eta: 0:44:11, time: 0.339, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1351, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2698, decode.loss_ce: 0.0009, decode.ce_acc_level_0: 97.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5385, loss: 0.9456, grad_norm: 4.7709
2022-01-10 19:52:41,623 - depth - INFO - Iter [31200/38400]	lr: 1.672e-05, eta: 0:43:34, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1343, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2683, decode.loss_ce: 0.0010, decode.ce_acc_level_0: 97.6875, decode.ce_acc_level_1: 99.7500, decode.loss_depth: 0.5354, loss: 0.9404, grad_norm: 4.6284
2022-01-10 19:52:55,558 - depth - INFO - Summary:
2022-01-10 19:52:55,561 - depth - INFO - 
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
| 0.8897 | 0.9823 | 0.9954 |  0.1124 | 0.382 | 0.0467 |  0.1391  | 11.234 | 0.0678 |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
2022-01-10 19:52:55,562 - depth - INFO - Iter(val) [82]	a1: 0.8897, a2: 0.9823, a3: 0.9954, abs_rel: 0.1124226525425911, rmse: 0.3820017874240875, log_10: 0.046720318496227264, rmse_log: 0.13905400037765503, silog: 11.2340, sq_rel: 0.0678430050611496
2022-01-10 19:53:29,788 - depth - INFO - Iter [31300/38400]	lr: 1.628e-05, eta: 0:43:01, time: 0.481, data_time: 0.145, memory: 15411, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1354, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2704, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.7500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5396, loss: 0.9473, grad_norm: 5.0589
2022-01-10 19:54:03,835 - depth - INFO - Iter [31400/38400]	lr: 1.585e-05, eta: 0:42:24, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1350, decode.aux_loss_ce_5: 0.0007, decode.aux_loss_depth_5: 0.2689, decode.loss_ce: 0.0009, decode.ce_acc_level_0: 97.3750, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5383, loss: 0.9443, grad_norm: 4.7833
2022-01-10 19:54:37,716 - depth - INFO - Iter [31500/38400]	lr: 1.543e-05, eta: 0:41:47, time: 0.339, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1357, decode.aux_loss_ce_5: 0.0007, decode.aux_loss_depth_5: 0.2708, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.6250, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5406, loss: 0.9492, grad_norm: 4.7002
2022-01-10 19:55:12,419 - depth - INFO - Iter [31600/38400]	lr: 1.501e-05, eta: 0:41:10, time: 0.347, data_time: 0.010, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1330, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2650, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 97.6250, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5293, loss: 0.9289, grad_norm: 4.7021
2022-01-10 19:55:46,597 - depth - INFO - Iter [31700/38400]	lr: 1.460e-05, eta: 0:40:33, time: 0.341, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1379, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2750, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 97.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5498, loss: 0.9645, grad_norm: 4.6371
2022-01-10 19:56:20,941 - depth - INFO - Iter [31800/38400]	lr: 1.419e-05, eta: 0:39:57, time: 0.344, data_time: 0.008, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1359, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2711, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 97.7500, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5418, loss: 0.9504, grad_norm: 4.4782
2022-01-10 19:56:55,479 - depth - INFO - Iter [31900/38400]	lr: 1.378e-05, eta: 0:39:20, time: 0.345, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1315, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2621, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 98.1250, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5234, loss: 0.9187, grad_norm: 4.8740
2022-01-10 19:57:29,725 - depth - INFO - Saving checkpoint at 32000 iterations
2022-01-10 19:57:32,104 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 19:57:32,104 - depth - INFO - Iter [32000/38400]	lr: 1.338e-05, eta: 0:38:44, time: 0.366, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1352, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2701, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 97.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5399, loss: 0.9471, grad_norm: 4.7200
2022-01-10 19:57:45,367 - depth - INFO - Summary:
2022-01-10 19:57:45,370 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8892 | 0.9823 | 0.9956 |  0.1119 | 0.3812 | 0.0465 |  0.1384  | 11.1854 | 0.0667 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 19:57:45,371 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 19:57:45,371 - depth - INFO - Iter(val) [82]	a1: 0.8892, a2: 0.9823, a3: 0.9956, abs_rel: 0.11191321164369583, rmse: 0.38123929500579834, log_10: 0.04650038480758667, rmse_log: 0.13843415677547455, silog: 11.1854, sq_rel: 0.06665809452533722
2022-01-10 19:58:19,419 - depth - INFO - Iter [32100/38400]	lr: 1.299e-05, eta: 0:38:10, time: 0.473, data_time: 0.138, memory: 15411, decode.aux_loss_ce_2: 0.0007, decode.aux_loss_depth_2: 0.1363, decode.aux_loss_ce_5: 0.0008, decode.aux_loss_depth_5: 0.2723, decode.loss_ce: 0.0010, decode.ce_acc_level_0: 97.1250, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5441, loss: 0.9553, grad_norm: 5.1281
2022-01-10 19:58:53,566 - depth - INFO - Iter [32200/38400]	lr: 1.260e-05, eta: 0:37:33, time: 0.341, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1326, decode.aux_loss_ce_5: 0.0007, decode.aux_loss_depth_5: 0.2644, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.4375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5289, loss: 0.9280, grad_norm: 4.4202
2022-01-10 19:59:28,021 - depth - INFO - Iter [32300/38400]	lr: 1.221e-05, eta: 0:36:56, time: 0.344, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1329, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2656, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.4375, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5297, loss: 0.9301, grad_norm: 4.7546
2022-01-10 20:00:02,065 - depth - INFO - Iter [32400/38400]	lr: 1.183e-05, eta: 0:36:19, time: 0.340, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1336, decode.aux_loss_ce_5: 0.0007, decode.aux_loss_depth_5: 0.2666, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.8750, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5320, loss: 0.9342, grad_norm: 4.9166
2022-01-10 20:00:36,561 - depth - INFO - Iter [32500/38400]	lr: 1.146e-05, eta: 0:35:43, time: 0.345, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1323, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2642, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.3125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5274, loss: 0.9254, grad_norm: 4.8472
2022-01-10 20:01:10,584 - depth - INFO - Iter [32600/38400]	lr: 1.109e-05, eta: 0:35:06, time: 0.340, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1355, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2702, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 97.6875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5391, loss: 0.9464, grad_norm: 4.8092
2022-01-10 20:01:44,650 - depth - INFO - Iter [32700/38400]	lr: 1.073e-05, eta: 0:34:29, time: 0.340, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1315, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2625, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 98.1250, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5240, loss: 0.9197, grad_norm: 4.4892
2022-01-10 20:02:18,886 - depth - INFO - Iter [32800/38400]	lr: 1.037e-05, eta: 0:33:53, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1350, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2695, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 98.1875, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5382, loss: 0.9445, grad_norm: 4.9169
2022-01-10 20:02:32,241 - depth - INFO - Summary:
2022-01-10 20:02:32,242 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8875 | 0.9821 | 0.9955 |  0.1132 | 0.3811 | 0.0468 |  0.1392  | 11.2147 | 0.0678 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 20:02:32,242 - depth - INFO - Iter(val) [82]	a1: 0.8875, a2: 0.9821, a3: 0.9955, abs_rel: 0.11319207400083542, rmse: 0.38114696741104126, log_10: 0.04678269475698471, rmse_log: 0.13924002647399902, silog: 11.2147, sq_rel: 0.06783482432365417
2022-01-10 20:03:06,210 - depth - INFO - Iter [32900/38400]	lr: 1.002e-05, eta: 0:33:18, time: 0.473, data_time: 0.139, memory: 15411, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1306, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2608, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 97.5000, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5206, loss: 0.9138, grad_norm: 4.5726
2022-01-10 20:03:40,561 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 20:03:40,561 - depth - INFO - Iter [33000/38400]	lr: 9.668e-06, eta: 0:32:41, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0007, decode.aux_loss_depth_2: 0.1353, decode.aux_loss_ce_5: 0.0009, decode.aux_loss_depth_5: 0.2701, decode.loss_ce: 0.0010, decode.ce_acc_level_0: 97.1875, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5395, loss: 0.9475, grad_norm: 4.7393
2022-01-10 20:04:14,940 - depth - INFO - Iter [33100/38400]	lr: 9.326e-06, eta: 0:32:05, time: 0.344, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1321, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2637, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 97.9375, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5269, loss: 0.9244, grad_norm: 4.3132
2022-01-10 20:04:48,941 - depth - INFO - Iter [33200/38400]	lr: 8.990e-06, eta: 0:31:28, time: 0.340, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1323, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2640, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.2500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5277, loss: 0.9253, grad_norm: 4.5967
2022-01-10 20:05:23,203 - depth - INFO - Iter [33300/38400]	lr: 8.659e-06, eta: 0:30:52, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1315, decode.aux_loss_ce_5: 0.0007, decode.aux_loss_depth_5: 0.2624, decode.loss_ce: 0.0009, decode.ce_acc_level_0: 97.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5247, loss: 0.9209, grad_norm: 4.6040
2022-01-10 20:05:57,362 - depth - INFO - Iter [33400/38400]	lr: 8.334e-06, eta: 0:30:15, time: 0.342, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1331, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2661, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.7500, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5308, loss: 0.9319, grad_norm: 4.9479
2022-01-10 20:06:31,563 - depth - INFO - Iter [33500/38400]	lr: 8.014e-06, eta: 0:29:38, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1328, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2653, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.1250, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5300, loss: 0.9295, grad_norm: 4.8022
2022-01-10 20:07:05,563 - depth - INFO - Saving checkpoint at 33600 iterations
2022-01-10 20:07:08,238 - depth - INFO - Iter [33600/38400]	lr: 7.701e-06, eta: 0:29:02, time: 0.367, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0006, decode.aux_loss_depth_2: 0.1329, decode.aux_loss_ce_5: 0.0007, decode.aux_loss_depth_5: 0.2650, decode.loss_ce: 0.0009, decode.ce_acc_level_0: 97.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5299, loss: 0.9299, grad_norm: 4.7589
2022-01-10 20:07:21,667 - depth - INFO - Summary:
2022-01-10 20:07:21,668 - depth - INFO - 
+------+--------+--------+---------+--------+--------+----------+---------+--------+
|  a1  |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.89 | 0.9826 | 0.9955 |  0.1122 | 0.3799 | 0.0465 |  0.1383  | 11.1326 | 0.067  |
+------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 20:07:21,669 - depth - INFO - Iter(val) [82]	a1: 0.8900, a2: 0.9826, a3: 0.9955, abs_rel: 0.11222746223211288, rmse: 0.3798610270023346, log_10: 0.04648331552743912, rmse_log: 0.138316810131073, silog: 11.1326, sq_rel: 0.06704380363225937
2022-01-10 20:07:56,464 - depth - INFO - Iter [33700/38400]	lr: 7.393e-06, eta: 0:28:27, time: 0.482, data_time: 0.140, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1308, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2611, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.5000, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5218, loss: 0.9155, grad_norm: 4.9216
2022-01-10 20:08:30,853 - depth - INFO - Iter [33800/38400]	lr: 7.091e-06, eta: 0:27:51, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1310, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2616, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.3750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5228, loss: 0.9168, grad_norm: 5.0166
2022-01-10 20:09:04,831 - depth - INFO - Iter [33900/38400]	lr: 6.795e-06, eta: 0:27:14, time: 0.339, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1319, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2633, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.9375, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5257, loss: 0.9220, grad_norm: 4.7994
2022-01-10 20:09:38,801 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 20:09:38,801 - depth - INFO - Iter [34000/38400]	lr: 6.504e-06, eta: 0:26:37, time: 0.340, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1284, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2558, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 97.7500, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5113, loss: 0.8970, grad_norm: 4.3630
2022-01-10 20:10:12,973 - depth - INFO - Iter [34100/38400]	lr: 6.220e-06, eta: 0:26:01, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1319, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2633, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 98.0000, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5261, loss: 0.9232, grad_norm: 4.7443
2022-01-10 20:10:47,219 - depth - INFO - Iter [34200/38400]	lr: 5.942e-06, eta: 0:25:24, time: 0.342, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1307, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2609, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 97.8750, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5208, loss: 0.9140, grad_norm: 4.5169
2022-01-10 20:11:21,583 - depth - INFO - Iter [34300/38400]	lr: 5.669e-06, eta: 0:24:48, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1332, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2657, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.7500, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5316, loss: 0.9320, grad_norm: 4.7434
2022-01-10 20:11:55,811 - depth - INFO - Iter [34400/38400]	lr: 5.403e-06, eta: 0:24:11, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1320, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2634, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 98.3125, decode.ce_acc_level_1: 99.8750, decode.loss_depth: 0.5259, loss: 0.9232, grad_norm: 4.4378
2022-01-10 20:12:09,238 - depth - INFO - Summary:
2022-01-10 20:12:09,239 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8891 | 0.9827 | 0.9955 |  0.1127 | 0.3802 | 0.0466 |  0.1386  | 11.1345 | 0.0674 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 20:12:09,240 - depth - INFO - Iter(val) [82]	a1: 0.8891, a2: 0.9827, a3: 0.9955, abs_rel: 0.11268679052591324, rmse: 0.38015395402908325, log_10: 0.04662875831127167, rmse_log: 0.13857875764369965, silog: 11.1345, sq_rel: 0.06736765056848526
2022-01-10 20:12:43,080 - depth - INFO - Iter [34500/38400]	lr: 5.143e-06, eta: 0:23:36, time: 0.473, data_time: 0.140, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1288, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2571, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.0625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5134, loss: 0.9006, grad_norm: 4.3563
2022-01-10 20:13:17,087 - depth - INFO - Iter [34600/38400]	lr: 4.889e-06, eta: 0:23:00, time: 0.340, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1302, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2599, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.5625, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5192, loss: 0.9105, grad_norm: 4.3986
2022-01-10 20:13:51,220 - depth - INFO - Iter [34700/38400]	lr: 4.641e-06, eta: 0:22:23, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1310, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2615, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5224, loss: 0.9162, grad_norm: 4.3540
2022-01-10 20:14:24,957 - depth - INFO - Iter [34800/38400]	lr: 4.399e-06, eta: 0:21:46, time: 0.337, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1306, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2604, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 98.4375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5207, loss: 0.9132, grad_norm: 4.4382
2022-01-10 20:14:59,226 - depth - INFO - Iter [34900/38400]	lr: 4.164e-06, eta: 0:21:10, time: 0.342, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0005, decode.aux_loss_depth_2: 0.1283, decode.aux_loss_ce_5: 0.0007, decode.aux_loss_depth_5: 0.2560, decode.loss_ce: 0.0009, decode.ce_acc_level_0: 97.5000, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5111, loss: 0.8975, grad_norm: 4.3352
2022-01-10 20:15:33,398 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 20:15:33,399 - depth - INFO - Iter [35000/38400]	lr: 3.934e-06, eta: 0:20:33, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1313, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2623, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.3750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5232, loss: 0.9183, grad_norm: 4.5830
2022-01-10 20:16:07,711 - depth - INFO - Iter [35100/38400]	lr: 3.712e-06, eta: 0:19:57, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1296, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2586, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.4375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5165, loss: 0.9061, grad_norm: 4.2850
2022-01-10 20:16:41,887 - depth - INFO - Saving checkpoint at 35200 iterations
2022-01-10 20:16:44,017 - depth - INFO - Iter [35200/38400]	lr: 3.495e-06, eta: 0:19:21, time: 0.363, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1311, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2617, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.5000, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5226, loss: 0.9166, grad_norm: 4.2657
2022-01-10 20:16:57,196 - depth - INFO - Summary:
2022-01-10 20:16:57,197 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8913 | 0.9828 | 0.9954 |  0.1113 | 0.3784 | 0.0463 |  0.1378  | 11.1408 | 0.0663 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 20:16:59,594 - depth - INFO - Now best checkpoint is saved as best_abs_rel_iter_35200.pth.
2022-01-10 20:16:59,595 - depth - INFO - Best abs_rel is 0.1113 at 35200 iter.
2022-01-10 20:16:59,596 - depth - INFO - Iter(val) [82]	a1: 0.8913, a2: 0.9828, a3: 0.9954, abs_rel: 0.11127839237451553, rmse: 0.3784281313419342, log_10: 0.04627477005124092, rmse_log: 0.13783890008926392, silog: 11.1408, sq_rel: 0.06630749255418777
2022-01-10 20:17:34,273 - depth - INFO - Iter [35300/38400]	lr: 3.285e-06, eta: 0:18:46, time: 0.502, data_time: 0.161, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1278, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2554, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 98.1250, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5102, loss: 0.8950, grad_norm: 4.6884
2022-01-10 20:18:08,842 - depth - INFO - Iter [35400/38400]	lr: 3.081e-06, eta: 0:18:09, time: 0.346, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1306, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2607, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5204, loss: 0.9129, grad_norm: 4.5482
2022-01-10 20:18:43,053 - depth - INFO - Iter [35500/38400]	lr: 2.883e-06, eta: 0:17:33, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1293, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2579, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5152, loss: 0.9036, grad_norm: 4.2170
2022-01-10 20:19:16,976 - depth - INFO - Iter [35600/38400]	lr: 2.692e-06, eta: 0:16:56, time: 0.339, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1299, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2592, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.3750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5179, loss: 0.9084, grad_norm: 4.3462
2022-01-10 20:19:50,967 - depth - INFO - Iter [35700/38400]	lr: 2.508e-06, eta: 0:16:20, time: 0.340, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1274, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2543, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 98.1875, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5080, loss: 0.8914, grad_norm: 4.4427
2022-01-10 20:20:25,675 - depth - INFO - Iter [35800/38400]	lr: 2.330e-06, eta: 0:15:43, time: 0.347, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1304, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2602, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.9375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5200, loss: 0.9118, grad_norm: 4.8205
2022-01-10 20:20:59,952 - depth - INFO - Iter [35900/38400]	lr: 2.158e-06, eta: 0:15:07, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1293, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2584, decode.loss_ce: 0.0004, decode.ce_acc_level_0: 98.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5158, loss: 0.9046, grad_norm: 4.2617
2022-01-10 20:21:34,278 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 20:21:34,278 - depth - INFO - Iter [36000/38400]	lr: 1.993e-06, eta: 0:14:30, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1281, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2558, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.3750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5108, loss: 0.8961, grad_norm: 4.8570
2022-01-10 20:21:47,734 - depth - INFO - Summary:
2022-01-10 20:21:47,734 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8904 | 0.9832 | 0.9955 |  0.112  | 0.3788 | 0.0464 |  0.1381  | 11.1246 | 0.0667 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 20:21:47,735 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 20:21:47,735 - depth - INFO - Iter(val) [82]	a1: 0.8904, a2: 0.9832, a3: 0.9955, abs_rel: 0.11196068674325943, rmse: 0.37878692150115967, log_10: 0.046410419046878815, rmse_log: 0.1380814015865326, silog: 11.1246, sq_rel: 0.06674087047576904
2022-01-10 20:22:21,977 - depth - INFO - Iter [36100/38400]	lr: 1.835e-06, eta: 0:13:55, time: 0.477, data_time: 0.140, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1315, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2622, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.3125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5243, loss: 0.9194, grad_norm: 4.4776
2022-01-10 20:22:56,157 - depth - INFO - Iter [36200/38400]	lr: 1.683e-06, eta: 0:13:18, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1301, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2599, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.3750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5191, loss: 0.9104, grad_norm: 4.5937
2022-01-10 20:23:30,467 - depth - INFO - Iter [36300/38400]	lr: 1.538e-06, eta: 0:12:42, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1287, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2564, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.6875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5125, loss: 0.8987, grad_norm: 4.2558
2022-01-10 20:24:04,514 - depth - INFO - Iter [36400/38400]	lr: 1.399e-06, eta: 0:12:06, time: 0.340, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1294, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2580, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.4375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5153, loss: 0.9041, grad_norm: 4.3946
2022-01-10 20:24:38,559 - depth - INFO - Iter [36500/38400]	lr: 1.267e-06, eta: 0:11:29, time: 0.340, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1255, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2507, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.3750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5006, loss: 0.8781, grad_norm: 4.1199
2022-01-10 20:25:12,782 - depth - INFO - Iter [36600/38400]	lr: 1.142e-06, eta: 0:10:53, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1305, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2603, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.1250, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5204, loss: 0.9127, grad_norm: 4.5246
2022-01-10 20:25:47,225 - depth - INFO - Iter [36700/38400]	lr: 1.023e-06, eta: 0:10:16, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1279, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2554, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.3125, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5097, loss: 0.8944, grad_norm: 4.1907
2022-01-10 20:26:21,805 - depth - INFO - Saving checkpoint at 36800 iterations
2022-01-10 20:26:23,975 - depth - INFO - Iter [36800/38400]	lr: 9.113e-07, eta: 0:09:40, time: 0.368, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1276, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2546, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.4375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5087, loss: 0.8921, grad_norm: 4.2984
2022-01-10 20:26:37,230 - depth - INFO - Summary:
2022-01-10 20:26:37,231 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8898 | 0.9828 | 0.9954 |  0.1126 | 0.3793 | 0.0466 |  0.1384  | 11.1259 | 0.0674 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 20:26:37,232 - depth - INFO - Iter(val) [82]	a1: 0.8898, a2: 0.9828, a3: 0.9954, abs_rel: 0.11258687824010849, rmse: 0.3793255090713501, log_10: 0.04656486213207245, rmse_log: 0.13841693103313446, silog: 11.1259, sq_rel: 0.06742305308580399
2022-01-10 20:27:11,427 - depth - INFO - Iter [36900/38400]	lr: 8.061e-07, eta: 0:09:04, time: 0.474, data_time: 0.137, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1293, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2582, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.8750, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5156, loss: 0.9042, grad_norm: 4.2610
2022-01-10 20:27:45,621 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 20:27:45,622 - depth - INFO - Iter [37000/38400]	lr: 7.076e-07, eta: 0:08:28, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0002, decode.aux_loss_depth_2: 0.1277, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2549, decode.loss_ce: 0.0004, decode.ce_acc_level_0: 99.0625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5096, loss: 0.8932, grad_norm: 4.3981
2022-01-10 20:28:19,939 - depth - INFO - Iter [37100/38400]	lr: 6.158e-07, eta: 0:07:51, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1288, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2572, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.0625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5135, loss: 0.9009, grad_norm: 4.5921
2022-01-10 20:28:53,840 - depth - INFO - Iter [37200/38400]	lr: 5.307e-07, eta: 0:07:15, time: 0.339, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1286, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2566, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.3125, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5125, loss: 0.8993, grad_norm: 4.6584
2022-01-10 20:29:28,162 - depth - INFO - Iter [37300/38400]	lr: 4.525e-07, eta: 0:06:39, time: 0.343, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1289, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2573, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.5625, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5138, loss: 0.9011, grad_norm: 4.0901
2022-01-10 20:30:02,701 - depth - INFO - Iter [37400/38400]	lr: 3.810e-07, eta: 0:06:02, time: 0.346, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1315, decode.aux_loss_ce_5: 0.0006, decode.aux_loss_depth_5: 0.2626, decode.loss_ce: 0.0008, decode.ce_acc_level_0: 97.4375, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5246, loss: 0.9204, grad_norm: 4.4672
2022-01-10 20:30:36,877 - depth - INFO - Iter [37500/38400]	lr: 3.162e-07, eta: 0:05:26, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1289, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2574, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.4375, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5143, loss: 0.9018, grad_norm: 4.2485
2022-01-10 20:31:11,573 - depth - INFO - Iter [37600/38400]	lr: 2.583e-07, eta: 0:04:50, time: 0.347, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1328, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2649, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.6875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5291, loss: 0.9280, grad_norm: 4.4289
2022-01-10 20:31:24,958 - depth - INFO - Summary:
2022-01-10 20:31:24,959 - depth - INFO - 
+--------+--------+--------+---------+------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel | rmse | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+------+--------+----------+---------+--------+
| 0.8892 | 0.9826 | 0.9954 |  0.1129 | 0.38 | 0.0467 |  0.1388  | 11.1589 | 0.0678 |
+--------+--------+--------+---------+------+--------+----------+---------+--------+
2022-01-10 20:31:24,960 - depth - INFO - Iter(val) [82]	a1: 0.8892, a2: 0.9826, a3: 0.9954, abs_rel: 0.11287903040647507, rmse: 0.37999987602233887, log_10: 0.04665828123688698, rmse_log: 0.13876672089099884, silog: 11.1589, sq_rel: 0.06775086373090744
2022-01-10 20:31:59,408 - depth - INFO - Iter [37700/38400]	lr: 2.072e-07, eta: 0:04:14, time: 0.479, data_time: 0.140, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1281, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2558, decode.loss_ce: 0.0007, decode.ce_acc_level_0: 98.4375, decode.ce_acc_level_1: 99.8125, decode.loss_depth: 0.5106, loss: 0.8961, grad_norm: 4.3842
2022-01-10 20:32:33,331 - depth - INFO - Iter [37800/38400]	lr: 1.628e-07, eta: 0:03:37, time: 0.339, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1304, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2601, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.6250, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5199, loss: 0.9117, grad_norm: 4.4162
2022-01-10 20:33:07,853 - depth - INFO - Iter [37900/38400]	lr: 1.253e-07, eta: 0:03:01, time: 0.345, data_time: 0.007, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1279, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2553, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.8125, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5101, loss: 0.8946, grad_norm: 4.7321
2022-01-10 20:33:42,060 - depth - INFO - Exp name: binsformer_full_fpn_wcls_swint_nyu_n64.py
2022-01-10 20:33:42,061 - depth - INFO - Iter [38000/38400]	lr: 9.461e-08, eta: 0:02:25, time: 0.342, data_time: 0.006, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1290, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2576, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.7500, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5147, loss: 0.9027, grad_norm: 4.5339
2022-01-10 20:34:16,443 - depth - INFO - Iter [38100/38400]	lr: 7.072e-08, eta: 0:01:48, time: 0.344, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1281, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2557, decode.loss_ce: 0.0005, decode.ce_acc_level_0: 98.8750, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5107, loss: 0.8959, grad_norm: 4.3277
2022-01-10 20:34:50,693 - depth - INFO - Iter [38200/38400]	lr: 5.365e-08, eta: 0:01:12, time: 0.342, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0004, decode.aux_loss_depth_2: 0.1267, decode.aux_loss_ce_5: 0.0005, decode.aux_loss_depth_5: 0.2528, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.4375, decode.ce_acc_level_1: 99.9375, decode.loss_depth: 0.5052, loss: 0.8861, grad_norm: 4.3032
2022-01-10 20:35:24,951 - depth - INFO - Iter [38300/38400]	lr: 4.341e-08, eta: 0:00:36, time: 0.343, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1273, decode.aux_loss_ce_5: 0.0004, decode.aux_loss_depth_5: 0.2540, decode.loss_ce: 0.0006, decode.ce_acc_level_0: 98.2500, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5072, loss: 0.8898, grad_norm: 4.3142
2022-01-10 20:35:59,417 - depth - INFO - Saving checkpoint at 38400 iterations
2022-01-10 20:36:01,598 - depth - INFO - Iter [38400/38400]	lr: 4.000e-08, eta: 0:00:00, time: 0.367, data_time: 0.005, memory: 15411, decode.aux_loss_ce_2: 0.0003, decode.aux_loss_depth_2: 0.1314, decode.aux_loss_ce_5: 0.0003, decode.aux_loss_depth_5: 0.2621, decode.loss_ce: 0.0004, decode.ce_acc_level_0: 99.1875, decode.ce_acc_level_1: 100.0000, decode.loss_depth: 0.5240, loss: 0.9185, grad_norm: 4.7746
2022-01-10 20:36:15,101 - depth - INFO - Summary:
2022-01-10 20:36:15,102 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8898 | 0.9827 | 0.9955 |  0.1126 | 0.3794 | 0.0466 |  0.1386  | 11.1559 | 0.0672 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-01-10 20:36:15,102 - depth - INFO - Iter(val) [82]	a1: 0.8898, a2: 0.9827, a3: 0.9955, abs_rel: 0.11255152523517609, rmse: 0.3794340193271637, log_10: 0.04660336673259735, rmse_log: 0.13855251669883728, silog: 11.1559, sq_rel: 0.06723781675100327
