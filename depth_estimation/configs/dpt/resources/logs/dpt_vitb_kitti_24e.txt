2022-06-05 00:25:59,134 - depth - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.11 (default, Jul 27 2021, 14:32:16) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: Tesla V100-SXM2-32GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.1, V10.1.243
GCC: gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
PyTorch: 1.6.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.3
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.7.0
OpenCV: 4.5.5
MMCV: 1.3.13
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.1
Depth: 0.0.0+37d5bde
------------------------------------------------------------

2022-06-05 00:25:59,135 - depth - INFO - Distributed training: True
2022-06-05 00:25:59,350 - depth - INFO - Config:
model = dict(
    type='DepthEncoderDecoder',
    pretrained='nfs/checkpoints/jx_vit_base_p16_224-80ecf9dd.pth',
    backbone=dict(
        type='VisionTransformer',
        img_size=224,
        embed_dims=768,
        num_layers=12,
        num_heads=12,
        out_indices=(2, 5, 8, 11),
        final_norm=False,
        with_cls_token=True,
        output_cls_token=True),
    decode_head=dict(
        type='DPTHead',
        in_channels=(768, 768, 768, 768),
        channels=256,
        embed_dims=768,
        post_process_channels=[96, 192, 384, 768],
        readout_type='project',
        norm_cfg=None,
        min_depth=0.001,
        max_depth=80,
        loss_decode=dict(
            type='SigLoss', valid_mask=True, loss_weight=1.0, warm_up=True)),
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
dataset_type = 'KITTIDataset'
data_root = 'data/kitti'
img_norm_cfg = dict(
    mean=[127.5, 127.5, 127.5], std=[127.5, 127.5, 127.5], to_rgb=True)
crop_size = (352, 704)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='DepthLoadAnnotations'),
    dict(type='LoadKITTICamIntrinsic'),
    dict(type='KBCrop', depth=True),
    dict(type='RandomRotate', prob=0.5, degree=2.5),
    dict(type='RandomFlip', prob=0.5),
    dict(type='RandomCrop', crop_size=(352, 704)),
    dict(
        type='ColorAug',
        prob=0.5,
        gamma_range=[0.9, 1.1],
        brightness_range=[0.9, 1.1],
        color_range=[0.9, 1.1]),
    dict(
        type='Normalize',
        mean=[127.5, 127.5, 127.5],
        std=[127.5, 127.5, 127.5],
        to_rgb=True),
    dict(type='DefaultFormatBundle'),
    dict(
        type='Collect',
        keys=['img', 'depth_gt'],
        meta_keys=('filename', 'ori_filename', 'ori_shape', 'img_shape',
                   'pad_shape', 'scale_factor', 'flip', 'flip_direction',
                   'img_norm_cfg', 'cam_intrinsic'))
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadKITTICamIntrinsic'),
    dict(type='KBCrop', depth=False),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1216, 352),
        flip=True,
        flip_direction='horizontal',
        transforms=[
            dict(type='RandomFlip', direction='horizontal'),
            dict(
                type='Normalize',
                mean=[127.5, 127.5, 127.5],
                std=[127.5, 127.5, 127.5],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(
                type='Collect',
                keys=['img'],
                meta_keys=('filename', 'ori_filename', 'ori_shape',
                           'img_shape', 'pad_shape', 'scale_factor', 'flip',
                           'flip_direction', 'img_norm_cfg', 'cam_intrinsic'))
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='KITTIDataset',
        data_root='data/kitti',
        img_dir='input',
        ann_dir='gt_depth',
        depth_scale=256,
        split='kitti_eigen_train.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='DepthLoadAnnotations'),
            dict(type='LoadKITTICamIntrinsic'),
            dict(type='KBCrop', depth=True),
            dict(type='RandomRotate', prob=0.5, degree=2.5),
            dict(type='RandomFlip', prob=0.5),
            dict(type='RandomCrop', crop_size=(352, 704)),
            dict(
                type='ColorAug',
                prob=0.5,
                gamma_range=[0.9, 1.1],
                brightness_range=[0.9, 1.1],
                color_range=[0.9, 1.1]),
            dict(
                type='Normalize',
                mean=[127.5, 127.5, 127.5],
                std=[127.5, 127.5, 127.5],
                to_rgb=True),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'depth_gt'],
                meta_keys=('filename', 'ori_filename', 'ori_shape',
                           'img_shape', 'pad_shape', 'scale_factor', 'flip',
                           'flip_direction', 'img_norm_cfg', 'cam_intrinsic'))
        ],
        garg_crop=True,
        eigen_crop=False,
        min_depth=0.001,
        max_depth=80),
    val=dict(
        type='KITTIDataset',
        data_root='data/kitti',
        img_dir='input',
        ann_dir='gt_depth',
        depth_scale=256,
        split='kitti_eigen_test.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadKITTICamIntrinsic'),
            dict(type='KBCrop', depth=False),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1216, 352),
                flip=True,
                flip_direction='horizontal',
                transforms=[
                    dict(type='RandomFlip', direction='horizontal'),
                    dict(
                        type='Normalize',
                        mean=[127.5, 127.5, 127.5],
                        std=[127.5, 127.5, 127.5],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(
                        type='Collect',
                        keys=['img'],
                        meta_keys=('filename', 'ori_filename', 'ori_shape',
                                   'img_shape', 'pad_shape', 'scale_factor',
                                   'flip', 'flip_direction', 'img_norm_cfg',
                                   'cam_intrinsic'))
                ])
        ],
        garg_crop=True,
        eigen_crop=False,
        min_depth=0.001,
        max_depth=80),
    test=dict(
        type='KITTIDataset',
        data_root='data/kitti',
        img_dir='input',
        ann_dir='gt_depth',
        depth_scale=256,
        split='kitti_eigen_test.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadKITTICamIntrinsic'),
            dict(type='KBCrop', depth=False),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1216, 352),
                flip=True,
                flip_direction='horizontal',
                transforms=[
                    dict(type='RandomFlip', direction='horizontal'),
                    dict(
                        type='Normalize',
                        mean=[127.5, 127.5, 127.5],
                        std=[127.5, 127.5, 127.5],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(
                        type='Collect',
                        keys=['img'],
                        meta_keys=('filename', 'ori_filename', 'ori_shape',
                                   'img_shape', 'pad_shape', 'scale_factor',
                                   'flip', 'flip_direction', 'img_norm_cfg',
                                   'cam_intrinsic'))
                ])
        ],
        garg_crop=True,
        eigen_crop=False,
        min_depth=0.001,
        max_depth=80))
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook', by_epoch=True),
        dict(type='TensorboardImageLoggerHook', by_epoch=True)
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
max_lr = 0.0001
optimizer = dict(
    type='AdamW',
    lr=0.0001,
    betas=(0.9, 0.999),
    weight_decay=0.01,
    paramwise_cfg=dict(
        custom_keys=dict(
            pos_embed=dict(decay_mult=0.0),
            cls_token=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0),
            backbone=dict(lr_mult=0.1))))
lr_config = dict(
    policy='OneCycle',
    max_lr=0.0001,
    div_factor=25,
    final_div_factor=100,
    by_epoch=False)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
runner = dict(type='EpochBasedRunner', max_epochs=24)
checkpoint_config = dict(by_epoch=True, max_keep_ckpts=2, interval=1)
evaluation = dict(by_epoch=True, interval=1, pre_eval=True)
momentum_config = dict(policy='OneCycle')
work_dir = 'nfs/saves/dpt/kitti'
gpu_ids = range(0, 1)

2022-06-05 00:26:00,312 - depth - INFO - Use load_from_local loader
2022-06-05 00:26:00,748 - depth - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: ln1.bias, ln1.weight

Name of parameter - Initialization information

backbone.cls_token - torch.Size([1, 1, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.pos_embed - torch.Size([1, 197, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.patch_embed.projection.weight - torch.Size([768, 3, 16, 16]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.patch_embed.projection.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

decode_head.conv_depth.head.0.weight - torch.Size([128, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_depth.head.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_depth.head.2.weight - torch.Size([32, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_depth.head.2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_depth.head.4.weight - torch.Size([1, 32, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_depth.head.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.projects.0.conv.weight - torch.Size([96, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.projects.0.conv.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.projects.1.conv.weight - torch.Size([192, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.projects.1.conv.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.projects.2.conv.weight - torch.Size([384, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.projects.2.conv.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.projects.3.conv.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.projects.3.conv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.resize_layers.0.weight - torch.Size([96, 96, 4, 4]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.resize_layers.0.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.resize_layers.1.weight - torch.Size([192, 192, 2, 2]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.resize_layers.1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.resize_layers.3.weight - torch.Size([768, 768, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.resize_layers.3.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.readout_projects.0.0.weight - torch.Size([768, 1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.readout_projects.0.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.readout_projects.1.0.weight - torch.Size([768, 1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.readout_projects.1.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.readout_projects.2.0.weight - torch.Size([768, 1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.readout_projects.2.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.readout_projects.3.0.weight - torch.Size([768, 1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.readout_projects.3.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.convs.0.conv.weight - torch.Size([256, 96, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.convs.1.conv.weight - torch.Size([256, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.convs.2.conv.weight - torch.Size([256, 384, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.convs.3.conv.weight - torch.Size([256, 768, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.0.project.conv.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.0.project.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.0.res_conv_unit2.conv1.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.0.res_conv_unit2.conv2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.1.project.conv.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.1.project.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.1.res_conv_unit1.conv1.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.1.res_conv_unit1.conv2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.1.res_conv_unit2.conv1.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.1.res_conv_unit2.conv2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.2.project.conv.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.2.project.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.2.res_conv_unit1.conv1.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.2.res_conv_unit1.conv2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.2.res_conv_unit2.conv1.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.2.res_conv_unit2.conv2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.3.project.conv.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.3.project.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.3.res_conv_unit1.conv1.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.3.res_conv_unit1.conv2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.3.res_conv_unit2.conv1.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.3.res_conv_unit2.conv2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.project.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ConvModule  

decode_head.project.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  
2022-06-05 00:26:00,824 - depth - INFO - DepthEncoderDecoder(
  (backbone): VisionTransformer(
    (patch_embed): PatchEmbed(
      (adap_padding): AdaptivePadding()
      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    )
    (drop_after_pos): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (1): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (2): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (3): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (4): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (5): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (6): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (7): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (8): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (9): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (10): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (11): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
    )
  )
  init_cfg={'type': 'Pretrained', 'checkpoint': 'nfs/checkpoints/jx_vit_base_p16_224-80ecf9dd.pth'}
  (decode_head): DPTHead(
    align_corners=False
    (loss_decode): SigLoss()
    (conv_depth): HeadDepth(
      (head): Sequential(
        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): Interpolate()
        (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (relu): ReLU()
    (sigmoid): Sigmoid()
    (reassemble_blocks): ReassembleBlocks(
      (projects): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(768, 96, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): ConvModule(
          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): ConvModule(
          (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
        )
        (3): ConvModule(
          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (resize_layers): ModuleList(
        (0): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(4, 4))
        (1): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))
        (2): Identity()
        (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
      (readout_projects): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=1536, out_features=768, bias=True)
          (1): GELU()
        )
        (1): Sequential(
          (0): Linear(in_features=1536, out_features=768, bias=True)
          (1): GELU()
        )
        (2): Sequential(
          (0): Linear(in_features=1536, out_features=768, bias=True)
          (1): GELU()
        )
        (3): Sequential(
          (0): Linear(in_features=1536, out_features=768, bias=True)
          (1): GELU()
        )
      )
    )
    (convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): ConvModule(
        (conv): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): ConvModule(
        (conv): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): ConvModule(
        (conv): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (fusion_blocks): ModuleList(
      (0): FeatureFusionBlock(
        (project): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (res_conv_unit1): None
        (res_conv_unit2): PreActResidualConvUnit(
          (conv1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
        )
      )
      (1): FeatureFusionBlock(
        (project): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (res_conv_unit1): PreActResidualConvUnit(
          (conv1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
        )
        (res_conv_unit2): PreActResidualConvUnit(
          (conv1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
        )
      )
      (2): FeatureFusionBlock(
        (project): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (res_conv_unit1): PreActResidualConvUnit(
          (conv1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
        )
        (res_conv_unit2): PreActResidualConvUnit(
          (conv1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
        )
      )
      (3): FeatureFusionBlock(
        (project): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (res_conv_unit1): PreActResidualConvUnit(
          (conv1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
        )
        (res_conv_unit2): PreActResidualConvUnit(
          (conv1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
        )
      )
    )
    (project): ConvModule(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (activate): ReLU(inplace=True)
    )
  )
)
2022-06-05 00:26:00,868 - depth - INFO - Loaded 23158 images. Totally 0 invalid pairs are filtered
2022-06-05 00:26:06,224 - depth - INFO - Loaded 652 images. Totally 45 invalid pairs are filtered
2022-06-05 00:26:06,225 - depth - INFO - Start running, host: root@mono3d-da-2-master-0, work_dir: /nfs/lizhenyu2/codes/Monocular-Depth-Estimation-Toolbox/nfs/saves/dpt/kitti
2022-06-05 00:26:06,225 - depth - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) OneCycleLrUpdaterHook              
(HIGH        ) OneCycleMomentumUpdaterHook        
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardImageLoggerHook         
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) OneCycleLrUpdaterHook              
(HIGH        ) OneCycleMomentumUpdaterHook        
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardImageLoggerHook         
 -------------------- 
before_train_iter:
(VERY_HIGH   ) OneCycleLrUpdaterHook              
(HIGH        ) OneCycleMomentumUpdaterHook        
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardImageLoggerHook         
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardImageLoggerHook         
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardImageLoggerHook         
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardImageLoggerHook         
 -------------------- 
after_run:
(VERY_LOW    ) TensorboardImageLoggerHook         
 -------------------- 
2022-06-05 00:26:06,226 - depth - INFO - workflow: [('train', 1)], max: 24 epochs
2022-06-05 00:26:41,288 - depth - INFO - Epoch [1][50/1447]	lr: 4.005e-06, eta: 6:43:39, time: 0.698, data_time: 0.230, memory: 12862, decode.loss_depth: 0.6919, loss: 0.6919, grad_norm: 88.9263
2022-06-05 00:27:03,713 - depth - INFO - Epoch [1][100/1447]	lr: 4.021e-06, eta: 5:30:57, time: 0.448, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1138, loss: 0.1138, grad_norm: 53.6335
2022-06-05 00:27:25,882 - depth - INFO - Epoch [1][150/1447]	lr: 4.048e-06, eta: 5:05:11, time: 0.442, data_time: 0.009, memory: 12862, decode.loss_depth: 0.6037, loss: 0.6037, grad_norm: 19.6283
2022-06-05 00:27:48,804 - depth - INFO - Epoch [1][200/1447]	lr: 4.086e-06, eta: 4:54:42, time: 0.460, data_time: 0.009, memory: 12862, decode.loss_depth: 0.3544, loss: 0.3544, grad_norm: 9.2687
2022-06-05 00:28:11,806 - depth - INFO - Epoch [1][250/1447]	lr: 4.135e-06, eta: 4:48:19, time: 0.460, data_time: 0.008, memory: 12862, decode.loss_depth: 0.2638, loss: 0.2638, grad_norm: 10.5460
2022-06-05 00:28:34,261 - depth - INFO - Epoch [1][300/1447]	lr: 4.195e-06, eta: 4:42:52, time: 0.449, data_time: 0.008, memory: 12862, decode.loss_depth: 0.2268, loss: 0.2268, grad_norm: 10.9680
2022-06-05 00:28:52,156 - depth - INFO - Epoch [1][350/1447]	lr: 4.266e-06, eta: 4:31:24, time: 0.358, data_time: 0.008, memory: 12862, decode.loss_depth: 0.2101, loss: 0.2101, grad_norm: 11.2779
2022-06-05 00:29:10,192 - depth - INFO - Epoch [1][400/1447]	lr: 4.347e-06, eta: 4:22:55, time: 0.361, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1932, loss: 0.1932, grad_norm: 6.8293
2022-06-05 00:29:32,703 - depth - INFO - Epoch [1][450/1447]	lr: 4.439e-06, eta: 4:21:57, time: 0.450, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1831, loss: 0.1831, grad_norm: 10.7850
2022-06-05 00:29:54,606 - depth - INFO - Epoch [1][500/1447]	lr: 4.542e-06, eta: 4:20:24, time: 0.438, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1784, loss: 0.1784, grad_norm: 8.0891
2022-06-05 00:30:16,801 - depth - INFO - Epoch [1][550/1447]	lr: 4.656e-06, eta: 4:19:22, time: 0.444, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1792, loss: 0.1792, grad_norm: 10.6597
2022-06-05 00:30:34,694 - depth - INFO - Epoch [1][600/1447]	lr: 4.781e-06, eta: 4:14:22, time: 0.358, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1742, loss: 0.1742, grad_norm: 7.6006
2022-06-05 00:30:56,834 - depth - INFO - Epoch [1][650/1447]	lr: 4.916e-06, eta: 4:13:48, time: 0.443, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1691, loss: 0.1691, grad_norm: 8.6255
2022-06-05 00:31:19,606 - depth - INFO - Epoch [1][700/1447]	lr: 5.063e-06, eta: 4:13:46, time: 0.455, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1656, loss: 0.1656, grad_norm: 7.6662
2022-06-05 00:31:42,021 - depth - INFO - Epoch [1][750/1447]	lr: 5.219e-06, eta: 4:13:26, time: 0.449, data_time: 0.010, memory: 12862, decode.loss_depth: 0.1646, loss: 0.1646, grad_norm: 15.5334
2022-06-05 00:32:04,882 - depth - INFO - Epoch [1][800/1447]	lr: 5.387e-06, eta: 4:13:24, time: 0.457, data_time: 0.010, memory: 12862, decode.loss_depth: 0.1608, loss: 0.1608, grad_norm: 9.0064
2022-06-05 00:32:22,689 - depth - INFO - Epoch [1][850/1447]	lr: 5.565e-06, eta: 4:09:58, time: 0.356, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1554, loss: 0.1554, grad_norm: 7.9713
2022-06-05 00:32:40,858 - depth - INFO - Epoch [1][900/1447]	lr: 5.753e-06, eta: 4:07:07, time: 0.363, data_time: 0.006, memory: 12862, decode.loss_depth: 0.1561, loss: 0.1561, grad_norm: 9.8240
2022-06-05 00:33:03,571 - depth - INFO - Epoch [1][950/1447]	lr: 5.952e-06, eta: 4:07:13, time: 0.454, data_time: 0.010, memory: 12862, decode.loss_depth: 0.1554, loss: 0.1554, grad_norm: 14.0137
2022-06-05 00:33:21,364 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 00:33:21,365 - depth - INFO - Epoch [1][1000/1447]	lr: 6.162e-06, eta: 4:04:31, time: 0.356, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1520, loss: 0.1520, grad_norm: 8.5342
2022-06-05 00:33:39,351 - depth - INFO - Epoch [1][1050/1447]	lr: 6.382e-06, eta: 4:02:08, time: 0.359, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1475, loss: 0.1475, grad_norm: 9.2225
2022-06-05 00:34:01,853 - depth - INFO - Epoch [1][1100/1447]	lr: 6.612e-06, eta: 4:02:15, time: 0.450, data_time: 0.009, memory: 12862, decode.loss_depth: 0.1475, loss: 0.1475, grad_norm: 9.0747
2022-06-05 00:34:24,485 - depth - INFO - Epoch [1][1150/1447]	lr: 6.853e-06, eta: 4:02:24, time: 0.453, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1411, loss: 0.1411, grad_norm: 6.5983
2022-06-05 00:34:46,726 - depth - INFO - Epoch [1][1200/1447]	lr: 7.104e-06, eta: 4:02:18, time: 0.445, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1429, loss: 0.1429, grad_norm: 9.5027
2022-06-05 00:35:08,990 - depth - INFO - Epoch [1][1250/1447]	lr: 7.365e-06, eta: 4:02:12, time: 0.445, data_time: 0.010, memory: 12862, decode.loss_depth: 0.1449, loss: 0.1449, grad_norm: 7.3595
2022-06-05 00:35:26,829 - depth - INFO - Epoch [1][1300/1447]	lr: 7.636e-06, eta: 4:00:11, time: 0.357, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1412, loss: 0.1412, grad_norm: 7.4767
2022-06-05 00:35:49,456 - depth - INFO - Epoch [1][1350/1447]	lr: 7.918e-06, eta: 4:00:16, time: 0.452, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1383, loss: 0.1383, grad_norm: 13.1107
2022-06-05 00:36:07,286 - depth - INFO - Epoch [1][1400/1447]	lr: 8.209e-06, eta: 3:58:25, time: 0.357, data_time: 0.006, memory: 12862, decode.loss_depth: 0.1354, loss: 0.1354, grad_norm: 7.5446
2022-06-05 00:36:29,219 - depth - INFO - Saving checkpoint at 1 epochs
2022-06-05 00:37:00,396 - depth - INFO - Summary:
2022-06-05 00:37:00,397 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8831 | 0.9801 | 0.9959 |  0.1087 | 3.4897 | 0.0461 |  0.1498  | 13.8321 | 0.5078 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-05 00:37:00,397 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 00:37:00,397 - depth - INFO - Epoch(val) [1][82]	a1: 0.8831, a2: 0.9801, a3: 0.9959, abs_rel: 0.10870340466499329, rmse: 3.489668130874634, log_10: 0.04606403782963753, rmse_log: 0.14980508387088776, silog: 13.8321, sq_rel: 0.5077571868896484
2022-06-05 00:37:31,963 - depth - INFO - Epoch [2][50/1447]	lr: 8.803e-06, eta: 3:53:59, time: 0.631, data_time: 0.204, memory: 12862, decode.loss_depth: 0.1344, loss: 0.1344, grad_norm: 7.2889
2022-06-05 00:37:54,948 - depth - INFO - Epoch [2][100/1447]	lr: 9.123e-06, eta: 3:54:18, time: 0.460, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1329, loss: 0.1329, grad_norm: 6.7973
2022-06-05 00:38:13,362 - depth - INFO - Epoch [2][150/1447]	lr: 9.453e-06, eta: 3:53:00, time: 0.368, data_time: 0.010, memory: 12862, decode.loss_depth: 0.1322, loss: 0.1322, grad_norm: 6.2982
2022-06-05 00:38:36,148 - depth - INFO - Epoch [2][200/1447]	lr: 9.793e-06, eta: 3:53:12, time: 0.456, data_time: 0.012, memory: 12862, decode.loss_depth: 0.1301, loss: 0.1301, grad_norm: 6.2116
2022-06-05 00:38:58,744 - depth - INFO - Epoch [2][250/1447]	lr: 1.014e-05, eta: 3:53:19, time: 0.452, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1292, loss: 0.1292, grad_norm: 6.5918
2022-06-05 00:39:21,016 - depth - INFO - Epoch [2][300/1447]	lr: 1.050e-05, eta: 3:53:18, time: 0.445, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1313, loss: 0.1313, grad_norm: 5.8511
2022-06-05 00:39:39,161 - depth - INFO - Epoch [2][350/1447]	lr: 1.087e-05, eta: 3:52:01, time: 0.363, data_time: 0.009, memory: 12862, decode.loss_depth: 0.1271, loss: 0.1271, grad_norm: 5.7969
2022-06-05 00:40:01,947 - depth - INFO - Epoch [2][400/1447]	lr: 1.125e-05, eta: 3:52:09, time: 0.456, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1260, loss: 0.1260, grad_norm: 8.8091
2022-06-05 00:40:25,060 - depth - INFO - Epoch [2][450/1447]	lr: 1.163e-05, eta: 3:52:21, time: 0.462, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1261, loss: 0.1261, grad_norm: 8.0579
2022-06-05 00:40:43,292 - depth - INFO - Epoch [2][500/1447]	lr: 1.203e-05, eta: 3:51:10, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1236, loss: 0.1236, grad_norm: 5.7898
2022-06-05 00:41:05,920 - depth - INFO - Epoch [2][550/1447]	lr: 1.244e-05, eta: 3:51:12, time: 0.452, data_time: 0.009, memory: 12862, decode.loss_depth: 0.1259, loss: 0.1259, grad_norm: 4.9175
2022-06-05 00:41:28,980 - depth - INFO - Epoch [2][600/1447]	lr: 1.285e-05, eta: 3:51:21, time: 0.461, data_time: 0.009, memory: 12862, decode.loss_depth: 0.1247, loss: 0.1247, grad_norm: 6.9166
2022-06-05 00:41:47,098 - depth - INFO - Epoch [2][650/1447]	lr: 1.327e-05, eta: 3:50:11, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1233, loss: 0.1233, grad_norm: 6.9246
2022-06-05 00:42:05,694 - depth - INFO - Epoch [2][700/1447]	lr: 1.371e-05, eta: 3:49:11, time: 0.371, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1212, loss: 0.1212, grad_norm: 5.6213
2022-06-05 00:42:28,310 - depth - INFO - Epoch [2][750/1447]	lr: 1.415e-05, eta: 3:49:12, time: 0.453, data_time: 0.009, memory: 12862, decode.loss_depth: 0.1218, loss: 0.1218, grad_norm: 4.4151
2022-06-05 00:42:46,631 - depth - INFO - Epoch [2][800/1447]	lr: 1.460e-05, eta: 3:48:11, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.1190, loss: 0.1190, grad_norm: 3.7322
2022-06-05 00:43:09,153 - depth - INFO - Epoch [2][850/1447]	lr: 1.505e-05, eta: 3:48:10, time: 0.450, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1180, loss: 0.1180, grad_norm: 5.1849
2022-06-05 00:43:31,919 - depth - INFO - Epoch [2][900/1447]	lr: 1.552e-05, eta: 3:48:12, time: 0.455, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1193, loss: 0.1193, grad_norm: 4.0362
2022-06-05 00:43:54,888 - depth - INFO - Epoch [2][950/1447]	lr: 1.599e-05, eta: 3:48:15, time: 0.459, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1174, loss: 0.1174, grad_norm: 5.9401
2022-06-05 00:44:17,959 - depth - INFO - Epoch [2][1000/1447]	lr: 1.648e-05, eta: 3:48:19, time: 0.462, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1177, loss: 0.1177, grad_norm: 4.7560
2022-06-05 00:44:36,049 - depth - INFO - Epoch [2][1050/1447]	lr: 1.697e-05, eta: 3:47:17, time: 0.362, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1146, loss: 0.1146, grad_norm: 5.0138
2022-06-05 00:44:58,945 - depth - INFO - Epoch [2][1100/1447]	lr: 1.747e-05, eta: 3:47:18, time: 0.458, data_time: 0.010, memory: 12862, decode.loss_depth: 0.1151, loss: 0.1151, grad_norm: 5.3373
2022-06-05 00:45:21,812 - depth - INFO - Epoch [2][1150/1447]	lr: 1.797e-05, eta: 3:47:18, time: 0.457, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1136, loss: 0.1136, grad_norm: 5.0496
2022-06-05 00:45:44,750 - depth - INFO - Epoch [2][1200/1447]	lr: 1.849e-05, eta: 3:47:17, time: 0.459, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1133, loss: 0.1133, grad_norm: 4.4377
2022-06-05 00:46:07,807 - depth - INFO - Epoch [2][1250/1447]	lr: 1.901e-05, eta: 3:47:17, time: 0.461, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1113, loss: 0.1113, grad_norm: 3.2221
2022-06-05 00:46:26,186 - depth - INFO - Epoch [2][1300/1447]	lr: 1.954e-05, eta: 3:46:22, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1122, loss: 0.1122, grad_norm: 4.3142
2022-06-05 00:46:44,752 - depth - INFO - Epoch [2][1350/1447]	lr: 2.008e-05, eta: 3:45:31, time: 0.371, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1136, loss: 0.1136, grad_norm: 6.0147
2022-06-05 00:47:03,197 - depth - INFO - Epoch [2][1400/1447]	lr: 2.062e-05, eta: 3:44:39, time: 0.369, data_time: 0.009, memory: 12862, decode.loss_depth: 0.1099, loss: 0.1099, grad_norm: 3.4433
2022-06-05 00:47:21,066 - depth - INFO - Saving checkpoint at 2 epochs
2022-06-05 00:47:50,450 - depth - INFO - Summary:
2022-06-05 00:47:50,451 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.9192 | 0.9877 | 0.9973 |  0.0883 | 3.0684 | 0.0372 |  0.1264  | 11.5771 | 0.3774 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-05 00:47:50,451 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 00:47:50,452 - depth - INFO - Epoch(val) [2][82]	a1: 0.9192, a2: 0.9877, a3: 0.9973, abs_rel: 0.08833696693181992, rmse: 3.068413019180298, log_10: 0.037212081253528595, rmse_log: 0.12644438445568085, silog: 11.5771, sq_rel: 0.3774172067642212
2022-06-05 00:48:22,863 - depth - INFO - Epoch [3][50/1447]	lr: 2.170e-05, eta: 3:42:25, time: 0.648, data_time: 0.202, memory: 12862, decode.loss_depth: 0.1110, loss: 0.1110, grad_norm: 4.8637
2022-06-05 00:48:41,279 - depth - INFO - Epoch [3][100/1447]	lr: 2.226e-05, eta: 3:41:36, time: 0.368, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1111, loss: 0.1111, grad_norm: 3.7423
2022-06-05 00:49:04,159 - depth - INFO - Epoch [3][150/1447]	lr: 2.283e-05, eta: 3:41:35, time: 0.458, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1073, loss: 0.1073, grad_norm: 3.5730
2022-06-05 00:49:22,647 - depth - INFO - Epoch [3][200/1447]	lr: 2.341e-05, eta: 3:40:49, time: 0.370, data_time: 0.009, memory: 12862, decode.loss_depth: 0.1078, loss: 0.1078, grad_norm: 2.9369
2022-06-05 00:49:45,260 - depth - INFO - Epoch [3][250/1447]	lr: 2.399e-05, eta: 3:40:45, time: 0.452, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1083, loss: 0.1083, grad_norm: 3.6438
2022-06-05 00:50:08,025 - depth - INFO - Epoch [3][300/1447]	lr: 2.459e-05, eta: 3:40:42, time: 0.455, data_time: 0.015, memory: 12862, decode.loss_depth: 0.1073, loss: 0.1073, grad_norm: 2.6268
2022-06-05 00:50:26,546 - depth - INFO - Epoch [3][350/1447]	lr: 2.518e-05, eta: 3:39:56, time: 0.370, data_time: 0.009, memory: 12862, decode.loss_depth: 0.1056, loss: 0.1056, grad_norm: 3.1146
2022-06-05 00:50:49,452 - depth - INFO - Epoch [3][400/1447]	lr: 2.579e-05, eta: 3:39:54, time: 0.459, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1057, loss: 0.1057, grad_norm: 2.9977
2022-06-05 00:51:12,211 - depth - INFO - Epoch [3][450/1447]	lr: 2.640e-05, eta: 3:39:50, time: 0.455, data_time: 0.009, memory: 12862, decode.loss_depth: 0.1047, loss: 0.1047, grad_norm: 3.1712
2022-06-05 00:51:35,271 - depth - INFO - Epoch [3][500/1447]	lr: 2.701e-05, eta: 3:39:48, time: 0.461, data_time: 0.009, memory: 12862, decode.loss_depth: 0.1028, loss: 0.1028, grad_norm: 2.2227
2022-06-05 00:51:58,162 - depth - INFO - Epoch [3][550/1447]	lr: 2.763e-05, eta: 3:39:43, time: 0.458, data_time: 0.014, memory: 12862, decode.loss_depth: 0.1053, loss: 0.1053, grad_norm: 4.0303
2022-06-05 00:52:21,079 - depth - INFO - Epoch [3][600/1447]	lr: 2.826e-05, eta: 3:39:39, time: 0.458, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1032, loss: 0.1032, grad_norm: 2.5657
2022-06-05 00:52:44,273 - depth - INFO - Epoch [3][650/1447]	lr: 2.889e-05, eta: 3:39:36, time: 0.464, data_time: 0.012, memory: 12862, decode.loss_depth: 0.1018, loss: 0.1018, grad_norm: 3.0518
2022-06-05 00:53:07,294 - depth - INFO - Epoch [3][700/1447]	lr: 2.953e-05, eta: 3:39:31, time: 0.460, data_time: 0.009, memory: 12862, decode.loss_depth: 0.1029, loss: 0.1029, grad_norm: 3.4749
2022-06-05 00:53:30,377 - depth - INFO - Epoch [3][750/1447]	lr: 3.017e-05, eta: 3:39:27, time: 0.462, data_time: 0.009, memory: 12862, decode.loss_depth: 0.1024, loss: 0.1024, grad_norm: 3.2666
2022-06-05 00:53:53,037 - depth - INFO - Epoch [3][800/1447]	lr: 3.082e-05, eta: 3:39:18, time: 0.453, data_time: 0.008, memory: 12862, decode.loss_depth: 0.1008, loss: 0.1008, grad_norm: 2.4183
2022-06-05 00:54:16,065 - depth - INFO - Epoch [3][850/1447]	lr: 3.147e-05, eta: 3:39:12, time: 0.461, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0995, loss: 0.0995, grad_norm: 3.0214
2022-06-05 00:54:34,517 - depth - INFO - Epoch [3][900/1447]	lr: 3.212e-05, eta: 3:38:28, time: 0.369, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1003, loss: 0.1003, grad_norm: 2.1915
2022-06-05 00:54:53,134 - depth - INFO - Epoch [3][950/1447]	lr: 3.279e-05, eta: 3:37:46, time: 0.372, data_time: 0.007, memory: 12862, decode.loss_depth: 0.1022, loss: 0.1022, grad_norm: 3.7702
2022-06-05 00:55:11,643 - depth - INFO - Epoch [3][1000/1447]	lr: 3.345e-05, eta: 3:37:04, time: 0.370, data_time: 0.009, memory: 12862, decode.loss_depth: 0.1006, loss: 0.1006, grad_norm: 2.8514
2022-06-05 00:55:29,913 - depth - INFO - Epoch [3][1050/1447]	lr: 3.412e-05, eta: 3:36:21, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0968, loss: 0.0968, grad_norm: 1.8984
2022-06-05 00:55:52,824 - depth - INFO - Epoch [3][1100/1447]	lr: 3.479e-05, eta: 3:36:14, time: 0.458, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0977, loss: 0.0977, grad_norm: 2.5759
2022-06-05 00:56:11,414 - depth - INFO - Epoch [3][1150/1447]	lr: 3.547e-05, eta: 3:35:34, time: 0.372, data_time: 0.009, memory: 12862, decode.loss_depth: 0.1003, loss: 0.1003, grad_norm: 2.8892
2022-06-05 00:56:34,104 - depth - INFO - Epoch [3][1200/1447]	lr: 3.615e-05, eta: 3:35:25, time: 0.454, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0946, loss: 0.0946, grad_norm: 2.0650
2022-06-05 00:56:52,625 - depth - INFO - Epoch [3][1250/1447]	lr: 3.684e-05, eta: 3:34:44, time: 0.371, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0960, loss: 0.0960, grad_norm: 2.3141
2022-06-05 00:57:15,160 - depth - INFO - Epoch [3][1300/1447]	lr: 3.753e-05, eta: 3:34:34, time: 0.450, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0974, loss: 0.0974, grad_norm: 2.4065
2022-06-05 00:57:33,640 - depth - INFO - Epoch [3][1350/1447]	lr: 3.822e-05, eta: 3:33:54, time: 0.370, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0960, loss: 0.0960, grad_norm: 2.7009
2022-06-05 00:57:56,264 - depth - INFO - Epoch [3][1400/1447]	lr: 3.891e-05, eta: 3:33:44, time: 0.452, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0938, loss: 0.0938, grad_norm: 2.1588
2022-06-05 00:58:14,082 - depth - INFO - Saving checkpoint at 3 epochs
2022-06-05 00:58:44,347 - depth - INFO - Summary:
2022-06-05 00:58:44,348 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+--------+--------+
| 0.9355 | 0.9906 | 0.998 |  0.0812 | 2.9022 | 0.0353 |  0.1171  | 10.624 | 0.3105 |
+--------+--------+-------+---------+--------+--------+----------+--------+--------+
2022-06-05 00:58:44,348 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 00:58:44,348 - depth - INFO - Epoch(val) [3][82]	a1: 0.9355, a2: 0.9906, a3: 0.9980, abs_rel: 0.08121909201145172, rmse: 2.902198076248169, log_10: 0.035312388092279434, rmse_log: 0.11708077043294907, silog: 10.6240, sq_rel: 0.3104962408542633
2022-06-05 00:59:16,209 - depth - INFO - Epoch [4][50/1447]	lr: 4.027e-05, eta: 3:32:01, time: 0.637, data_time: 0.231, memory: 12862, decode.loss_depth: 0.0959, loss: 0.0959, grad_norm: 2.9192
2022-06-05 00:59:38,952 - depth - INFO - Epoch [4][100/1447]	lr: 4.097e-05, eta: 3:31:52, time: 0.455, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0958, loss: 0.0958, grad_norm: 2.6856
2022-06-05 01:00:02,052 - depth - INFO - Epoch [4][150/1447]	lr: 4.168e-05, eta: 3:31:46, time: 0.462, data_time: 0.011, memory: 12862, decode.loss_depth: 0.0914, loss: 0.0914, grad_norm: 1.8459
2022-06-05 01:00:20,312 - depth - INFO - Epoch [4][200/1447]	lr: 4.239e-05, eta: 3:31:06, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0927, loss: 0.0927, grad_norm: 1.5762
2022-06-05 01:00:38,931 - depth - INFO - Epoch [4][250/1447]	lr: 4.310e-05, eta: 3:30:30, time: 0.372, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0935, loss: 0.0935, grad_norm: 1.8758
2022-06-05 01:00:57,317 - depth - INFO - Epoch [4][300/1447]	lr: 4.381e-05, eta: 3:29:52, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0925, loss: 0.0925, grad_norm: 2.4780
2022-06-05 01:01:15,902 - depth - INFO - Epoch [4][350/1447]	lr: 4.452e-05, eta: 3:29:16, time: 0.372, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0928, loss: 0.0928, grad_norm: 2.2819
2022-06-05 01:01:34,226 - depth - INFO - Epoch [4][400/1447]	lr: 4.524e-05, eta: 3:28:39, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0915, loss: 0.0915, grad_norm: 2.3440
2022-06-05 01:01:52,793 - depth - INFO - Epoch [4][450/1447]	lr: 4.596e-05, eta: 3:28:04, time: 0.371, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0924, loss: 0.0924, grad_norm: 1.9758
2022-06-05 01:02:11,440 - depth - INFO - Epoch [4][500/1447]	lr: 4.667e-05, eta: 3:27:29, time: 0.373, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0904, loss: 0.0904, grad_norm: 1.8755
2022-06-05 01:02:29,940 - depth - INFO - Epoch [4][550/1447]	lr: 4.739e-05, eta: 3:26:54, time: 0.370, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0905, loss: 0.0905, grad_norm: 1.6481
2022-06-05 01:02:48,439 - depth - INFO - Epoch [4][600/1447]	lr: 4.811e-05, eta: 3:26:19, time: 0.370, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0911, loss: 0.0911, grad_norm: 2.0674
2022-06-05 01:03:06,697 - depth - INFO - Epoch [4][650/1447]	lr: 4.884e-05, eta: 3:25:44, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0907, loss: 0.0907, grad_norm: 2.0681
2022-06-05 01:03:25,268 - depth - INFO - Epoch [4][700/1447]	lr: 4.956e-05, eta: 3:25:10, time: 0.371, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0917, loss: 0.0917, grad_norm: 1.6149
2022-06-05 01:03:43,567 - depth - INFO - Epoch [4][750/1447]	lr: 5.028e-05, eta: 3:24:35, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0886, loss: 0.0886, grad_norm: 1.5681
2022-06-05 01:04:01,918 - depth - INFO - Epoch [4][800/1447]	lr: 5.101e-05, eta: 3:24:01, time: 0.367, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0888, loss: 0.0888, grad_norm: 1.5061
2022-06-05 01:04:20,616 - depth - INFO - Epoch [4][850/1447]	lr: 5.173e-05, eta: 3:23:29, time: 0.374, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0871, loss: 0.0871, grad_norm: 1.9962
2022-06-05 01:04:39,140 - depth - INFO - Epoch [4][900/1447]	lr: 5.245e-05, eta: 3:22:56, time: 0.371, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0904, loss: 0.0904, grad_norm: 2.5217
2022-06-05 01:04:57,580 - depth - INFO - Epoch [4][950/1447]	lr: 5.318e-05, eta: 3:22:23, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0877, loss: 0.0877, grad_norm: 1.5536
2022-06-05 01:05:15,848 - depth - INFO - Epoch [4][1000/1447]	lr: 5.390e-05, eta: 3:21:50, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0883, loss: 0.0883, grad_norm: 1.4265
2022-06-05 01:05:34,201 - depth - INFO - Epoch [4][1050/1447]	lr: 5.462e-05, eta: 3:21:17, time: 0.367, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0849, loss: 0.0849, grad_norm: 1.4573
2022-06-05 01:05:52,528 - depth - INFO - Epoch [4][1100/1447]	lr: 5.535e-05, eta: 3:20:44, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0856, loss: 0.0856, grad_norm: 1.5614
2022-06-05 01:06:10,799 - depth - INFO - Epoch [4][1150/1447]	lr: 5.607e-05, eta: 3:20:11, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0848, loss: 0.0848, grad_norm: 1.3802
2022-06-05 01:06:29,255 - depth - INFO - Epoch [4][1200/1447]	lr: 5.679e-05, eta: 3:19:40, time: 0.369, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0859, loss: 0.0859, grad_norm: 1.8295
2022-06-05 01:06:47,773 - depth - INFO - Epoch [4][1250/1447]	lr: 5.751e-05, eta: 3:19:09, time: 0.370, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0844, loss: 0.0844, grad_norm: 1.3934
2022-06-05 01:07:06,284 - depth - INFO - Epoch [4][1300/1447]	lr: 5.823e-05, eta: 3:18:38, time: 0.370, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0861, loss: 0.0861, grad_norm: 1.8402
2022-06-05 01:07:24,660 - depth - INFO - Epoch [4][1350/1447]	lr: 5.894e-05, eta: 3:18:07, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0842, loss: 0.0842, grad_norm: 1.3832
2022-06-05 01:07:42,886 - depth - INFO - Epoch [4][1400/1447]	lr: 5.966e-05, eta: 3:17:35, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0849, loss: 0.0849, grad_norm: 1.9707
2022-06-05 01:08:00,576 - depth - INFO - Saving checkpoint at 4 epochs
2022-06-05 01:08:30,971 - depth - INFO - Summary:
2022-06-05 01:08:30,972 - depth - INFO - 
+--------+-------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2  |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+-------+--------+---------+--------+--------+----------+---------+--------+
| 0.8959 | 0.987 | 0.9977 |  0.1144 | 3.3759 | 0.0454 |  0.1394  | 10.4895 | 0.4999 |
+--------+-------+--------+---------+--------+--------+----------+---------+--------+
2022-06-05 01:08:30,972 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 01:08:30,972 - depth - INFO - Epoch(val) [4][82]	a1: 0.8959, a2: 0.9870, a3: 0.9977, abs_rel: 0.11435562372207642, rmse: 3.375920295715332, log_10: 0.0453638955950737, rmse_log: 0.13936644792556763, silog: 10.4895, sq_rel: 0.4998841881752014
2022-06-05 01:09:00,624 - depth - INFO - Epoch [5][50/1447]	lr: 6.104e-05, eta: 3:16:05, time: 0.593, data_time: 0.196, memory: 12862, decode.loss_depth: 0.0877, loss: 0.0877, grad_norm: 1.8801
2022-06-05 01:09:18,841 - depth - INFO - Epoch [5][100/1447]	lr: 6.175e-05, eta: 3:15:35, time: 0.364, data_time: 0.006, memory: 12862, decode.loss_depth: 0.0843, loss: 0.0843, grad_norm: 1.1542
2022-06-05 01:09:37,124 - depth - INFO - Epoch [5][150/1447]	lr: 6.246e-05, eta: 3:15:04, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0824, loss: 0.0824, grad_norm: 1.1282
2022-06-05 01:09:55,431 - depth - INFO - Epoch [5][200/1447]	lr: 6.316e-05, eta: 3:14:34, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0844, loss: 0.0844, grad_norm: 1.4981
2022-06-05 01:10:13,701 - depth - INFO - Epoch [5][250/1447]	lr: 6.387e-05, eta: 3:14:04, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0831, loss: 0.0831, grad_norm: 1.4461
2022-06-05 01:10:32,149 - depth - INFO - Epoch [5][300/1447]	lr: 6.457e-05, eta: 3:13:35, time: 0.369, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0833, loss: 0.0833, grad_norm: 1.7266
2022-06-05 01:10:50,480 - depth - INFO - Epoch [5][350/1447]	lr: 6.526e-05, eta: 3:13:06, time: 0.367, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0835, loss: 0.0835, grad_norm: 1.5305
2022-06-05 01:11:08,752 - depth - INFO - Epoch [5][400/1447]	lr: 6.596e-05, eta: 3:12:36, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0821, loss: 0.0821, grad_norm: 1.2241
2022-06-05 01:11:27,228 - depth - INFO - Epoch [5][450/1447]	lr: 6.665e-05, eta: 3:12:08, time: 0.369, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0887, loss: 0.0887, grad_norm: 2.1538
2022-06-05 01:11:45,422 - depth - INFO - Epoch [5][500/1447]	lr: 6.734e-05, eta: 3:11:39, time: 0.364, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0955, loss: 0.0955, grad_norm: 2.3183
2022-06-05 01:12:04,103 - depth - INFO - Epoch [5][550/1447]	lr: 6.802e-05, eta: 3:11:12, time: 0.373, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0856, loss: 0.0856, grad_norm: 1.2399
2022-06-05 01:12:22,278 - depth - INFO - Epoch [5][600/1447]	lr: 6.870e-05, eta: 3:10:42, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0841, loss: 0.0841, grad_norm: 1.4720
2022-06-05 01:12:40,581 - depth - INFO - Epoch [5][650/1447]	lr: 6.938e-05, eta: 3:10:14, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0837, loss: 0.0837, grad_norm: 1.4656
2022-06-05 01:12:58,812 - depth - INFO - Epoch [5][700/1447]	lr: 7.005e-05, eta: 3:09:45, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0827, loss: 0.0827, grad_norm: 1.0303
2022-06-05 01:13:16,962 - depth - INFO - Epoch [5][750/1447]	lr: 7.072e-05, eta: 3:09:16, time: 0.363, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0814, loss: 0.0814, grad_norm: 1.0283
2022-06-05 01:13:35,199 - depth - INFO - Epoch [5][800/1447]	lr: 7.138e-05, eta: 3:08:48, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0835, loss: 0.0835, grad_norm: 1.5173
2022-06-05 01:13:53,459 - depth - INFO - Epoch [5][850/1447]	lr: 7.204e-05, eta: 3:08:20, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0802, loss: 0.0802, grad_norm: 1.0619
2022-06-05 01:14:11,850 - depth - INFO - Epoch [5][900/1447]	lr: 7.270e-05, eta: 3:07:53, time: 0.368, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0810, loss: 0.0810, grad_norm: 1.1935
2022-06-05 01:14:30,171 - depth - INFO - Epoch [5][950/1447]	lr: 7.335e-05, eta: 3:07:25, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0807, loss: 0.0807, grad_norm: 1.4618
2022-06-05 01:14:48,512 - depth - INFO - Epoch [5][1000/1447]	lr: 7.399e-05, eta: 3:06:58, time: 0.367, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0842, loss: 0.0842, grad_norm: 1.7140
2022-06-05 01:15:06,787 - depth - INFO - Epoch [5][1050/1447]	lr: 7.463e-05, eta: 3:06:31, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0792, loss: 0.0792, grad_norm: 1.2662
2022-06-05 01:15:25,107 - depth - INFO - Epoch [5][1100/1447]	lr: 7.527e-05, eta: 3:06:03, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0803, loss: 0.0803, grad_norm: 1.4666
2022-06-05 01:15:43,476 - depth - INFO - Epoch [5][1150/1447]	lr: 7.590e-05, eta: 3:05:37, time: 0.367, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0814, loss: 0.0814, grad_norm: 1.5206
2022-06-05 01:16:01,843 - depth - INFO - Epoch [5][1200/1447]	lr: 7.653e-05, eta: 3:05:10, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0809, loss: 0.0809, grad_norm: 1.1412
2022-06-05 01:16:20,173 - depth - INFO - Epoch [5][1250/1447]	lr: 7.714e-05, eta: 3:04:43, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0806, loss: 0.0806, grad_norm: 1.3846
2022-06-05 01:16:38,486 - depth - INFO - Epoch [5][1300/1447]	lr: 7.776e-05, eta: 3:04:17, time: 0.366, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0800, loss: 0.0800, grad_norm: 1.3893
2022-06-05 01:16:56,950 - depth - INFO - Epoch [5][1350/1447]	lr: 7.837e-05, eta: 3:03:51, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0800, loss: 0.0800, grad_norm: 1.7232
2022-06-05 01:17:15,297 - depth - INFO - Epoch [5][1400/1447]	lr: 7.897e-05, eta: 3:03:24, time: 0.367, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0776, loss: 0.0776, grad_norm: 1.3251
2022-06-05 01:17:32,949 - depth - INFO - Saving checkpoint at 5 epochs
2022-06-05 01:18:02,646 - depth - INFO - Summary:
2022-06-05 01:18:02,647 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9457 | 0.9923 | 0.9984 |  0.0774 | 2.7687 | 0.0327 |  0.1091  | 9.9926 | 0.2968 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 01:18:02,647 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 01:18:02,647 - depth - INFO - Epoch(val) [5][82]	a1: 0.9457, a2: 0.9923, a3: 0.9984, abs_rel: 0.0774187222123146, rmse: 2.768720865249634, log_10: 0.03274604305624962, rmse_log: 0.10913221538066864, silog: 9.9926, sq_rel: 0.2967897057533264
2022-06-05 01:18:32,339 - depth - INFO - Epoch [6][50/1447]	lr: 8.012e-05, eta: 3:02:11, time: 0.593, data_time: 0.225, memory: 12862, decode.loss_depth: 0.0795, loss: 0.0795, grad_norm: 1.2527
2022-06-05 01:18:50,637 - depth - INFO - Epoch [6][100/1447]	lr: 8.070e-05, eta: 3:01:45, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0794, loss: 0.0794, grad_norm: 1.2269
2022-06-05 01:19:09,109 - depth - INFO - Epoch [6][150/1447]	lr: 8.128e-05, eta: 3:01:20, time: 0.369, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0757, loss: 0.0757, grad_norm: 1.0886
2022-06-05 01:19:27,593 - depth - INFO - Epoch [6][200/1447]	lr: 8.185e-05, eta: 3:00:55, time: 0.370, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0759, loss: 0.0759, grad_norm: 1.0400
2022-06-05 01:19:45,836 - depth - INFO - Epoch [6][250/1447]	lr: 8.241e-05, eta: 3:00:29, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0784, loss: 0.0784, grad_norm: 1.5412
2022-06-05 01:20:04,085 - depth - INFO - Epoch [6][300/1447]	lr: 8.297e-05, eta: 3:00:04, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0779, loss: 0.0779, grad_norm: 1.1371
2022-06-05 01:20:22,413 - depth - INFO - Epoch [6][350/1447]	lr: 8.352e-05, eta: 2:59:38, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0763, loss: 0.0763, grad_norm: 1.0119
2022-06-05 01:20:40,733 - depth - INFO - Epoch [6][400/1447]	lr: 8.406e-05, eta: 2:59:13, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0758, loss: 0.0758, grad_norm: 1.1449
2022-06-05 01:20:59,063 - depth - INFO - Epoch [6][450/1447]	lr: 8.459e-05, eta: 2:58:48, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0747, loss: 0.0747, grad_norm: 0.8704
2022-06-05 01:21:17,530 - depth - INFO - Epoch [6][500/1447]	lr: 8.512e-05, eta: 2:58:23, time: 0.369, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0758, loss: 0.0758, grad_norm: 1.0661
2022-06-05 01:21:36,173 - depth - INFO - Epoch [6][550/1447]	lr: 8.564e-05, eta: 2:57:59, time: 0.373, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0757, loss: 0.0757, grad_norm: 0.9828
2022-06-05 01:21:54,674 - depth - INFO - Epoch [6][600/1447]	lr: 8.615e-05, eta: 2:57:35, time: 0.370, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0759, loss: 0.0759, grad_norm: 0.9190
2022-06-05 01:22:13,003 - depth - INFO - Epoch [6][650/1447]	lr: 8.666e-05, eta: 2:57:10, time: 0.367, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0750, loss: 0.0750, grad_norm: 0.9309
2022-06-05 01:22:31,576 - depth - INFO - Epoch [6][700/1447]	lr: 8.716e-05, eta: 2:56:46, time: 0.371, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0747, loss: 0.0747, grad_norm: 1.3433
2022-06-05 01:22:50,122 - depth - INFO - Epoch [6][750/1447]	lr: 8.765e-05, eta: 2:56:22, time: 0.371, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0756, loss: 0.0756, grad_norm: 0.8540
2022-06-05 01:23:08,691 - depth - INFO - Epoch [6][800/1447]	lr: 8.813e-05, eta: 2:55:58, time: 0.371, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0767, loss: 0.0767, grad_norm: 1.2717
2022-06-05 01:23:27,130 - depth - INFO - Epoch [6][850/1447]	lr: 8.860e-05, eta: 2:55:34, time: 0.368, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0739, loss: 0.0739, grad_norm: 1.0610
2022-06-05 01:23:45,465 - depth - INFO - Epoch [6][900/1447]	lr: 8.906e-05, eta: 2:55:10, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0758, loss: 0.0758, grad_norm: 1.0894
2022-06-05 01:24:03,927 - depth - INFO - Epoch [6][950/1447]	lr: 8.952e-05, eta: 2:54:46, time: 0.369, data_time: 0.006, memory: 12862, decode.loss_depth: 0.0746, loss: 0.0746, grad_norm: 0.9249
2022-06-05 01:24:22,199 - depth - INFO - Epoch [6][1000/1447]	lr: 8.997e-05, eta: 2:54:21, time: 0.366, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0776, loss: 0.0776, grad_norm: 0.9191
2022-06-05 01:24:40,780 - depth - INFO - Epoch [6][1050/1447]	lr: 9.040e-05, eta: 2:53:58, time: 0.371, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0732, loss: 0.0732, grad_norm: 0.8610
2022-06-05 01:24:59,382 - depth - INFO - Epoch [6][1100/1447]	lr: 9.083e-05, eta: 2:53:34, time: 0.372, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0750, loss: 0.0750, grad_norm: 1.2328
2022-06-05 01:25:17,863 - depth - INFO - Epoch [6][1150/1447]	lr: 9.125e-05, eta: 2:53:11, time: 0.370, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0751, loss: 0.0751, grad_norm: 1.1353
2022-06-05 01:25:36,619 - depth - INFO - Epoch [6][1200/1447]	lr: 9.167e-05, eta: 2:52:48, time: 0.375, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0728, loss: 0.0728, grad_norm: 1.1001
2022-06-05 01:25:55,099 - depth - INFO - Epoch [6][1250/1447]	lr: 9.207e-05, eta: 2:52:24, time: 0.370, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0734, loss: 0.0734, grad_norm: 0.9841
2022-06-05 01:26:13,521 - depth - INFO - Epoch [6][1300/1447]	lr: 9.246e-05, eta: 2:52:01, time: 0.369, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0748, loss: 0.0748, grad_norm: 1.3676
2022-06-05 01:26:31,756 - depth - INFO - Epoch [6][1350/1447]	lr: 9.285e-05, eta: 2:51:37, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0736, loss: 0.0736, grad_norm: 1.2350
2022-06-05 01:26:50,122 - depth - INFO - Epoch [6][1400/1447]	lr: 9.322e-05, eta: 2:51:13, time: 0.367, data_time: 0.006, memory: 12862, decode.loss_depth: 0.0733, loss: 0.0733, grad_norm: 1.0397
2022-06-05 01:27:07,917 - depth - INFO - Saving checkpoint at 6 epochs
2022-06-05 01:27:37,300 - depth - INFO - Summary:
2022-06-05 01:27:37,301 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
| 0.9445 | 0.9926 | 0.9986 |  0.0737 | 2.6061 | 0.0309 |  0.1054  | 9.442 | 0.263  |
+--------+--------+--------+---------+--------+--------+----------+-------+--------+
2022-06-05 01:27:37,301 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 01:27:37,301 - depth - INFO - Epoch(val) [6][82]	a1: 0.9445, a2: 0.9926, a3: 0.9986, abs_rel: 0.07370132952928543, rmse: 2.6061439514160156, log_10: 0.030939113348722458, rmse_log: 0.1054348275065422, silog: 9.4420, sq_rel: 0.26297956705093384
2022-06-05 01:28:07,588 - depth - INFO - Epoch [7][50/1447]	lr: 9.393e-05, eta: 2:50:11, time: 0.605, data_time: 0.201, memory: 12862, decode.loss_depth: 0.0730, loss: 0.0730, grad_norm: 1.1735
2022-06-05 01:28:26,058 - depth - INFO - Epoch [7][100/1447]	lr: 9.427e-05, eta: 2:49:48, time: 0.369, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0708, loss: 0.0708, grad_norm: 0.6762
2022-06-05 01:28:44,566 - depth - INFO - Epoch [7][150/1447]	lr: 9.461e-05, eta: 2:49:25, time: 0.370, data_time: 0.011, memory: 12862, decode.loss_depth: 0.0713, loss: 0.0713, grad_norm: 0.8398
2022-06-05 01:29:03,019 - depth - INFO - Epoch [7][200/1447]	lr: 9.494e-05, eta: 2:49:02, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0726, loss: 0.0726, grad_norm: 1.3215
2022-06-05 01:29:21,315 - depth - INFO - Epoch [7][250/1447]	lr: 9.526e-05, eta: 2:48:38, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0710, loss: 0.0710, grad_norm: 0.9409
2022-06-05 01:29:39,766 - depth - INFO - Epoch [7][300/1447]	lr: 9.557e-05, eta: 2:48:16, time: 0.369, data_time: 0.011, memory: 12862, decode.loss_depth: 0.0743, loss: 0.0743, grad_norm: 1.0922
2022-06-05 01:29:58,252 - depth - INFO - Epoch [7][350/1447]	lr: 9.587e-05, eta: 2:47:53, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0729, loss: 0.0729, grad_norm: 1.1481
2022-06-05 01:30:16,490 - depth - INFO - Epoch [7][400/1447]	lr: 9.615e-05, eta: 2:47:29, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0727, loss: 0.0727, grad_norm: 1.1723
2022-06-05 01:30:34,813 - depth - INFO - Epoch [7][450/1447]	lr: 9.643e-05, eta: 2:47:06, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0698, loss: 0.0698, grad_norm: 0.6835
2022-06-05 01:30:53,104 - depth - INFO - Epoch [7][500/1447]	lr: 9.670e-05, eta: 2:46:43, time: 0.366, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0711, loss: 0.0711, grad_norm: 0.9578
2022-06-05 01:31:11,461 - depth - INFO - Epoch [7][550/1447]	lr: 9.696e-05, eta: 2:46:20, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0707, loss: 0.0707, grad_norm: 1.1411
2022-06-05 01:31:29,706 - depth - INFO - Epoch [7][600/1447]	lr: 9.721e-05, eta: 2:45:57, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0718, loss: 0.0718, grad_norm: 1.1308
2022-06-05 01:31:48,336 - depth - INFO - Epoch [7][650/1447]	lr: 9.745e-05, eta: 2:45:35, time: 0.373, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0701, loss: 0.0701, grad_norm: 1.0216
2022-06-05 01:32:06,843 - depth - INFO - Epoch [7][700/1447]	lr: 9.767e-05, eta: 2:45:12, time: 0.370, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0702, loss: 0.0702, grad_norm: 1.3820
2022-06-05 01:32:25,031 - depth - INFO - Epoch [7][750/1447]	lr: 9.789e-05, eta: 2:44:49, time: 0.364, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0699, loss: 0.0699, grad_norm: 0.9981
2022-06-05 01:32:43,354 - depth - INFO - Epoch [7][800/1447]	lr: 9.810e-05, eta: 2:44:26, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0716, loss: 0.0716, grad_norm: 1.0131
2022-06-05 01:33:02,010 - depth - INFO - Epoch [7][850/1447]	lr: 9.830e-05, eta: 2:44:04, time: 0.373, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0679, loss: 0.0679, grad_norm: 0.9911
2022-06-05 01:33:20,404 - depth - INFO - Epoch [7][900/1447]	lr: 9.848e-05, eta: 2:43:42, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0702, loss: 0.0702, grad_norm: 0.8676
2022-06-05 01:33:38,752 - depth - INFO - Epoch [7][950/1447]	lr: 9.866e-05, eta: 2:43:19, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0712, loss: 0.0712, grad_norm: 1.1103
2022-06-05 01:33:57,166 - depth - INFO - Epoch [7][1000/1447]	lr: 9.882e-05, eta: 2:42:57, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0695, loss: 0.0695, grad_norm: 1.1850
2022-06-05 01:34:15,728 - depth - INFO - Epoch [7][1050/1447]	lr: 9.898e-05, eta: 2:42:35, time: 0.371, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0691, loss: 0.0691, grad_norm: 1.2087
2022-06-05 01:34:34,005 - depth - INFO - Epoch [7][1100/1447]	lr: 9.912e-05, eta: 2:42:12, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0678, loss: 0.0678, grad_norm: 0.9557
2022-06-05 01:34:52,380 - depth - INFO - Epoch [7][1150/1447]	lr: 9.925e-05, eta: 2:41:50, time: 0.368, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0680, loss: 0.0680, grad_norm: 1.0134
2022-06-05 01:35:10,979 - depth - INFO - Epoch [7][1200/1447]	lr: 9.937e-05, eta: 2:41:28, time: 0.372, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0664, loss: 0.0664, grad_norm: 0.7122
2022-06-05 01:35:29,222 - depth - INFO - Epoch [7][1250/1447]	lr: 9.948e-05, eta: 2:41:05, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0673, loss: 0.0673, grad_norm: 0.9895
2022-06-05 01:35:47,547 - depth - INFO - Epoch [7][1300/1447]	lr: 9.958e-05, eta: 2:40:43, time: 0.366, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0692, loss: 0.0692, grad_norm: 1.1229
2022-06-05 01:36:05,940 - depth - INFO - Epoch [7][1350/1447]	lr: 9.967e-05, eta: 2:40:21, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0672, loss: 0.0672, grad_norm: 0.9744
2022-06-05 01:36:24,441 - depth - INFO - Epoch [7][1400/1447]	lr: 9.975e-05, eta: 2:39:59, time: 0.370, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0670, loss: 0.0670, grad_norm: 0.8745
2022-06-05 01:36:42,459 - depth - INFO - Saving checkpoint at 7 epochs
2022-06-05 01:37:12,728 - depth - INFO - Summary:
2022-06-05 01:37:12,729 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9382 | 0.9922 | 0.9986 |  0.0784 | 2.6722 | 0.0324 |  0.1092  | 9.6113 | 0.2821 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 01:37:12,729 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 01:37:12,729 - depth - INFO - Epoch(val) [7][82]	a1: 0.9382, a2: 0.9922, a3: 0.9986, abs_rel: 0.07838302105665207, rmse: 2.6721925735473633, log_10: 0.032406922429800034, rmse_log: 0.10923157632350922, silog: 9.6113, sq_rel: 0.2820824980735779
2022-06-05 01:37:42,345 - depth - INFO - Epoch [8][50/1447]	lr: 9.987e-05, eta: 2:39:01, time: 0.592, data_time: 0.194, memory: 12862, decode.loss_depth: 0.0692, loss: 0.0692, grad_norm: 0.7097
2022-06-05 01:38:00,657 - depth - INFO - Epoch [8][100/1447]	lr: 9.992e-05, eta: 2:38:39, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0674, loss: 0.0674, grad_norm: 0.8393
2022-06-05 01:38:19,078 - depth - INFO - Epoch [8][150/1447]	lr: 9.996e-05, eta: 2:38:18, time: 0.368, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0680, loss: 0.0680, grad_norm: 0.8665
2022-06-05 01:38:37,508 - depth - INFO - Epoch [8][200/1447]	lr: 9.998e-05, eta: 2:37:56, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0664, loss: 0.0664, grad_norm: 0.9409
2022-06-05 01:38:56,006 - depth - INFO - Epoch [8][250/1447]	lr: 1.000e-04, eta: 2:37:34, time: 0.370, data_time: 0.006, memory: 12862, decode.loss_depth: 0.0656, loss: 0.0656, grad_norm: 1.1867
2022-06-05 01:39:14,729 - depth - INFO - Epoch [8][300/1447]	lr: 1.000e-04, eta: 2:37:13, time: 0.374, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0670, loss: 0.0670, grad_norm: 0.7028
2022-06-05 01:39:33,146 - depth - INFO - Epoch [8][350/1447]	lr: 1.000e-04, eta: 2:36:51, time: 0.368, data_time: 0.006, memory: 12862, decode.loss_depth: 0.0667, loss: 0.0667, grad_norm: 1.3450
2022-06-05 01:39:51,576 - depth - INFO - Epoch [8][400/1447]	lr: 9.999e-05, eta: 2:36:30, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0653, loss: 0.0653, grad_norm: 0.8242
2022-06-05 01:40:10,145 - depth - INFO - Epoch [8][450/1447]	lr: 9.999e-05, eta: 2:36:08, time: 0.372, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0662, loss: 0.0662, grad_norm: 1.2392
2022-06-05 01:40:28,573 - depth - INFO - Epoch [8][500/1447]	lr: 9.998e-05, eta: 2:35:47, time: 0.368, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0657, loss: 0.0657, grad_norm: 1.0460
2022-06-05 01:40:47,171 - depth - INFO - Epoch [8][550/1447]	lr: 9.997e-05, eta: 2:35:26, time: 0.372, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0657, loss: 0.0657, grad_norm: 0.9411
2022-06-05 01:41:05,622 - depth - INFO - Epoch [8][600/1447]	lr: 9.996e-05, eta: 2:35:04, time: 0.369, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0673, loss: 0.0673, grad_norm: 1.6586
2022-06-05 01:41:23,782 - depth - INFO - Epoch [8][650/1447]	lr: 9.995e-05, eta: 2:34:42, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0652, loss: 0.0652, grad_norm: 1.0145
2022-06-05 01:41:42,090 - depth - INFO - Epoch [8][700/1447]	lr: 9.993e-05, eta: 2:34:20, time: 0.366, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0658, loss: 0.0658, grad_norm: 1.0905
2022-06-05 01:42:01,397 - depth - INFO - Epoch [8][750/1447]	lr: 9.991e-05, eta: 2:34:01, time: 0.386, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0658, loss: 0.0658, grad_norm: 1.1063
2022-06-05 01:42:19,758 - depth - INFO - Epoch [8][800/1447]	lr: 9.989e-05, eta: 2:33:39, time: 0.367, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0649, loss: 0.0649, grad_norm: 0.6996
2022-06-05 01:42:38,081 - depth - INFO - Epoch [8][850/1447]	lr: 9.987e-05, eta: 2:33:18, time: 0.367, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0620, loss: 0.0620, grad_norm: 0.7374
2022-06-05 01:42:56,637 - depth - INFO - Epoch [8][900/1447]	lr: 9.984e-05, eta: 2:32:56, time: 0.371, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0657, loss: 0.0657, grad_norm: 0.8446
2022-06-05 01:43:14,958 - depth - INFO - Epoch [8][950/1447]	lr: 9.982e-05, eta: 2:32:35, time: 0.367, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0659, loss: 0.0659, grad_norm: 1.3823
2022-06-05 01:43:33,402 - depth - INFO - Epoch [8][1000/1447]	lr: 9.979e-05, eta: 2:32:14, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0652, loss: 0.0652, grad_norm: 1.1031
2022-06-05 01:43:51,779 - depth - INFO - Epoch [8][1050/1447]	lr: 9.976e-05, eta: 2:31:52, time: 0.367, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0649, loss: 0.0649, grad_norm: 0.7870
2022-06-05 01:44:10,219 - depth - INFO - Epoch [8][1100/1447]	lr: 9.973e-05, eta: 2:31:31, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0637, loss: 0.0637, grad_norm: 0.7062
2022-06-05 01:44:28,656 - depth - INFO - Epoch [8][1150/1447]	lr: 9.969e-05, eta: 2:31:10, time: 0.369, data_time: 0.006, memory: 12862, decode.loss_depth: 0.0631, loss: 0.0631, grad_norm: 0.9613
2022-06-05 01:44:47,031 - depth - INFO - Epoch [8][1200/1447]	lr: 9.965e-05, eta: 2:30:48, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0621, loss: 0.0621, grad_norm: 0.9287
2022-06-05 01:45:05,352 - depth - INFO - Epoch [8][1250/1447]	lr: 9.962e-05, eta: 2:30:27, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0631, loss: 0.0631, grad_norm: 0.8067
2022-06-05 01:45:23,788 - depth - INFO - Epoch [8][1300/1447]	lr: 9.957e-05, eta: 2:30:06, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0635, loss: 0.0635, grad_norm: 1.0195
2022-06-05 01:45:42,293 - depth - INFO - Epoch [8][1350/1447]	lr: 9.953e-05, eta: 2:29:45, time: 0.370, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0638, loss: 0.0638, grad_norm: 1.0134
2022-06-05 01:46:00,841 - depth - INFO - Epoch [8][1400/1447]	lr: 9.949e-05, eta: 2:29:24, time: 0.371, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0628, loss: 0.0628, grad_norm: 1.1294
2022-06-05 01:46:18,709 - depth - INFO - Saving checkpoint at 8 epochs
2022-06-05 01:46:47,900 - depth - INFO - Summary:
2022-06-05 01:46:47,900 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9416 | 0.9925 | 0.9986 |  0.078  | 2.6018 | 0.0322 |  0.1078  | 9.3085 | 0.2729 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 01:46:47,901 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 01:46:47,901 - depth - INFO - Epoch(val) [8][82]	a1: 0.9416, a2: 0.9925, a3: 0.9986, abs_rel: 0.07801572233438492, rmse: 2.6018028259277344, log_10: 0.0321669764816761, rmse_log: 0.10776755213737488, silog: 9.3085, sq_rel: 0.2728584110736847
2022-06-05 01:47:17,949 - depth - INFO - Epoch [9][50/1447]	lr: 9.939e-05, eta: 2:28:32, time: 0.601, data_time: 0.200, memory: 12862, decode.loss_depth: 0.0616, loss: 0.0616, grad_norm: 0.7252
2022-06-05 01:47:36,300 - depth - INFO - Epoch [9][100/1447]	lr: 9.934e-05, eta: 2:28:10, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0613, loss: 0.0613, grad_norm: 0.7849
2022-06-05 01:47:54,753 - depth - INFO - Epoch [9][150/1447]	lr: 9.929e-05, eta: 2:27:50, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0615, loss: 0.0615, grad_norm: 1.2737
2022-06-05 01:48:13,359 - depth - INFO - Epoch [9][200/1447]	lr: 9.923e-05, eta: 2:27:29, time: 0.372, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0634, loss: 0.0634, grad_norm: 1.2660
2022-06-05 01:48:32,229 - depth - INFO - Epoch [9][250/1447]	lr: 9.918e-05, eta: 2:27:09, time: 0.378, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0627, loss: 0.0627, grad_norm: 0.9637
2022-06-05 01:48:50,769 - depth - INFO - Epoch [9][300/1447]	lr: 9.912e-05, eta: 2:26:48, time: 0.371, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0634, loss: 0.0634, grad_norm: 0.9928
2022-06-05 01:49:09,359 - depth - INFO - Epoch [9][350/1447]	lr: 9.905e-05, eta: 2:26:28, time: 0.372, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0626, loss: 0.0626, grad_norm: 1.3195
2022-06-05 01:49:27,734 - depth - INFO - Epoch [9][400/1447]	lr: 9.899e-05, eta: 2:26:07, time: 0.368, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0615, loss: 0.0615, grad_norm: 1.4135
2022-06-05 01:49:46,156 - depth - INFO - Epoch [9][450/1447]	lr: 9.893e-05, eta: 2:25:46, time: 0.369, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0614, loss: 0.0614, grad_norm: 1.0805
2022-06-05 01:50:04,625 - depth - INFO - Epoch [9][500/1447]	lr: 9.886e-05, eta: 2:25:25, time: 0.369, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0606, loss: 0.0606, grad_norm: 0.7027
2022-06-05 01:50:23,159 - depth - INFO - Epoch [9][550/1447]	lr: 9.879e-05, eta: 2:25:04, time: 0.370, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0619, loss: 0.0619, grad_norm: 0.7597
2022-06-05 01:50:41,563 - depth - INFO - Epoch [9][600/1447]	lr: 9.872e-05, eta: 2:24:44, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0628, loss: 0.0628, grad_norm: 0.9533
2022-06-05 01:50:59,942 - depth - INFO - Epoch [9][650/1447]	lr: 9.864e-05, eta: 2:24:23, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0617, loss: 0.0617, grad_norm: 0.5941
2022-06-05 01:51:18,431 - depth - INFO - Epoch [9][700/1447]	lr: 9.857e-05, eta: 2:24:02, time: 0.370, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0612, loss: 0.0612, grad_norm: 0.9791
2022-06-05 01:51:36,848 - depth - INFO - Epoch [9][750/1447]	lr: 9.849e-05, eta: 2:23:41, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0614, loss: 0.0614, grad_norm: 0.8189
2022-06-05 01:51:55,450 - depth - INFO - Epoch [9][800/1447]	lr: 9.841e-05, eta: 2:23:21, time: 0.372, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0622, loss: 0.0622, grad_norm: 1.3056
2022-06-05 01:52:13,896 - depth - INFO - Epoch [9][850/1447]	lr: 9.833e-05, eta: 2:23:00, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0599, loss: 0.0599, grad_norm: 0.8956
2022-06-05 01:52:32,479 - depth - INFO - Epoch [9][900/1447]	lr: 9.824e-05, eta: 2:22:40, time: 0.372, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0605, loss: 0.0605, grad_norm: 0.8062
2022-06-05 01:52:50,825 - depth - INFO - Epoch [9][950/1447]	lr: 9.816e-05, eta: 2:22:19, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0591, loss: 0.0591, grad_norm: 0.9201
2022-06-05 01:53:09,643 - depth - INFO - Epoch [9][1000/1447]	lr: 9.807e-05, eta: 2:21:59, time: 0.376, data_time: 0.006, memory: 12862, decode.loss_depth: 0.0607, loss: 0.0607, grad_norm: 0.9424
2022-06-05 01:53:28,093 - depth - INFO - Epoch [9][1050/1447]	lr: 9.798e-05, eta: 2:21:38, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0601, loss: 0.0601, grad_norm: 0.7572
2022-06-05 01:53:46,626 - depth - INFO - Epoch [9][1100/1447]	lr: 9.789e-05, eta: 2:21:18, time: 0.371, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0595, loss: 0.0595, grad_norm: 1.0800
2022-06-05 01:54:05,012 - depth - INFO - Epoch [9][1150/1447]	lr: 9.779e-05, eta: 2:20:57, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0593, loss: 0.0593, grad_norm: 0.9032
2022-06-05 01:54:23,346 - depth - INFO - Epoch [9][1200/1447]	lr: 9.770e-05, eta: 2:20:37, time: 0.367, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0593, loss: 0.0593, grad_norm: 1.0438
2022-06-05 01:54:41,783 - depth - INFO - Epoch [9][1250/1447]	lr: 9.760e-05, eta: 2:20:16, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0596, loss: 0.0596, grad_norm: 1.2699
2022-06-05 01:55:00,036 - depth - INFO - Epoch [9][1300/1447]	lr: 9.750e-05, eta: 2:19:55, time: 0.365, data_time: 0.006, memory: 12862, decode.loss_depth: 0.0585, loss: 0.0585, grad_norm: 0.6968
2022-06-05 01:55:18,465 - depth - INFO - Epoch [9][1350/1447]	lr: 9.740e-05, eta: 2:19:35, time: 0.368, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0595, loss: 0.0595, grad_norm: 0.9688
2022-06-05 01:55:36,885 - depth - INFO - Epoch [9][1400/1447]	lr: 9.729e-05, eta: 2:19:14, time: 0.369, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0591, loss: 0.0591, grad_norm: 1.1089
2022-06-05 01:55:54,745 - depth - INFO - Saving checkpoint at 9 epochs
2022-06-05 01:56:24,795 - depth - INFO - Summary:
2022-06-05 01:56:24,796 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9475 | 0.9934 | 0.9989 |  0.073  | 2.5939 | 0.0306 |  0.1035  | 9.3844 | 0.2595 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 01:56:24,796 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 01:56:24,797 - depth - INFO - Epoch(val) [9][82]	a1: 0.9475, a2: 0.9934, a3: 0.9989, abs_rel: 0.07295563817024231, rmse: 2.593916893005371, log_10: 0.03062593750655651, rmse_log: 0.10350509732961655, silog: 9.3844, sq_rel: 0.25945407152175903
2022-06-05 01:56:54,193 - depth - INFO - Epoch [10][50/1447]	lr: 9.709e-05, eta: 2:18:24, time: 0.588, data_time: 0.200, memory: 12862, decode.loss_depth: 0.0594, loss: 0.0594, grad_norm: 0.5839
2022-06-05 01:57:12,394 - depth - INFO - Epoch [10][100/1447]	lr: 9.698e-05, eta: 2:18:03, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0583, loss: 0.0583, grad_norm: 0.7219
2022-06-05 01:57:30,653 - depth - INFO - Epoch [10][150/1447]	lr: 9.687e-05, eta: 2:17:42, time: 0.365, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0576, loss: 0.0576, grad_norm: 0.8321
2022-06-05 01:57:48,848 - depth - INFO - Epoch [10][200/1447]	lr: 9.675e-05, eta: 2:17:22, time: 0.364, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0586, loss: 0.0586, grad_norm: 0.8511
2022-06-05 01:58:07,224 - depth - INFO - Epoch [10][250/1447]	lr: 9.664e-05, eta: 2:17:01, time: 0.368, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0579, loss: 0.0579, grad_norm: 0.7642
2022-06-05 01:58:25,727 - depth - INFO - Epoch [10][300/1447]	lr: 9.652e-05, eta: 2:16:41, time: 0.370, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0571, loss: 0.0571, grad_norm: 0.7895
2022-06-05 01:58:44,347 - depth - INFO - Epoch [10][350/1447]	lr: 9.640e-05, eta: 2:16:21, time: 0.372, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0568, loss: 0.0568, grad_norm: 0.7441
2022-06-05 01:59:02,724 - depth - INFO - Epoch [10][400/1447]	lr: 9.628e-05, eta: 2:16:01, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0570, loss: 0.0570, grad_norm: 0.6929
2022-06-05 01:59:21,021 - depth - INFO - Epoch [10][450/1447]	lr: 9.616e-05, eta: 2:15:40, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0571, loss: 0.0571, grad_norm: 0.9580
2022-06-05 01:59:39,247 - depth - INFO - Epoch [10][500/1447]	lr: 9.603e-05, eta: 2:15:20, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0572, loss: 0.0572, grad_norm: 1.0435
2022-06-05 01:59:57,551 - depth - INFO - Epoch [10][550/1447]	lr: 9.590e-05, eta: 2:14:59, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0566, loss: 0.0566, grad_norm: 0.7432
2022-06-05 02:00:15,873 - depth - INFO - Epoch [10][600/1447]	lr: 9.577e-05, eta: 2:14:39, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0582, loss: 0.0582, grad_norm: 1.0476
2022-06-05 02:00:34,174 - depth - INFO - Epoch [10][650/1447]	lr: 9.564e-05, eta: 2:14:18, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0576, loss: 0.0576, grad_norm: 0.9280
2022-06-05 02:00:52,440 - depth - INFO - Epoch [10][700/1447]	lr: 9.551e-05, eta: 2:13:58, time: 0.365, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0558, loss: 0.0558, grad_norm: 0.7328
2022-06-05 02:01:10,786 - depth - INFO - Epoch [10][750/1447]	lr: 9.538e-05, eta: 2:13:37, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0577, loss: 0.0577, grad_norm: 0.8290
2022-06-05 02:01:29,056 - depth - INFO - Epoch [10][800/1447]	lr: 9.524e-05, eta: 2:13:17, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0565, loss: 0.0565, grad_norm: 0.8902
2022-06-05 02:01:47,150 - depth - INFO - Epoch [10][850/1447]	lr: 9.510e-05, eta: 2:12:56, time: 0.362, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0560, loss: 0.0560, grad_norm: 1.2844
2022-06-05 02:02:05,365 - depth - INFO - Epoch [10][900/1447]	lr: 9.496e-05, eta: 2:12:36, time: 0.364, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0569, loss: 0.0569, grad_norm: 0.8716
2022-06-05 02:02:23,654 - depth - INFO - Epoch [10][950/1447]	lr: 9.482e-05, eta: 2:12:15, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0558, loss: 0.0558, grad_norm: 0.7806
2022-06-05 02:02:42,052 - depth - INFO - Epoch [10][1000/1447]	lr: 9.467e-05, eta: 2:11:55, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0571, loss: 0.0571, grad_norm: 0.9672
2022-06-05 02:03:00,070 - depth - INFO - Epoch [10][1050/1447]	lr: 9.453e-05, eta: 2:11:35, time: 0.360, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0563, loss: 0.0563, grad_norm: 0.7747
2022-06-05 02:03:18,479 - depth - INFO - Epoch [10][1100/1447]	lr: 9.438e-05, eta: 2:11:14, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0559, loss: 0.0559, grad_norm: 0.8600
2022-06-05 02:03:36,757 - depth - INFO - Epoch [10][1150/1447]	lr: 9.423e-05, eta: 2:10:54, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0558, loss: 0.0558, grad_norm: 0.7855
2022-06-05 02:03:55,489 - depth - INFO - Epoch [10][1200/1447]	lr: 9.408e-05, eta: 2:10:34, time: 0.375, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0558, loss: 0.0558, grad_norm: 0.7662
2022-06-05 02:04:13,814 - depth - INFO - Epoch [10][1250/1447]	lr: 9.393e-05, eta: 2:10:14, time: 0.367, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0561, loss: 0.0561, grad_norm: 0.9678
2022-06-05 02:04:32,052 - depth - INFO - Epoch [10][1300/1447]	lr: 9.377e-05, eta: 2:09:54, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0558, loss: 0.0558, grad_norm: 0.9575
2022-06-05 02:04:50,375 - depth - INFO - Epoch [10][1350/1447]	lr: 9.361e-05, eta: 2:09:34, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0556, loss: 0.0556, grad_norm: 0.6809
2022-06-05 02:05:08,634 - depth - INFO - Epoch [10][1400/1447]	lr: 9.345e-05, eta: 2:09:13, time: 0.365, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0560, loss: 0.0560, grad_norm: 1.1298
2022-06-05 02:05:26,305 - depth - INFO - Saving checkpoint at 10 epochs
2022-06-05 02:05:55,468 - depth - INFO - Summary:
2022-06-05 02:05:55,469 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9482 | 0.9933 | 0.9989 |  0.071  | 2.6163 |  0.03  |  0.1024  | 9.3049 | 0.2484 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 02:05:55,469 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 02:05:55,470 - depth - INFO - Epoch(val) [10][82]	a1: 0.9482, a2: 0.9933, a3: 0.9989, abs_rel: 0.07099071145057678, rmse: 2.6163299083709717, log_10: 0.03000398352742195, rmse_log: 0.10242574661970139, silog: 9.3049, sq_rel: 0.2484239935874939
2022-06-05 02:06:24,303 - depth - INFO - Epoch [11][50/1447]	lr: 9.314e-05, eta: 2:08:25, time: 0.576, data_time: 0.192, memory: 12862, decode.loss_depth: 0.0557, loss: 0.0557, grad_norm: 0.9633
2022-06-05 02:06:42,597 - depth - INFO - Epoch [11][100/1447]	lr: 9.298e-05, eta: 2:08:05, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0548, loss: 0.0548, grad_norm: 0.9713
2022-06-05 02:07:00,805 - depth - INFO - Epoch [11][150/1447]	lr: 9.281e-05, eta: 2:07:45, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0546, loss: 0.0546, grad_norm: 0.7406
2022-06-05 02:07:18,979 - depth - INFO - Epoch [11][200/1447]	lr: 9.264e-05, eta: 2:07:24, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0554, loss: 0.0554, grad_norm: 0.8085
2022-06-05 02:07:37,104 - depth - INFO - Epoch [11][250/1447]	lr: 9.247e-05, eta: 2:07:04, time: 0.362, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0554, loss: 0.0554, grad_norm: 0.7570
2022-06-05 02:07:55,351 - depth - INFO - Epoch [11][300/1447]	lr: 9.230e-05, eta: 2:06:44, time: 0.365, data_time: 0.011, memory: 12862, decode.loss_depth: 0.0555, loss: 0.0555, grad_norm: 1.1586
2022-06-05 02:08:13,804 - depth - INFO - Epoch [11][350/1447]	lr: 9.213e-05, eta: 2:06:24, time: 0.369, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0539, loss: 0.0539, grad_norm: 0.8680
2022-06-05 02:08:31,969 - depth - INFO - Epoch [11][400/1447]	lr: 9.195e-05, eta: 2:06:04, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0539, loss: 0.0539, grad_norm: 0.9680
2022-06-05 02:08:50,550 - depth - INFO - Epoch [11][450/1447]	lr: 9.178e-05, eta: 2:05:44, time: 0.372, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0536, loss: 0.0536, grad_norm: 0.9645
2022-06-05 02:09:08,928 - depth - INFO - Epoch [11][500/1447]	lr: 9.160e-05, eta: 2:05:24, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0536, loss: 0.0536, grad_norm: 0.7905
2022-06-05 02:09:27,258 - depth - INFO - Epoch [11][550/1447]	lr: 9.142e-05, eta: 2:05:04, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0535, loss: 0.0535, grad_norm: 0.5973
2022-06-05 02:09:45,527 - depth - INFO - Epoch [11][600/1447]	lr: 9.124e-05, eta: 2:04:44, time: 0.365, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0539, loss: 0.0539, grad_norm: 0.9103
2022-06-05 02:10:03,806 - depth - INFO - Epoch [11][650/1447]	lr: 9.105e-05, eta: 2:04:24, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0548, loss: 0.0548, grad_norm: 0.6481
2022-06-05 02:10:22,347 - depth - INFO - Epoch [11][700/1447]	lr: 9.087e-05, eta: 2:04:04, time: 0.371, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0543, loss: 0.0543, grad_norm: 0.8188
2022-06-05 02:10:40,668 - depth - INFO - Epoch [11][750/1447]	lr: 9.068e-05, eta: 2:03:44, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0533, loss: 0.0533, grad_norm: 0.7234
2022-06-05 02:10:58,900 - depth - INFO - Epoch [11][800/1447]	lr: 9.049e-05, eta: 2:03:24, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0531, loss: 0.0531, grad_norm: 0.8312
2022-06-05 02:11:17,038 - depth - INFO - Epoch [11][850/1447]	lr: 9.030e-05, eta: 2:03:04, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0530, loss: 0.0530, grad_norm: 0.7830
2022-06-05 02:11:35,304 - depth - INFO - Epoch [11][900/1447]	lr: 9.011e-05, eta: 2:02:44, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0537, loss: 0.0537, grad_norm: 1.0142
2022-06-05 02:11:53,741 - depth - INFO - Epoch [11][950/1447]	lr: 8.992e-05, eta: 2:02:25, time: 0.369, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0539, loss: 0.0539, grad_norm: 0.9182
2022-06-05 02:12:11,954 - depth - INFO - Epoch [11][1000/1447]	lr: 8.972e-05, eta: 2:02:05, time: 0.364, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0544, loss: 0.0544, grad_norm: 0.9207
2022-06-05 02:12:30,224 - depth - INFO - Epoch [11][1050/1447]	lr: 8.953e-05, eta: 2:01:45, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0532, loss: 0.0532, grad_norm: 1.0752
2022-06-05 02:12:48,437 - depth - INFO - Epoch [11][1100/1447]	lr: 8.933e-05, eta: 2:01:25, time: 0.364, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0531, loss: 0.0531, grad_norm: 0.8362
2022-06-05 02:13:06,751 - depth - INFO - Epoch [11][1150/1447]	lr: 8.913e-05, eta: 2:01:05, time: 0.366, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0541, loss: 0.0541, grad_norm: 0.9214
2022-06-05 02:13:25,078 - depth - INFO - Epoch [11][1200/1447]	lr: 8.892e-05, eta: 2:00:45, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0531, loss: 0.0531, grad_norm: 0.8479
2022-06-05 02:13:43,301 - depth - INFO - Epoch [11][1250/1447]	lr: 8.872e-05, eta: 2:00:25, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0523, loss: 0.0523, grad_norm: 0.8697
2022-06-05 02:14:01,482 - depth - INFO - Epoch [11][1300/1447]	lr: 8.852e-05, eta: 2:00:05, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0528, loss: 0.0528, grad_norm: 1.0483
2022-06-05 02:14:19,818 - depth - INFO - Epoch [11][1350/1447]	lr: 8.831e-05, eta: 1:59:45, time: 0.366, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0530, loss: 0.0530, grad_norm: 1.2299
2022-06-05 02:14:37,945 - depth - INFO - Epoch [11][1400/1447]	lr: 8.810e-05, eta: 1:59:25, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0524, loss: 0.0524, grad_norm: 0.8282
2022-06-05 02:14:55,415 - depth - INFO - Saving checkpoint at 11 epochs
2022-06-05 02:15:26,141 - depth - INFO - Summary:
2022-06-05 02:15:26,142 - depth - INFO - 
+--------+--------+--------+---------+------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel | rmse | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+------+--------+----------+--------+--------+
| 0.9495 | 0.9933 | 0.9988 |   0.07  | 2.59 | 0.0297 |  0.1018  | 9.1647 | 0.249  |
+--------+--------+--------+---------+------+--------+----------+--------+--------+
2022-06-05 02:15:26,142 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 02:15:26,142 - depth - INFO - Epoch(val) [11][82]	a1: 0.9495, a2: 0.9933, a3: 0.9988, abs_rel: 0.0700167790055275, rmse: 2.58998703956604, log_10: 0.029654093086719513, rmse_log: 0.10175147652626038, silog: 9.1647, sq_rel: 0.24895219504833221
2022-06-05 02:15:56,869 - depth - INFO - Epoch [12][50/1447]	lr: 8.769e-05, eta: 1:58:41, time: 0.614, data_time: 0.247, memory: 12862, decode.loss_depth: 0.0519, loss: 0.0519, grad_norm: 0.6317
2022-06-05 02:16:15,176 - depth - INFO - Epoch [12][100/1447]	lr: 8.748e-05, eta: 1:58:21, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0523, loss: 0.0523, grad_norm: 0.8125
2022-06-05 02:16:33,701 - depth - INFO - Epoch [12][150/1447]	lr: 8.726e-05, eta: 1:58:02, time: 0.371, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0520, loss: 0.0520, grad_norm: 0.8322
2022-06-05 02:16:52,102 - depth - INFO - Epoch [12][200/1447]	lr: 8.705e-05, eta: 1:57:42, time: 0.368, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0521, loss: 0.0521, grad_norm: 0.6344
2022-06-05 02:17:10,542 - depth - INFO - Epoch [12][250/1447]	lr: 8.683e-05, eta: 1:57:22, time: 0.369, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0523, loss: 0.0523, grad_norm: 0.7726
2022-06-05 02:17:28,841 - depth - INFO - Epoch [12][300/1447]	lr: 8.661e-05, eta: 1:57:03, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0525, loss: 0.0525, grad_norm: 0.8881
2022-06-05 02:17:47,192 - depth - INFO - Epoch [12][350/1447]	lr: 8.639e-05, eta: 1:56:43, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0520, loss: 0.0520, grad_norm: 0.7716
2022-06-05 02:18:05,659 - depth - INFO - Epoch [12][400/1447]	lr: 8.617e-05, eta: 1:56:24, time: 0.369, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0523, loss: 0.0523, grad_norm: 1.2955
2022-06-05 02:18:23,942 - depth - INFO - Epoch [12][450/1447]	lr: 8.594e-05, eta: 1:56:04, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0519, loss: 0.0519, grad_norm: 0.9885
2022-06-05 02:18:42,409 - depth - INFO - Epoch [12][500/1447]	lr: 8.572e-05, eta: 1:55:44, time: 0.369, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0520, loss: 0.0520, grad_norm: 0.5968
2022-06-05 02:19:00,567 - depth - INFO - Epoch [12][550/1447]	lr: 8.549e-05, eta: 1:55:24, time: 0.363, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0507, loss: 0.0507, grad_norm: 0.8393
2022-06-05 02:19:19,110 - depth - INFO - Epoch [12][600/1447]	lr: 8.526e-05, eta: 1:55:05, time: 0.371, data_time: 0.011, memory: 12862, decode.loss_depth: 0.0508, loss: 0.0508, grad_norm: 1.1995
2022-06-05 02:19:37,198 - depth - INFO - Epoch [12][650/1447]	lr: 8.503e-05, eta: 1:54:45, time: 0.362, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0515, loss: 0.0515, grad_norm: 0.7837
2022-06-05 02:19:55,769 - depth - INFO - Epoch [12][700/1447]	lr: 8.480e-05, eta: 1:54:26, time: 0.371, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0508, loss: 0.0508, grad_norm: 0.8480
2022-06-05 02:20:13,956 - depth - INFO - Epoch [12][750/1447]	lr: 8.457e-05, eta: 1:54:06, time: 0.364, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0516, loss: 0.0516, grad_norm: 0.7927
2022-06-05 02:20:32,373 - depth - INFO - Epoch [12][800/1447]	lr: 8.434e-05, eta: 1:53:46, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0510, loss: 0.0510, grad_norm: 0.8888
2022-06-05 02:20:51,184 - depth - INFO - Epoch [12][850/1447]	lr: 8.410e-05, eta: 1:53:27, time: 0.376, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0499, loss: 0.0499, grad_norm: 0.6619
2022-06-05 02:21:09,397 - depth - INFO - Epoch [12][900/1447]	lr: 8.386e-05, eta: 1:53:08, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0498, loss: 0.0498, grad_norm: 0.8050
2022-06-05 02:21:27,739 - depth - INFO - Epoch [12][950/1447]	lr: 8.363e-05, eta: 1:52:48, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0511, loss: 0.0511, grad_norm: 0.6732
2022-06-05 02:21:46,076 - depth - INFO - Epoch [12][1000/1447]	lr: 8.339e-05, eta: 1:52:28, time: 0.367, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0508, loss: 0.0508, grad_norm: 0.7005
2022-06-05 02:22:04,213 - depth - INFO - Epoch [12][1050/1447]	lr: 8.314e-05, eta: 1:52:09, time: 0.363, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0498, loss: 0.0498, grad_norm: 0.6822
2022-06-05 02:22:22,422 - depth - INFO - Epoch [12][1100/1447]	lr: 8.290e-05, eta: 1:51:49, time: 0.364, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0503, loss: 0.0503, grad_norm: 0.9534
2022-06-05 02:22:40,612 - depth - INFO - Epoch [12][1150/1447]	lr: 8.266e-05, eta: 1:51:29, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0502, loss: 0.0502, grad_norm: 0.7932
2022-06-05 02:22:59,105 - depth - INFO - Epoch [12][1200/1447]	lr: 8.241e-05, eta: 1:51:10, time: 0.370, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0495, loss: 0.0495, grad_norm: 0.7727
2022-06-05 02:23:17,343 - depth - INFO - Epoch [12][1250/1447]	lr: 8.217e-05, eta: 1:50:50, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0501, loss: 0.0501, grad_norm: 0.8707
2022-06-05 02:23:35,685 - depth - INFO - Epoch [12][1300/1447]	lr: 8.192e-05, eta: 1:50:31, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0495, loss: 0.0495, grad_norm: 0.7625
2022-06-05 02:23:53,962 - depth - INFO - Epoch [12][1350/1447]	lr: 8.167e-05, eta: 1:50:11, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0500, loss: 0.0500, grad_norm: 0.9110
2022-06-05 02:24:12,523 - depth - INFO - Epoch [12][1400/1447]	lr: 8.142e-05, eta: 1:49:52, time: 0.372, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0489, loss: 0.0489, grad_norm: 0.5568
2022-06-05 02:24:30,298 - depth - INFO - Saving checkpoint at 12 epochs
2022-06-05 02:25:00,452 - depth - INFO - Summary:
2022-06-05 02:25:00,452 - depth - INFO - 
+-------+-------+--------+---------+--------+--------+----------+--------+--------+
|   a1  |   a2  |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+-------+-------+--------+---------+--------+--------+----------+--------+--------+
| 0.944 | 0.993 | 0.9987 |  0.0748 | 2.6253 | 0.0313 |  0.1052  | 9.3024 | 0.2619 |
+-------+-------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 02:25:00,453 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 02:25:00,453 - depth - INFO - Epoch(val) [12][82]	a1: 0.9440, a2: 0.9930, a3: 0.9987, abs_rel: 0.07483725994825363, rmse: 2.6253244876861572, log_10: 0.03125876560807228, rmse_log: 0.10520043224096298, silog: 9.3024, sq_rel: 0.2618638277053833
2022-06-05 02:25:29,813 - depth - INFO - Epoch [13][50/1447]	lr: 8.093e-05, eta: 1:49:08, time: 0.587, data_time: 0.203, memory: 12862, decode.loss_depth: 0.0497, loss: 0.0497, grad_norm: 0.8318
2022-06-05 02:25:48,271 - depth - INFO - Epoch [13][100/1447]	lr: 8.067e-05, eta: 1:48:48, time: 0.369, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0493, loss: 0.0493, grad_norm: 0.6230
2022-06-05 02:26:06,525 - depth - INFO - Epoch [13][150/1447]	lr: 8.042e-05, eta: 1:48:29, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0491, loss: 0.0491, grad_norm: 0.8442
2022-06-05 02:26:24,976 - depth - INFO - Epoch [13][200/1447]	lr: 8.016e-05, eta: 1:48:09, time: 0.369, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0494, loss: 0.0494, grad_norm: 0.7420
2022-06-05 02:26:43,259 - depth - INFO - Epoch [13][250/1447]	lr: 7.990e-05, eta: 1:47:50, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0495, loss: 0.0495, grad_norm: 0.9936
2022-06-05 02:27:01,476 - depth - INFO - Epoch [13][300/1447]	lr: 7.964e-05, eta: 1:47:30, time: 0.364, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0486, loss: 0.0486, grad_norm: 1.1605
2022-06-05 02:27:19,733 - depth - INFO - Epoch [13][350/1447]	lr: 7.938e-05, eta: 1:47:11, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0496, loss: 0.0496, grad_norm: 0.8179
2022-06-05 02:27:38,027 - depth - INFO - Epoch [13][400/1447]	lr: 7.912e-05, eta: 1:46:51, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0494, loss: 0.0494, grad_norm: 0.8850
2022-06-05 02:27:56,406 - depth - INFO - Epoch [13][450/1447]	lr: 7.886e-05, eta: 1:46:32, time: 0.368, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0487, loss: 0.0487, grad_norm: 0.7821
2022-06-05 02:28:14,700 - depth - INFO - Epoch [13][500/1447]	lr: 7.859e-05, eta: 1:46:12, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0490, loss: 0.0490, grad_norm: 0.6690
2022-06-05 02:28:33,011 - depth - INFO - Epoch [13][550/1447]	lr: 7.833e-05, eta: 1:45:53, time: 0.366, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0483, loss: 0.0483, grad_norm: 0.6929
2022-06-05 02:28:51,328 - depth - INFO - Epoch [13][600/1447]	lr: 7.806e-05, eta: 1:45:34, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0487, loss: 0.0487, grad_norm: 0.7774
2022-06-05 02:29:09,826 - depth - INFO - Epoch [13][650/1447]	lr: 7.779e-05, eta: 1:45:14, time: 0.370, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0485, loss: 0.0485, grad_norm: 0.6126
2022-06-05 02:29:27,974 - depth - INFO - Epoch [13][700/1447]	lr: 7.752e-05, eta: 1:44:55, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0489, loss: 0.0489, grad_norm: 0.5205
2022-06-05 02:29:46,102 - depth - INFO - Epoch [13][750/1447]	lr: 7.725e-05, eta: 1:44:35, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0488, loss: 0.0488, grad_norm: 0.6985
2022-06-05 02:30:04,185 - depth - INFO - Epoch [13][800/1447]	lr: 7.698e-05, eta: 1:44:15, time: 0.362, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0484, loss: 0.0484, grad_norm: 0.6895
2022-06-05 02:30:22,499 - depth - INFO - Epoch [13][850/1447]	lr: 7.671e-05, eta: 1:43:56, time: 0.366, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0482, loss: 0.0482, grad_norm: 0.7455
2022-06-05 02:30:40,866 - depth - INFO - Epoch [13][900/1447]	lr: 7.644e-05, eta: 1:43:37, time: 0.367, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0489, loss: 0.0489, grad_norm: 0.8502
2022-06-05 02:30:59,230 - depth - INFO - Epoch [13][950/1447]	lr: 7.616e-05, eta: 1:43:17, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0473, loss: 0.0473, grad_norm: 0.8522
2022-06-05 02:31:17,453 - depth - INFO - Epoch [13][1000/1447]	lr: 7.589e-05, eta: 1:42:58, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0486, loss: 0.0486, grad_norm: 0.7346
2022-06-05 02:31:35,771 - depth - INFO - Epoch [13][1050/1447]	lr: 7.561e-05, eta: 1:42:39, time: 0.366, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0474, loss: 0.0474, grad_norm: 0.6396
2022-06-05 02:31:54,079 - depth - INFO - Epoch [13][1100/1447]	lr: 7.533e-05, eta: 1:42:19, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0476, loss: 0.0476, grad_norm: 0.9145
2022-06-05 02:32:12,790 - depth - INFO - Epoch [13][1150/1447]	lr: 7.505e-05, eta: 1:42:00, time: 0.374, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0483, loss: 0.0483, grad_norm: 1.1854
2022-06-05 02:32:31,005 - depth - INFO - Epoch [13][1200/1447]	lr: 7.477e-05, eta: 1:41:41, time: 0.364, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0471, loss: 0.0471, grad_norm: 0.6852
2022-06-05 02:32:49,412 - depth - INFO - Epoch [13][1250/1447]	lr: 7.449e-05, eta: 1:41:21, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0467, loss: 0.0467, grad_norm: 0.7491
2022-06-05 02:33:07,858 - depth - INFO - Epoch [13][1300/1447]	lr: 7.421e-05, eta: 1:41:02, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0472, loss: 0.0472, grad_norm: 0.8573
2022-06-05 02:33:26,120 - depth - INFO - Epoch [13][1350/1447]	lr: 7.393e-05, eta: 1:40:43, time: 0.365, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0465, loss: 0.0465, grad_norm: 0.6854
2022-06-05 02:33:44,469 - depth - INFO - Epoch [13][1400/1447]	lr: 7.364e-05, eta: 1:40:23, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0467, loss: 0.0467, grad_norm: 0.9363
2022-06-05 02:34:02,209 - depth - INFO - Saving checkpoint at 13 epochs
2022-06-05 02:34:32,592 - depth - INFO - Summary:
2022-06-05 02:34:32,593 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9499 | 0.9933 | 0.9989 |  0.0736 | 2.5913 | 0.0308 |  0.1025  | 8.9873 | 0.2582 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 02:34:32,593 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 02:34:32,593 - depth - INFO - Epoch(val) [13][82]	a1: 0.9499, a2: 0.9933, a3: 0.9989, abs_rel: 0.07364041358232498, rmse: 2.591259241104126, log_10: 0.030774516984820366, rmse_log: 0.10252579301595688, silog: 8.9873, sq_rel: 0.25815001130104065
2022-06-05 02:35:02,099 - depth - INFO - Epoch [14][50/1447]	lr: 7.309e-05, eta: 1:39:41, time: 0.590, data_time: 0.207, memory: 12862, decode.loss_depth: 0.0471, loss: 0.0471, grad_norm: 0.5578
2022-06-05 02:35:20,376 - depth - INFO - Epoch [14][100/1447]	lr: 7.280e-05, eta: 1:39:22, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0468, loss: 0.0468, grad_norm: 0.7226
2022-06-05 02:35:38,919 - depth - INFO - Epoch [14][150/1447]	lr: 7.251e-05, eta: 1:39:02, time: 0.371, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0463, loss: 0.0463, grad_norm: 0.7027
2022-06-05 02:35:57,136 - depth - INFO - Epoch [14][200/1447]	lr: 7.222e-05, eta: 1:38:43, time: 0.364, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0474, loss: 0.0474, grad_norm: 0.6871
2022-06-05 02:36:15,369 - depth - INFO - Epoch [14][250/1447]	lr: 7.193e-05, eta: 1:38:24, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0464, loss: 0.0464, grad_norm: 0.6200
2022-06-05 02:36:33,726 - depth - INFO - Epoch [14][300/1447]	lr: 7.164e-05, eta: 1:38:05, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0463, loss: 0.0463, grad_norm: 0.9450
2022-06-05 02:36:52,131 - depth - INFO - Epoch [14][350/1447]	lr: 7.135e-05, eta: 1:37:45, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0459, loss: 0.0459, grad_norm: 0.7075
2022-06-05 02:37:10,361 - depth - INFO - Epoch [14][400/1447]	lr: 7.106e-05, eta: 1:37:26, time: 0.365, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0460, loss: 0.0460, grad_norm: 0.9829
2022-06-05 02:37:28,781 - depth - INFO - Epoch [14][450/1447]	lr: 7.077e-05, eta: 1:37:07, time: 0.368, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0461, loss: 0.0461, grad_norm: 0.7026
2022-06-05 02:37:47,021 - depth - INFO - Epoch [14][500/1447]	lr: 7.047e-05, eta: 1:36:47, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0457, loss: 0.0457, grad_norm: 0.7939
2022-06-05 02:38:05,316 - depth - INFO - Epoch [14][550/1447]	lr: 7.018e-05, eta: 1:36:28, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0460, loss: 0.0460, grad_norm: 0.7450
2022-06-05 02:38:23,710 - depth - INFO - Epoch [14][600/1447]	lr: 6.988e-05, eta: 1:36:09, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0459, loss: 0.0459, grad_norm: 0.7022
2022-06-05 02:38:41,861 - depth - INFO - Epoch [14][650/1447]	lr: 6.958e-05, eta: 1:35:50, time: 0.363, data_time: 0.006, memory: 12862, decode.loss_depth: 0.0459, loss: 0.0459, grad_norm: 0.7339
2022-06-05 02:39:00,107 - depth - INFO - Epoch [14][700/1447]	lr: 6.929e-05, eta: 1:35:30, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0459, loss: 0.0459, grad_norm: 0.8391
2022-06-05 02:39:18,472 - depth - INFO - Epoch [14][750/1447]	lr: 6.899e-05, eta: 1:35:11, time: 0.367, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0461, loss: 0.0461, grad_norm: 0.6527
2022-06-05 02:39:36,583 - depth - INFO - Epoch [14][800/1447]	lr: 6.869e-05, eta: 1:34:52, time: 0.362, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0464, loss: 0.0464, grad_norm: 0.5398
2022-06-05 02:39:54,856 - depth - INFO - Epoch [14][850/1447]	lr: 6.839e-05, eta: 1:34:33, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0450, loss: 0.0450, grad_norm: 0.6300
2022-06-05 02:40:13,015 - depth - INFO - Epoch [14][900/1447]	lr: 6.809e-05, eta: 1:34:13, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0458, loss: 0.0458, grad_norm: 0.5247
2022-06-05 02:40:31,479 - depth - INFO - Epoch [14][950/1447]	lr: 6.779e-05, eta: 1:33:54, time: 0.370, data_time: 0.011, memory: 12862, decode.loss_depth: 0.0453, loss: 0.0453, grad_norm: 0.7154
2022-06-05 02:40:49,670 - depth - INFO - Epoch [14][1000/1447]	lr: 6.748e-05, eta: 1:33:35, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0451, loss: 0.0451, grad_norm: 0.7751
2022-06-05 02:41:08,116 - depth - INFO - Epoch [14][1050/1447]	lr: 6.718e-05, eta: 1:33:16, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0453, loss: 0.0453, grad_norm: 1.0348
2022-06-05 02:41:26,217 - depth - INFO - Epoch [14][1100/1447]	lr: 6.688e-05, eta: 1:32:56, time: 0.362, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0448, loss: 0.0448, grad_norm: 0.7519
2022-06-05 02:41:44,500 - depth - INFO - Epoch [14][1150/1447]	lr: 6.657e-05, eta: 1:32:37, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0451, loss: 0.0451, grad_norm: 0.6650
2022-06-05 02:42:02,921 - depth - INFO - Epoch [14][1200/1447]	lr: 6.627e-05, eta: 1:32:18, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0448, loss: 0.0448, grad_norm: 0.9142
2022-06-05 02:42:21,584 - depth - INFO - Epoch [14][1250/1447]	lr: 6.596e-05, eta: 1:31:59, time: 0.373, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0451, loss: 0.0451, grad_norm: 0.8795
2022-06-05 02:42:40,021 - depth - INFO - Epoch [14][1300/1447]	lr: 6.566e-05, eta: 1:31:40, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0453, loss: 0.0453, grad_norm: 0.7449
2022-06-05 02:42:58,405 - depth - INFO - Epoch [14][1350/1447]	lr: 6.535e-05, eta: 1:31:21, time: 0.367, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0451, loss: 0.0451, grad_norm: 0.5104
2022-06-05 02:43:17,016 - depth - INFO - Epoch [14][1400/1447]	lr: 6.504e-05, eta: 1:31:02, time: 0.372, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0447, loss: 0.0447, grad_norm: 0.5698
2022-06-05 02:43:34,621 - depth - INFO - Saving checkpoint at 14 epochs
2022-06-05 02:44:05,558 - depth - INFO - Summary:
2022-06-05 02:44:05,559 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9505 | 0.9935 | 0.9989 |  0.0705 | 2.5862 | 0.0296 |  0.1011  | 9.0121 | 0.2504 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 02:44:05,559 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 02:44:05,559 - depth - INFO - Epoch(val) [14][82]	a1: 0.9505, a2: 0.9935, a3: 0.9989, abs_rel: 0.07051379233598709, rmse: 2.5861830711364746, log_10: 0.02959120087325573, rmse_log: 0.10107813030481339, silog: 9.0121, sq_rel: 0.25037655234336853
2022-06-05 02:44:35,399 - depth - INFO - Epoch [15][50/1447]	lr: 6.444e-05, eta: 1:30:21, time: 0.596, data_time: 0.239, memory: 12862, decode.loss_depth: 0.0449, loss: 0.0449, grad_norm: 0.5897
2022-06-05 02:44:53,524 - depth - INFO - Epoch [15][100/1447]	lr: 6.413e-05, eta: 1:30:01, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0441, loss: 0.0441, grad_norm: 0.6265
2022-06-05 02:45:11,738 - depth - INFO - Epoch [15][150/1447]	lr: 6.382e-05, eta: 1:29:42, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0443, loss: 0.0443, grad_norm: 0.7998
2022-06-05 02:45:30,160 - depth - INFO - Epoch [15][200/1447]	lr: 6.351e-05, eta: 1:29:23, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0443, loss: 0.0443, grad_norm: 0.5252
2022-06-05 02:45:48,823 - depth - INFO - Epoch [15][250/1447]	lr: 6.320e-05, eta: 1:29:04, time: 0.373, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0448, loss: 0.0448, grad_norm: 0.8813
2022-06-05 02:46:07,302 - depth - INFO - Epoch [15][300/1447]	lr: 6.289e-05, eta: 1:28:45, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0449, loss: 0.0449, grad_norm: 0.8861
2022-06-05 02:46:25,580 - depth - INFO - Epoch [15][350/1447]	lr: 6.258e-05, eta: 1:28:26, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0439, loss: 0.0439, grad_norm: 0.9199
2022-06-05 02:46:44,228 - depth - INFO - Epoch [15][400/1447]	lr: 6.226e-05, eta: 1:28:07, time: 0.373, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0435, loss: 0.0435, grad_norm: 0.6448
2022-06-05 02:47:02,606 - depth - INFO - Epoch [15][450/1447]	lr: 6.195e-05, eta: 1:27:48, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0446, loss: 0.0446, grad_norm: 0.7220
2022-06-05 02:47:20,912 - depth - INFO - Epoch [15][500/1447]	lr: 6.164e-05, eta: 1:27:29, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0444, loss: 0.0444, grad_norm: 0.6583
2022-06-05 02:47:39,746 - depth - INFO - Epoch [15][550/1447]	lr: 6.132e-05, eta: 1:27:10, time: 0.376, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0441, loss: 0.0441, grad_norm: 0.6311
2022-06-05 02:47:58,193 - depth - INFO - Epoch [15][600/1447]	lr: 6.101e-05, eta: 1:26:51, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0444, loss: 0.0444, grad_norm: 0.7546
2022-06-05 02:48:16,299 - depth - INFO - Epoch [15][650/1447]	lr: 6.069e-05, eta: 1:26:32, time: 0.362, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0434, loss: 0.0434, grad_norm: 0.5493
2022-06-05 02:48:34,625 - depth - INFO - Epoch [15][700/1447]	lr: 6.038e-05, eta: 1:26:13, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0440, loss: 0.0440, grad_norm: 0.8448
2022-06-05 02:48:52,914 - depth - INFO - Epoch [15][750/1447]	lr: 6.006e-05, eta: 1:25:54, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0444, loss: 0.0444, grad_norm: 0.6111
2022-06-05 02:49:11,060 - depth - INFO - Epoch [15][800/1447]	lr: 5.974e-05, eta: 1:25:35, time: 0.363, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0439, loss: 0.0439, grad_norm: 1.0548
2022-06-05 02:49:29,558 - depth - INFO - Epoch [15][850/1447]	lr: 5.943e-05, eta: 1:25:16, time: 0.370, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0430, loss: 0.0430, grad_norm: 0.6345
2022-06-05 02:49:47,756 - depth - INFO - Epoch [15][900/1447]	lr: 5.911e-05, eta: 1:24:57, time: 0.364, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0437, loss: 0.0437, grad_norm: 0.7972
2022-06-05 02:50:06,292 - depth - INFO - Epoch [15][950/1447]	lr: 5.879e-05, eta: 1:24:38, time: 0.371, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0432, loss: 0.0432, grad_norm: 0.7966
2022-06-05 02:50:24,483 - depth - INFO - Epoch [15][1000/1447]	lr: 5.847e-05, eta: 1:24:18, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0444, loss: 0.0444, grad_norm: 0.7831
2022-06-05 02:50:42,724 - depth - INFO - Epoch [15][1050/1447]	lr: 5.816e-05, eta: 1:23:59, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0432, loss: 0.0432, grad_norm: 0.5753
2022-06-05 02:51:00,968 - depth - INFO - Epoch [15][1100/1447]	lr: 5.784e-05, eta: 1:23:40, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0431, loss: 0.0431, grad_norm: 0.7602
2022-06-05 02:51:19,202 - depth - INFO - Epoch [15][1150/1447]	lr: 5.752e-05, eta: 1:23:21, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0430, loss: 0.0430, grad_norm: 0.7913
2022-06-05 02:51:37,389 - depth - INFO - Epoch [15][1200/1447]	lr: 5.720e-05, eta: 1:23:02, time: 0.364, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0430, loss: 0.0430, grad_norm: 0.7262
2022-06-05 02:51:55,591 - depth - INFO - Epoch [15][1250/1447]	lr: 5.688e-05, eta: 1:22:43, time: 0.364, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0426, loss: 0.0426, grad_norm: 0.6384
2022-06-05 02:52:14,051 - depth - INFO - Epoch [15][1300/1447]	lr: 5.656e-05, eta: 1:22:24, time: 0.369, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0424, loss: 0.0424, grad_norm: 0.6420
2022-06-05 02:52:32,610 - depth - INFO - Epoch [15][1350/1447]	lr: 5.624e-05, eta: 1:22:05, time: 0.371, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0426, loss: 0.0426, grad_norm: 0.6700
2022-06-05 02:52:50,991 - depth - INFO - Epoch [15][1400/1447]	lr: 5.592e-05, eta: 1:21:46, time: 0.368, data_time: 0.011, memory: 12862, decode.loss_depth: 0.0427, loss: 0.0427, grad_norm: 0.5329
2022-06-05 02:53:08,805 - depth - INFO - Saving checkpoint at 15 epochs
2022-06-05 02:53:39,764 - depth - INFO - Summary:
2022-06-05 02:53:39,765 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9442 | 0.9933 | 0.9989 |  0.0776 | 2.5996 | 0.0322 |  0.1058  | 9.1179 | 0.2645 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 02:53:39,765 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 02:53:39,766 - depth - INFO - Epoch(val) [15][82]	a1: 0.9442, a2: 0.9933, a3: 0.9989, abs_rel: 0.07755224406719208, rmse: 2.5996298789978027, log_10: 0.03219373896718025, rmse_log: 0.10582612454891205, silog: 9.1179, sq_rel: 0.26446327567100525
2022-06-05 02:54:09,277 - depth - INFO - Epoch [16][50/1447]	lr: 5.529e-05, eta: 1:21:05, time: 0.590, data_time: 0.225, memory: 12862, decode.loss_depth: 0.0433, loss: 0.0433, grad_norm: 0.6309
2022-06-05 02:54:27,601 - depth - INFO - Epoch [16][100/1447]	lr: 5.497e-05, eta: 1:20:46, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0427, loss: 0.0427, grad_norm: 0.8300
2022-06-05 02:54:45,837 - depth - INFO - Epoch [16][150/1447]	lr: 5.465e-05, eta: 1:20:27, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0427, loss: 0.0427, grad_norm: 0.5632
2022-06-05 02:55:04,556 - depth - INFO - Epoch [16][200/1447]	lr: 5.433e-05, eta: 1:20:09, time: 0.374, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0429, loss: 0.0429, grad_norm: 0.5510
2022-06-05 02:55:22,776 - depth - INFO - Epoch [16][250/1447]	lr: 5.401e-05, eta: 1:19:50, time: 0.364, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0425, loss: 0.0425, grad_norm: 0.8152
2022-06-05 02:55:41,042 - depth - INFO - Epoch [16][300/1447]	lr: 5.369e-05, eta: 1:19:30, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0425, loss: 0.0425, grad_norm: 1.0554
2022-06-05 02:55:59,279 - depth - INFO - Epoch [16][350/1447]	lr: 5.336e-05, eta: 1:19:11, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0423, loss: 0.0423, grad_norm: 0.5741
2022-06-05 02:56:17,681 - depth - INFO - Epoch [16][400/1447]	lr: 5.304e-05, eta: 1:18:52, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0422, loss: 0.0422, grad_norm: 0.7364
2022-06-05 02:56:35,994 - depth - INFO - Epoch [16][450/1447]	lr: 5.272e-05, eta: 1:18:34, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0421, loss: 0.0421, grad_norm: 0.8651
2022-06-05 02:56:54,488 - depth - INFO - Epoch [16][500/1447]	lr: 5.240e-05, eta: 1:18:15, time: 0.370, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0419, loss: 0.0419, grad_norm: 0.6636
2022-06-05 02:57:12,663 - depth - INFO - Epoch [16][550/1447]	lr: 5.207e-05, eta: 1:17:56, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0423, loss: 0.0423, grad_norm: 0.5323
2022-06-05 02:57:30,868 - depth - INFO - Epoch [16][600/1447]	lr: 5.175e-05, eta: 1:17:36, time: 0.364, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0416, loss: 0.0416, grad_norm: 0.6148
2022-06-05 02:57:49,126 - depth - INFO - Epoch [16][650/1447]	lr: 5.143e-05, eta: 1:17:17, time: 0.365, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0420, loss: 0.0420, grad_norm: 0.5934
2022-06-05 02:58:07,431 - depth - INFO - Epoch [16][700/1447]	lr: 5.111e-05, eta: 1:16:59, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0415, loss: 0.0415, grad_norm: 0.6194
2022-06-05 02:58:25,838 - depth - INFO - Epoch [16][750/1447]	lr: 5.078e-05, eta: 1:16:40, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0420, loss: 0.0420, grad_norm: 0.7120
2022-06-05 02:58:43,961 - depth - INFO - Epoch [16][800/1447]	lr: 5.046e-05, eta: 1:16:21, time: 0.362, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0415, loss: 0.0415, grad_norm: 0.4723
2022-06-05 02:59:02,230 - depth - INFO - Epoch [16][850/1447]	lr: 5.014e-05, eta: 1:16:02, time: 0.365, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0409, loss: 0.0409, grad_norm: 0.6256
2022-06-05 02:59:20,361 - depth - INFO - Epoch [16][900/1447]	lr: 4.981e-05, eta: 1:15:42, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0416, loss: 0.0416, grad_norm: 0.7701
2022-06-05 02:59:38,563 - depth - INFO - Epoch [16][950/1447]	lr: 4.949e-05, eta: 1:15:23, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0416, loss: 0.0416, grad_norm: 0.7907
2022-06-05 02:59:57,161 - depth - INFO - Epoch [16][1000/1447]	lr: 4.917e-05, eta: 1:15:05, time: 0.372, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0418, loss: 0.0418, grad_norm: 0.8445
2022-06-05 03:00:15,481 - depth - INFO - Epoch [16][1050/1447]	lr: 4.885e-05, eta: 1:14:46, time: 0.367, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0411, loss: 0.0411, grad_norm: 0.5357
2022-06-05 03:00:33,750 - depth - INFO - Epoch [16][1100/1447]	lr: 4.852e-05, eta: 1:14:27, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0411, loss: 0.0411, grad_norm: 0.5649
2022-06-05 03:00:52,112 - depth - INFO - Epoch [16][1150/1447]	lr: 4.820e-05, eta: 1:14:08, time: 0.367, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0409, loss: 0.0409, grad_norm: 0.7412
2022-06-05 03:01:10,331 - depth - INFO - Epoch [16][1200/1447]	lr: 4.788e-05, eta: 1:13:49, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0410, loss: 0.0410, grad_norm: 0.8143
2022-06-05 03:01:28,594 - depth - INFO - Epoch [16][1250/1447]	lr: 4.755e-05, eta: 1:13:30, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0409, loss: 0.0409, grad_norm: 0.5689
2022-06-05 03:01:46,930 - depth - INFO - Epoch [16][1300/1447]	lr: 4.723e-05, eta: 1:13:11, time: 0.367, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0405, loss: 0.0405, grad_norm: 0.7223
2022-06-05 03:02:05,323 - depth - INFO - Epoch [16][1350/1447]	lr: 4.691e-05, eta: 1:12:52, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0406, loss: 0.0406, grad_norm: 0.5817
2022-06-05 03:02:23,762 - depth - INFO - Epoch [16][1400/1447]	lr: 4.659e-05, eta: 1:12:33, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0405, loss: 0.0405, grad_norm: 0.7239
2022-06-05 03:02:41,300 - depth - INFO - Saving checkpoint at 16 epochs
2022-06-05 03:03:11,070 - depth - INFO - Summary:
2022-06-05 03:03:11,070 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9461 | 0.9933 | 0.9988 |  0.075  | 2.5962 | 0.0312 |  0.1044  | 9.0706 | 0.266  |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 03:03:11,071 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 03:03:11,071 - depth - INFO - Epoch(val) [16][82]	a1: 0.9461, a2: 0.9933, a3: 0.9988, abs_rel: 0.07504889369010925, rmse: 2.596240520477295, log_10: 0.031154323369264603, rmse_log: 0.10435447096824646, silog: 9.0706, sq_rel: 0.26599907875061035
2022-06-05 03:03:40,393 - depth - INFO - Epoch [17][50/1447]	lr: 4.596e-05, eta: 1:11:53, time: 0.586, data_time: 0.204, memory: 12862, decode.loss_depth: 0.0408, loss: 0.0408, grad_norm: 0.5891
2022-06-05 03:03:58,637 - depth - INFO - Epoch [17][100/1447]	lr: 4.564e-05, eta: 1:11:34, time: 0.365, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0407, loss: 0.0407, grad_norm: 0.5146
2022-06-05 03:04:16,928 - depth - INFO - Epoch [17][150/1447]	lr: 4.532e-05, eta: 1:11:15, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0404, loss: 0.0404, grad_norm: 0.4138
2022-06-05 03:04:35,189 - depth - INFO - Epoch [17][200/1447]	lr: 4.500e-05, eta: 1:10:56, time: 0.365, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0408, loss: 0.0408, grad_norm: 0.5031
2022-06-05 03:04:53,694 - depth - INFO - Epoch [17][250/1447]	lr: 4.468e-05, eta: 1:10:38, time: 0.370, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0407, loss: 0.0407, grad_norm: 0.6190
2022-06-05 03:05:12,053 - depth - INFO - Epoch [17][300/1447]	lr: 4.436e-05, eta: 1:10:19, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0407, loss: 0.0407, grad_norm: 0.4986
2022-06-05 03:05:30,373 - depth - INFO - Epoch [17][350/1447]	lr: 4.404e-05, eta: 1:10:00, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0402, loss: 0.0402, grad_norm: 0.4133
2022-06-05 03:05:48,773 - depth - INFO - Epoch [17][400/1447]	lr: 4.371e-05, eta: 1:09:41, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0394, loss: 0.0394, grad_norm: 0.3614
2022-06-05 03:06:07,041 - depth - INFO - Epoch [17][450/1447]	lr: 4.339e-05, eta: 1:09:22, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0399, loss: 0.0399, grad_norm: 0.3672
2022-06-05 03:06:25,152 - depth - INFO - Epoch [17][500/1447]	lr: 4.307e-05, eta: 1:09:03, time: 0.362, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0401, loss: 0.0401, grad_norm: 0.5620
2022-06-05 03:06:43,364 - depth - INFO - Epoch [17][550/1447]	lr: 4.275e-05, eta: 1:08:44, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0404, loss: 0.0404, grad_norm: 0.7546
2022-06-05 03:07:01,559 - depth - INFO - Epoch [17][600/1447]	lr: 4.244e-05, eta: 1:08:25, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0407, loss: 0.0407, grad_norm: 0.5101
2022-06-05 03:07:19,985 - depth - INFO - Epoch [17][650/1447]	lr: 4.212e-05, eta: 1:08:06, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0402, loss: 0.0402, grad_norm: 0.7441
2022-06-05 03:07:38,339 - depth - INFO - Epoch [17][700/1447]	lr: 4.180e-05, eta: 1:07:48, time: 0.367, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0399, loss: 0.0399, grad_norm: 0.7839
2022-06-05 03:07:56,525 - depth - INFO - Epoch [17][750/1447]	lr: 4.148e-05, eta: 1:07:29, time: 0.364, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0406, loss: 0.0406, grad_norm: 0.5270
2022-06-05 03:08:14,789 - depth - INFO - Epoch [17][800/1447]	lr: 4.116e-05, eta: 1:07:10, time: 0.366, data_time: 0.006, memory: 12862, decode.loss_depth: 0.0401, loss: 0.0401, grad_norm: 0.5149
2022-06-05 03:08:32,940 - depth - INFO - Epoch [17][850/1447]	lr: 4.084e-05, eta: 1:06:51, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0391, loss: 0.0391, grad_norm: 0.4292
2022-06-05 03:08:51,285 - depth - INFO - Epoch [17][900/1447]	lr: 4.053e-05, eta: 1:06:32, time: 0.367, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0400, loss: 0.0400, grad_norm: 0.5109
2022-06-05 03:09:09,842 - depth - INFO - Epoch [17][950/1447]	lr: 4.021e-05, eta: 1:06:13, time: 0.371, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0402, loss: 0.0402, grad_norm: 0.4616
2022-06-05 03:09:28,284 - depth - INFO - Epoch [17][1000/1447]	lr: 3.989e-05, eta: 1:05:54, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0399, loss: 0.0399, grad_norm: 0.5423
2022-06-05 03:09:46,364 - depth - INFO - Epoch [17][1050/1447]	lr: 3.958e-05, eta: 1:05:35, time: 0.362, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0390, loss: 0.0390, grad_norm: 0.4249
2022-06-05 03:10:04,638 - depth - INFO - Epoch [17][1100/1447]	lr: 3.926e-05, eta: 1:05:17, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0391, loss: 0.0391, grad_norm: 0.3661
2022-06-05 03:10:22,890 - depth - INFO - Epoch [17][1150/1447]	lr: 3.895e-05, eta: 1:04:58, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0393, loss: 0.0393, grad_norm: 0.4927
2022-06-05 03:10:41,511 - depth - INFO - Epoch [17][1200/1447]	lr: 3.863e-05, eta: 1:04:39, time: 0.372, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0394, loss: 0.0394, grad_norm: 0.6313
2022-06-05 03:10:59,747 - depth - INFO - Epoch [17][1250/1447]	lr: 3.832e-05, eta: 1:04:20, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0388, loss: 0.0388, grad_norm: 0.4948
2022-06-05 03:11:18,098 - depth - INFO - Epoch [17][1300/1447]	lr: 3.800e-05, eta: 1:04:01, time: 0.367, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0388, loss: 0.0388, grad_norm: 0.6290
2022-06-05 03:11:36,549 - depth - INFO - Epoch [17][1350/1447]	lr: 3.769e-05, eta: 1:03:42, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0391, loss: 0.0391, grad_norm: 0.6234
2022-06-05 03:11:54,983 - depth - INFO - Epoch [17][1400/1447]	lr: 3.738e-05, eta: 1:03:24, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0387, loss: 0.0387, grad_norm: 0.5631
2022-06-05 03:12:12,575 - depth - INFO - Saving checkpoint at 17 epochs
2022-06-05 03:12:43,179 - depth - INFO - Summary:
2022-06-05 03:12:43,180 - depth - INFO - 
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
| 0.9469 | 0.9935 | 0.9989 |  0.0753 | 2.586 | 0.0314 |  0.104   | 9.0169 | 0.2583 |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
2022-06-05 03:12:43,180 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 03:12:43,181 - depth - INFO - Epoch(val) [17][82]	a1: 0.9469, a2: 0.9935, a3: 0.9989, abs_rel: 0.07530933618545532, rmse: 2.5860466957092285, log_10: 0.03140023723244667, rmse_log: 0.10401071608066559, silog: 9.0169, sq_rel: 0.25829190015792847
2022-06-05 03:13:12,768 - depth - INFO - Epoch [18][50/1447]	lr: 3.677e-05, eta: 1:02:45, time: 0.591, data_time: 0.231, memory: 12862, decode.loss_depth: 0.0392, loss: 0.0392, grad_norm: 0.6395
2022-06-05 03:13:31,349 - depth - INFO - Epoch [18][100/1447]	lr: 3.646e-05, eta: 1:02:26, time: 0.372, data_time: 0.014, memory: 12862, decode.loss_depth: 0.0393, loss: 0.0393, grad_norm: 0.5237
2022-06-05 03:13:49,394 - depth - INFO - Epoch [18][150/1447]	lr: 3.615e-05, eta: 1:02:07, time: 0.361, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0387, loss: 0.0387, grad_norm: 0.4416
2022-06-05 03:14:07,640 - depth - INFO - Epoch [18][200/1447]	lr: 3.584e-05, eta: 1:01:48, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0388, loss: 0.0388, grad_norm: 0.4368
2022-06-05 03:14:25,902 - depth - INFO - Epoch [18][250/1447]	lr: 3.553e-05, eta: 1:01:29, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0391, loss: 0.0391, grad_norm: 0.6002
2022-06-05 03:14:44,222 - depth - INFO - Epoch [18][300/1447]	lr: 3.522e-05, eta: 1:01:10, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0388, loss: 0.0388, grad_norm: 0.6435
2022-06-05 03:15:02,640 - depth - INFO - Epoch [18][350/1447]	lr: 3.491e-05, eta: 1:00:52, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0382, loss: 0.0382, grad_norm: 0.3859
2022-06-05 03:15:20,867 - depth - INFO - Epoch [18][400/1447]	lr: 3.461e-05, eta: 1:00:33, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0383, loss: 0.0383, grad_norm: 0.7221
2022-06-05 03:15:39,198 - depth - INFO - Epoch [18][450/1447]	lr: 3.430e-05, eta: 1:00:14, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0386, loss: 0.0386, grad_norm: 0.6112
2022-06-05 03:15:57,509 - depth - INFO - Epoch [18][500/1447]	lr: 3.399e-05, eta: 0:59:55, time: 0.366, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0383, loss: 0.0383, grad_norm: 0.4976
2022-06-05 03:16:15,917 - depth - INFO - Epoch [18][550/1447]	lr: 3.369e-05, eta: 0:59:36, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0385, loss: 0.0385, grad_norm: 0.6070
2022-06-05 03:16:34,270 - depth - INFO - Epoch [18][600/1447]	lr: 3.338e-05, eta: 0:59:18, time: 0.367, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0385, loss: 0.0385, grad_norm: 0.3849
2022-06-05 03:16:52,630 - depth - INFO - Epoch [18][650/1447]	lr: 3.308e-05, eta: 0:58:59, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0382, loss: 0.0382, grad_norm: 0.7714
2022-06-05 03:17:10,934 - depth - INFO - Epoch [18][700/1447]	lr: 3.278e-05, eta: 0:58:40, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0380, loss: 0.0380, grad_norm: 0.4752
2022-06-05 03:17:29,103 - depth - INFO - Epoch [18][750/1447]	lr: 3.247e-05, eta: 0:58:21, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0387, loss: 0.0387, grad_norm: 0.5847
2022-06-05 03:17:47,334 - depth - INFO - Epoch [18][800/1447]	lr: 3.217e-05, eta: 0:58:02, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0383, loss: 0.0383, grad_norm: 0.5061
2022-06-05 03:18:05,795 - depth - INFO - Epoch [18][850/1447]	lr: 3.187e-05, eta: 0:57:44, time: 0.369, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0376, loss: 0.0376, grad_norm: 0.5833
2022-06-05 03:18:23,977 - depth - INFO - Epoch [18][900/1447]	lr: 3.157e-05, eta: 0:57:25, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0385, loss: 0.0385, grad_norm: 0.6535
2022-06-05 03:18:42,281 - depth - INFO - Epoch [18][950/1447]	lr: 3.127e-05, eta: 0:57:06, time: 0.366, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0381, loss: 0.0381, grad_norm: 0.5009
2022-06-05 03:19:00,500 - depth - INFO - Epoch [18][1000/1447]	lr: 3.097e-05, eta: 0:56:47, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0386, loss: 0.0386, grad_norm: 0.3169
2022-06-05 03:19:18,780 - depth - INFO - Epoch [18][1050/1447]	lr: 3.067e-05, eta: 0:56:28, time: 0.366, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0378, loss: 0.0378, grad_norm: 0.3967
2022-06-05 03:19:37,067 - depth - INFO - Epoch [18][1100/1447]	lr: 3.037e-05, eta: 0:56:10, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0377, loss: 0.0377, grad_norm: 0.3813
2022-06-05 03:19:55,230 - depth - INFO - Epoch [18][1150/1447]	lr: 3.008e-05, eta: 0:55:51, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0379, loss: 0.0379, grad_norm: 0.3888
2022-06-05 03:20:13,650 - depth - INFO - Epoch [18][1200/1447]	lr: 2.978e-05, eta: 0:55:32, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0375, loss: 0.0375, grad_norm: 0.4681
2022-06-05 03:20:31,995 - depth - INFO - Epoch [18][1250/1447]	lr: 2.949e-05, eta: 0:55:13, time: 0.367, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0376, loss: 0.0376, grad_norm: 0.4358
2022-06-05 03:20:50,318 - depth - INFO - Epoch [18][1300/1447]	lr: 2.919e-05, eta: 0:54:54, time: 0.366, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0373, loss: 0.0373, grad_norm: 0.4415
2022-06-05 03:21:08,655 - depth - INFO - Epoch [18][1350/1447]	lr: 2.890e-05, eta: 0:54:36, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0376, loss: 0.0376, grad_norm: 0.5259
2022-06-05 03:21:27,087 - depth - INFO - Epoch [18][1400/1447]	lr: 2.861e-05, eta: 0:54:17, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0375, loss: 0.0375, grad_norm: 0.5967
2022-06-05 03:21:44,792 - depth - INFO - Saving checkpoint at 18 epochs
2022-06-05 03:22:14,605 - depth - INFO - Summary:
2022-06-05 03:22:14,605 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9443 | 0.9933 | 0.9988 |  0.0751 | 2.5889 | 0.0312 |  0.1045  | 9.0886 | 0.2644 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 03:22:14,606 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 03:22:14,606 - depth - INFO - Epoch(val) [18][82]	a1: 0.9443, a2: 0.9933, a3: 0.9988, abs_rel: 0.0751083716750145, rmse: 2.5888986587524414, log_10: 0.031171025708317757, rmse_log: 0.10449476540088654, silog: 9.0886, sq_rel: 0.2644446790218353
2022-06-05 03:22:44,961 - depth - INFO - Epoch [19][50/1447]	lr: 2.804e-05, eta: 0:53:39, time: 0.607, data_time: 0.230, memory: 12862, decode.loss_depth: 0.0375, loss: 0.0375, grad_norm: 0.4758
2022-06-05 03:23:03,214 - depth - INFO - Epoch [19][100/1447]	lr: 2.775e-05, eta: 0:53:20, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0375, loss: 0.0375, grad_norm: 0.3107
2022-06-05 03:23:21,479 - depth - INFO - Epoch [19][150/1447]	lr: 2.747e-05, eta: 0:53:01, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0372, loss: 0.0372, grad_norm: 0.6602
2022-06-05 03:23:39,940 - depth - INFO - Epoch [19][200/1447]	lr: 2.718e-05, eta: 0:52:43, time: 0.369, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0376, loss: 0.0376, grad_norm: 0.4674
2022-06-05 03:23:58,149 - depth - INFO - Epoch [19][250/1447]	lr: 2.689e-05, eta: 0:52:24, time: 0.364, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0372, loss: 0.0372, grad_norm: 0.4927
2022-06-05 03:24:16,501 - depth - INFO - Epoch [19][300/1447]	lr: 2.661e-05, eta: 0:52:05, time: 0.367, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0374, loss: 0.0374, grad_norm: 0.3866
2022-06-05 03:24:34,738 - depth - INFO - Epoch [19][350/1447]	lr: 2.632e-05, eta: 0:51:46, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0371, loss: 0.0371, grad_norm: 0.5785
2022-06-05 03:24:53,146 - depth - INFO - Epoch [19][400/1447]	lr: 2.604e-05, eta: 0:51:28, time: 0.368, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0367, loss: 0.0367, grad_norm: 0.7050
2022-06-05 03:25:11,375 - depth - INFO - Epoch [19][450/1447]	lr: 2.575e-05, eta: 0:51:09, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0376, loss: 0.0376, grad_norm: 0.4070
2022-06-05 03:25:29,698 - depth - INFO - Epoch [19][500/1447]	lr: 2.547e-05, eta: 0:50:50, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0368, loss: 0.0368, grad_norm: 0.6402
2022-06-05 03:25:47,929 - depth - INFO - Epoch [19][550/1447]	lr: 2.519e-05, eta: 0:50:31, time: 0.365, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0376, loss: 0.0376, grad_norm: 0.3300
2022-06-05 03:26:06,079 - depth - INFO - Epoch [19][600/1447]	lr: 2.491e-05, eta: 0:50:12, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0375, loss: 0.0375, grad_norm: 0.4324
2022-06-05 03:26:24,372 - depth - INFO - Epoch [19][650/1447]	lr: 2.463e-05, eta: 0:49:54, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0372, loss: 0.0372, grad_norm: 0.4519
2022-06-05 03:26:42,623 - depth - INFO - Epoch [19][700/1447]	lr: 2.436e-05, eta: 0:49:35, time: 0.365, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0364, loss: 0.0364, grad_norm: 0.4869
2022-06-05 03:27:00,992 - depth - INFO - Epoch [19][750/1447]	lr: 2.408e-05, eta: 0:49:16, time: 0.368, data_time: 0.012, memory: 12862, decode.loss_depth: 0.0376, loss: 0.0376, grad_norm: 0.3068
2022-06-05 03:27:19,293 - depth - INFO - Epoch [19][800/1447]	lr: 2.380e-05, eta: 0:48:57, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0369, loss: 0.0369, grad_norm: 0.4558
2022-06-05 03:27:37,627 - depth - INFO - Epoch [19][850/1447]	lr: 2.353e-05, eta: 0:48:39, time: 0.367, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0367, loss: 0.0367, grad_norm: 0.5814
2022-06-05 03:27:56,041 - depth - INFO - Epoch [19][900/1447]	lr: 2.326e-05, eta: 0:48:20, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0375, loss: 0.0375, grad_norm: 0.5967
2022-06-05 03:28:14,249 - depth - INFO - Epoch [19][950/1447]	lr: 2.298e-05, eta: 0:48:01, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0371, loss: 0.0371, grad_norm: 0.3890
2022-06-05 03:28:32,451 - depth - INFO - Epoch [19][1000/1447]	lr: 2.271e-05, eta: 0:47:43, time: 0.364, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0370, loss: 0.0370, grad_norm: 0.4694
2022-06-05 03:28:50,682 - depth - INFO - Epoch [19][1050/1447]	lr: 2.244e-05, eta: 0:47:24, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0370, loss: 0.0370, grad_norm: 0.3889
2022-06-05 03:29:08,797 - depth - INFO - Epoch [19][1100/1447]	lr: 2.217e-05, eta: 0:47:05, time: 0.362, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0362, loss: 0.0362, grad_norm: 0.3439
2022-06-05 03:29:27,132 - depth - INFO - Epoch [19][1150/1447]	lr: 2.191e-05, eta: 0:46:46, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0364, loss: 0.0364, grad_norm: 0.3537
2022-06-05 03:29:45,271 - depth - INFO - Epoch [19][1200/1447]	lr: 2.164e-05, eta: 0:46:28, time: 0.363, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0366, loss: 0.0366, grad_norm: 0.5242
2022-06-05 03:30:03,496 - depth - INFO - Epoch [19][1250/1447]	lr: 2.137e-05, eta: 0:46:09, time: 0.364, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 0.3657
2022-06-05 03:30:22,244 - depth - INFO - Epoch [19][1300/1447]	lr: 2.111e-05, eta: 0:45:50, time: 0.375, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0364, loss: 0.0364, grad_norm: 0.3420
2022-06-05 03:30:41,001 - depth - INFO - Epoch [19][1350/1447]	lr: 2.085e-05, eta: 0:45:32, time: 0.375, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0363, loss: 0.0363, grad_norm: 0.3321
2022-06-05 03:30:59,333 - depth - INFO - Epoch [19][1400/1447]	lr: 2.059e-05, eta: 0:45:13, time: 0.367, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0363, loss: 0.0363, grad_norm: 0.4149
2022-06-05 03:31:17,294 - depth - INFO - Saving checkpoint at 19 epochs
2022-06-05 03:31:47,130 - depth - INFO - Summary:
2022-06-05 03:31:47,130 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9463 | 0.9935 | 0.9989 |  0.0734 | 2.5912 | 0.0306 |  0.1032  | 9.0358 |  0.26  |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 03:31:47,131 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 03:31:47,131 - depth - INFO - Epoch(val) [19][82]	a1: 0.9463, a2: 0.9935, a3: 0.9989, abs_rel: 0.07336611300706863, rmse: 2.591216564178467, log_10: 0.030574049800634384, rmse_log: 0.10323844105005264, silog: 9.0358, sq_rel: 0.260021448135376
2022-06-05 03:32:15,730 - depth - INFO - Epoch [20][50/1447]	lr: 2.008e-05, eta: 0:44:35, time: 0.572, data_time: 0.198, memory: 12862, decode.loss_depth: 0.0363, loss: 0.0363, grad_norm: 0.4549
2022-06-05 03:32:34,128 - depth - INFO - Epoch [20][100/1447]	lr: 1.982e-05, eta: 0:44:16, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0368, loss: 0.0368, grad_norm: 0.2804
2022-06-05 03:32:52,408 - depth - INFO - Epoch [20][150/1447]	lr: 1.957e-05, eta: 0:43:57, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 0.2414
2022-06-05 03:33:10,688 - depth - INFO - Epoch [20][200/1447]	lr: 1.931e-05, eta: 0:43:39, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0362, loss: 0.0362, grad_norm: 0.4133
2022-06-05 03:33:29,253 - depth - INFO - Epoch [20][250/1447]	lr: 1.906e-05, eta: 0:43:20, time: 0.371, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0364, loss: 0.0364, grad_norm: 0.3323
2022-06-05 03:33:47,784 - depth - INFO - Epoch [20][300/1447]	lr: 1.880e-05, eta: 0:43:01, time: 0.371, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0362, loss: 0.0362, grad_norm: 0.3080
2022-06-05 03:34:06,185 - depth - INFO - Epoch [20][350/1447]	lr: 1.855e-05, eta: 0:42:43, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0362, loss: 0.0362, grad_norm: 0.4471
2022-06-05 03:34:24,430 - depth - INFO - Epoch [20][400/1447]	lr: 1.830e-05, eta: 0:42:24, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.3371
2022-06-05 03:34:42,578 - depth - INFO - Epoch [20][450/1447]	lr: 1.805e-05, eta: 0:42:05, time: 0.363, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0363, loss: 0.0363, grad_norm: 0.3699
2022-06-05 03:35:01,097 - depth - INFO - Epoch [20][500/1447]	lr: 1.781e-05, eta: 0:41:47, time: 0.371, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.3791
2022-06-05 03:35:19,322 - depth - INFO - Epoch [20][550/1447]	lr: 1.756e-05, eta: 0:41:28, time: 0.364, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.4381
2022-06-05 03:35:37,694 - depth - INFO - Epoch [20][600/1447]	lr: 1.732e-05, eta: 0:41:09, time: 0.367, data_time: 0.006, memory: 12862, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 0.3105
2022-06-05 03:35:55,983 - depth - INFO - Epoch [20][650/1447]	lr: 1.707e-05, eta: 0:40:51, time: 0.366, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.4308
2022-06-05 03:36:14,318 - depth - INFO - Epoch [20][700/1447]	lr: 1.683e-05, eta: 0:40:32, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.2768
2022-06-05 03:36:32,446 - depth - INFO - Epoch [20][750/1447]	lr: 1.659e-05, eta: 0:40:13, time: 0.363, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0364, loss: 0.0364, grad_norm: 0.5109
2022-06-05 03:36:50,776 - depth - INFO - Epoch [20][800/1447]	lr: 1.635e-05, eta: 0:39:55, time: 0.366, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 0.3296
2022-06-05 03:37:09,044 - depth - INFO - Epoch [20][850/1447]	lr: 1.611e-05, eta: 0:39:36, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 0.3221
2022-06-05 03:37:27,378 - depth - INFO - Epoch [20][900/1447]	lr: 1.587e-05, eta: 0:39:17, time: 0.367, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 0.3787
2022-06-05 03:37:45,956 - depth - INFO - Epoch [20][950/1447]	lr: 1.564e-05, eta: 0:38:59, time: 0.372, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.3766
2022-06-05 03:38:04,161 - depth - INFO - Epoch [20][1000/1447]	lr: 1.541e-05, eta: 0:38:40, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 0.3647
2022-06-05 03:38:22,544 - depth - INFO - Epoch [20][1050/1447]	lr: 1.517e-05, eta: 0:38:21, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.2786
2022-06-05 03:38:40,766 - depth - INFO - Epoch [20][1100/1447]	lr: 1.494e-05, eta: 0:38:03, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 0.2751
2022-06-05 03:38:59,184 - depth - INFO - Epoch [20][1150/1447]	lr: 1.471e-05, eta: 0:37:44, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.4381
2022-06-05 03:39:17,592 - depth - INFO - Epoch [20][1200/1447]	lr: 1.449e-05, eta: 0:37:25, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.5553
2022-06-05 03:39:35,840 - depth - INFO - Epoch [20][1250/1447]	lr: 1.426e-05, eta: 0:37:07, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.3473
2022-06-05 03:39:54,202 - depth - INFO - Epoch [20][1300/1447]	lr: 1.403e-05, eta: 0:36:48, time: 0.367, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.4139
2022-06-05 03:40:12,427 - depth - INFO - Epoch [20][1350/1447]	lr: 1.381e-05, eta: 0:36:29, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.3266
2022-06-05 03:40:30,869 - depth - INFO - Epoch [20][1400/1447]	lr: 1.359e-05, eta: 0:36:11, time: 0.369, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.3790
2022-06-05 03:40:48,481 - depth - INFO - Saving checkpoint at 20 epochs
2022-06-05 03:41:19,129 - depth - INFO - Summary:
2022-06-05 03:41:19,130 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9473 | 0.9935 | 0.9989 |  0.0728 | 2.5915 | 0.0304 |  0.1027  | 9.0199 | 0.2583 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 03:41:19,130 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 03:41:19,130 - depth - INFO - Epoch(val) [20][82]	a1: 0.9473, a2: 0.9935, a3: 0.9989, abs_rel: 0.0728040263056755, rmse: 2.591487169265747, log_10: 0.030418436974287033, rmse_log: 0.10265297442674637, silog: 9.0199, sq_rel: 0.25832676887512207
2022-06-05 03:41:47,529 - depth - INFO - Epoch [21][50/1447]	lr: 1.316e-05, eta: 0:35:33, time: 0.568, data_time: 0.209, memory: 12862, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.4064
2022-06-05 03:42:05,838 - depth - INFO - Epoch [21][100/1447]	lr: 1.295e-05, eta: 0:35:14, time: 0.366, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 0.4004
2022-06-05 03:42:23,985 - depth - INFO - Epoch [21][150/1447]	lr: 1.273e-05, eta: 0:34:56, time: 0.363, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.4342
2022-06-05 03:42:42,390 - depth - INFO - Epoch [21][200/1447]	lr: 1.252e-05, eta: 0:34:37, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.2961
2022-06-05 03:43:00,627 - depth - INFO - Epoch [21][250/1447]	lr: 1.230e-05, eta: 0:34:18, time: 0.365, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 0.2538
2022-06-05 03:43:18,808 - depth - INFO - Epoch [21][300/1447]	lr: 1.209e-05, eta: 0:34:00, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.4258
2022-06-05 03:43:36,900 - depth - INFO - Epoch [21][350/1447]	lr: 1.188e-05, eta: 0:33:41, time: 0.362, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0350, loss: 0.0350, grad_norm: 0.4939
2022-06-05 03:43:55,498 - depth - INFO - Epoch [21][400/1447]	lr: 1.167e-05, eta: 0:33:22, time: 0.372, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0350, loss: 0.0350, grad_norm: 0.2881
2022-06-05 03:44:13,762 - depth - INFO - Epoch [21][450/1447]	lr: 1.147e-05, eta: 0:33:04, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 0.2149
2022-06-05 03:44:32,223 - depth - INFO - Epoch [21][500/1447]	lr: 1.126e-05, eta: 0:32:45, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0349, loss: 0.0349, grad_norm: 0.3203
2022-06-05 03:44:50,752 - depth - INFO - Epoch [21][550/1447]	lr: 1.106e-05, eta: 0:32:27, time: 0.371, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0350, loss: 0.0350, grad_norm: 0.3380
2022-06-05 03:45:09,163 - depth - INFO - Epoch [21][600/1447]	lr: 1.086e-05, eta: 0:32:08, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 0.2990
2022-06-05 03:45:27,558 - depth - INFO - Epoch [21][650/1447]	lr: 1.066e-05, eta: 0:31:49, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.3101
2022-06-05 03:45:45,931 - depth - INFO - Epoch [21][700/1447]	lr: 1.046e-05, eta: 0:31:31, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0346, loss: 0.0346, grad_norm: 0.4467
2022-06-05 03:46:04,456 - depth - INFO - Epoch [21][750/1447]	lr: 1.026e-05, eta: 0:31:12, time: 0.371, data_time: 0.011, memory: 12862, decode.loss_depth: 0.0355, loss: 0.0355, grad_norm: 0.3594
2022-06-05 03:46:22,920 - depth - INFO - Epoch [21][800/1447]	lr: 1.007e-05, eta: 0:30:54, time: 0.369, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 0.2983
2022-06-05 03:46:41,254 - depth - INFO - Epoch [21][850/1447]	lr: 9.876e-06, eta: 0:30:35, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0346, loss: 0.0346, grad_norm: 0.2170
2022-06-05 03:46:59,405 - depth - INFO - Epoch [21][900/1447]	lr: 9.685e-06, eta: 0:30:16, time: 0.363, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 0.2672
2022-06-05 03:47:17,908 - depth - INFO - Epoch [21][950/1447]	lr: 9.495e-06, eta: 0:29:58, time: 0.370, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.2016
2022-06-05 03:47:36,278 - depth - INFO - Epoch [21][1000/1447]	lr: 9.307e-06, eta: 0:29:39, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 0.2325
2022-06-05 03:47:54,529 - depth - INFO - Epoch [21][1050/1447]	lr: 9.120e-06, eta: 0:29:20, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.3004
2022-06-05 03:48:12,981 - depth - INFO - Epoch [21][1100/1447]	lr: 8.935e-06, eta: 0:29:02, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0350, loss: 0.0350, grad_norm: 0.3540
2022-06-05 03:48:31,390 - depth - INFO - Epoch [21][1150/1447]	lr: 8.752e-06, eta: 0:28:43, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0345, loss: 0.0345, grad_norm: 0.2173
2022-06-05 03:48:49,572 - depth - INFO - Epoch [21][1200/1447]	lr: 8.571e-06, eta: 0:28:25, time: 0.364, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.2960
2022-06-05 03:49:07,956 - depth - INFO - Epoch [21][1250/1447]	lr: 8.391e-06, eta: 0:28:06, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.3754
2022-06-05 03:49:26,530 - depth - INFO - Epoch [21][1300/1447]	lr: 8.213e-06, eta: 0:27:47, time: 0.372, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.2850
2022-06-05 03:49:44,868 - depth - INFO - Epoch [21][1350/1447]	lr: 8.037e-06, eta: 0:27:29, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.2313
2022-06-05 03:50:03,243 - depth - INFO - Epoch [21][1400/1447]	lr: 7.863e-06, eta: 0:27:10, time: 0.367, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.4693
2022-06-05 03:50:20,735 - depth - INFO - Saving checkpoint at 21 epochs
2022-06-05 03:50:50,984 - depth - INFO - Summary:
2022-06-05 03:50:50,985 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9468 | 0.9934 | 0.9989 |  0.0724 | 2.5981 | 0.0303 |  0.1028  | 9.0721 | 0.259  |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 03:50:50,985 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 03:50:50,985 - depth - INFO - Epoch(val) [21][82]	a1: 0.9468, a2: 0.9934, a3: 0.9989, abs_rel: 0.07240501046180725, rmse: 2.5981240272521973, log_10: 0.030269687995314598, rmse_log: 0.10276582837104797, silog: 9.0721, sq_rel: 0.2589527368545532
2022-06-05 03:51:19,937 - depth - INFO - Epoch [22][50/1447]	lr: 7.530e-06, eta: 0:26:33, time: 0.579, data_time: 0.211, memory: 12862, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.3551
2022-06-05 03:51:38,361 - depth - INFO - Epoch [22][100/1447]	lr: 7.361e-06, eta: 0:26:14, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0346, loss: 0.0346, grad_norm: 0.2343
2022-06-05 03:51:56,657 - depth - INFO - Epoch [22][150/1447]	lr: 7.193e-06, eta: 0:25:56, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0343, loss: 0.0343, grad_norm: 0.2862
2022-06-05 03:52:14,804 - depth - INFO - Epoch [22][200/1447]	lr: 7.028e-06, eta: 0:25:37, time: 0.363, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.3959
2022-06-05 03:52:33,038 - depth - INFO - Epoch [22][250/1447]	lr: 6.864e-06, eta: 0:25:19, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0350, loss: 0.0350, grad_norm: 0.3567
2022-06-05 03:52:51,497 - depth - INFO - Epoch [22][300/1447]	lr: 6.702e-06, eta: 0:25:00, time: 0.369, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0346, loss: 0.0346, grad_norm: 0.2842
2022-06-05 03:53:09,909 - depth - INFO - Epoch [22][350/1447]	lr: 6.542e-06, eta: 0:24:41, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0343, loss: 0.0343, grad_norm: 0.2397
2022-06-05 03:53:28,430 - depth - INFO - Epoch [22][400/1447]	lr: 6.383e-06, eta: 0:24:23, time: 0.371, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.2105
2022-06-05 03:53:46,789 - depth - INFO - Epoch [22][450/1447]	lr: 6.227e-06, eta: 0:24:04, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.2580
2022-06-05 03:54:05,156 - depth - INFO - Epoch [22][500/1447]	lr: 6.072e-06, eta: 0:23:46, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.2424
2022-06-05 03:54:23,472 - depth - INFO - Epoch [22][550/1447]	lr: 5.919e-06, eta: 0:23:27, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0345, loss: 0.0345, grad_norm: 0.3686
2022-06-05 03:54:41,690 - depth - INFO - Epoch [22][600/1447]	lr: 5.768e-06, eta: 0:23:09, time: 0.364, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0349, loss: 0.0349, grad_norm: 0.2447
2022-06-05 03:55:00,006 - depth - INFO - Epoch [22][650/1447]	lr: 5.619e-06, eta: 0:22:50, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.2125
2022-06-05 03:55:18,318 - depth - INFO - Epoch [22][700/1447]	lr: 5.472e-06, eta: 0:22:31, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.2062
2022-06-05 03:55:36,782 - depth - INFO - Epoch [22][750/1447]	lr: 5.326e-06, eta: 0:22:13, time: 0.369, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.2884
2022-06-05 03:55:54,972 - depth - INFO - Epoch [22][800/1447]	lr: 5.182e-06, eta: 0:21:54, time: 0.364, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0346, loss: 0.0346, grad_norm: 0.2345
2022-06-05 03:56:13,193 - depth - INFO - Epoch [22][850/1447]	lr: 5.041e-06, eta: 0:21:36, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.2443
2022-06-05 03:56:31,564 - depth - INFO - Epoch [22][900/1447]	lr: 4.901e-06, eta: 0:21:17, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.1856
2022-06-05 03:56:50,136 - depth - INFO - Epoch [22][950/1447]	lr: 4.763e-06, eta: 0:20:58, time: 0.370, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.2305
2022-06-05 03:57:08,369 - depth - INFO - Epoch [22][1000/1447]	lr: 4.627e-06, eta: 0:20:40, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0350, loss: 0.0350, grad_norm: 0.2373
2022-06-05 03:57:26,622 - depth - INFO - Epoch [22][1050/1447]	lr: 4.493e-06, eta: 0:20:21, time: 0.365, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0345, loss: 0.0345, grad_norm: 0.2283
2022-06-05 03:57:44,927 - depth - INFO - Epoch [22][1100/1447]	lr: 4.360e-06, eta: 0:20:03, time: 0.366, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0345, loss: 0.0345, grad_norm: 0.3038
2022-06-05 03:58:03,417 - depth - INFO - Epoch [22][1150/1447]	lr: 4.230e-06, eta: 0:19:44, time: 0.370, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0341, loss: 0.0341, grad_norm: 0.2336
2022-06-05 03:58:21,772 - depth - INFO - Epoch [22][1200/1447]	lr: 4.101e-06, eta: 0:19:25, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0341, loss: 0.0341, grad_norm: 0.2196
2022-06-05 03:58:40,196 - depth - INFO - Epoch [22][1250/1447]	lr: 3.975e-06, eta: 0:19:07, time: 0.368, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0338, loss: 0.0338, grad_norm: 0.2097
2022-06-05 03:58:58,424 - depth - INFO - Epoch [22][1300/1447]	lr: 3.850e-06, eta: 0:18:48, time: 0.364, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.2086
2022-06-05 03:59:16,874 - depth - INFO - Epoch [22][1350/1447]	lr: 3.727e-06, eta: 0:18:30, time: 0.369, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.2170
2022-06-05 03:59:35,084 - depth - INFO - Epoch [22][1400/1447]	lr: 3.607e-06, eta: 0:18:11, time: 0.364, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.2334
2022-06-05 03:59:52,631 - depth - INFO - Saving checkpoint at 22 epochs
2022-06-05 04:00:23,303 - depth - INFO - Summary:
2022-06-05 04:00:23,303 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9461 | 0.9933 | 0.9989 |  0.0736 | 2.6057 | 0.0306 |  0.1035  | 9.0806 | 0.2642 |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 04:00:23,304 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 04:00:23,304 - depth - INFO - Epoch(val) [22][82]	a1: 0.9461, a2: 0.9933, a3: 0.9989, abs_rel: 0.07355149835348129, rmse: 2.605661630630493, log_10: 0.03064241074025631, rmse_log: 0.10354483127593994, silog: 9.0806, sq_rel: 0.2641974687576294
2022-06-05 04:00:53,063 - depth - INFO - Epoch [23][50/1447]	lr: 3.378e-06, eta: 0:17:35, time: 0.595, data_time: 0.231, memory: 12862, decode.loss_depth: 0.0343, loss: 0.0343, grad_norm: 0.2745
2022-06-05 04:01:11,452 - depth - INFO - Epoch [23][100/1447]	lr: 3.263e-06, eta: 0:17:16, time: 0.368, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.2353
2022-06-05 04:01:29,999 - depth - INFO - Epoch [23][150/1447]	lr: 3.150e-06, eta: 0:16:57, time: 0.371, data_time: 0.011, memory: 12862, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.2403
2022-06-05 04:01:48,439 - depth - INFO - Epoch [23][200/1447]	lr: 3.039e-06, eta: 0:16:39, time: 0.369, data_time: 0.011, memory: 12862, decode.loss_depth: 0.0346, loss: 0.0346, grad_norm: 0.2823
2022-06-05 04:02:06,741 - depth - INFO - Epoch [23][250/1447]	lr: 2.929e-06, eta: 0:16:20, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.3071
2022-06-05 04:02:25,199 - depth - INFO - Epoch [23][300/1447]	lr: 2.822e-06, eta: 0:16:02, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.2694
2022-06-05 04:02:43,727 - depth - INFO - Epoch [23][350/1447]	lr: 2.717e-06, eta: 0:15:43, time: 0.370, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0343, loss: 0.0343, grad_norm: 0.2126
2022-06-05 04:03:02,061 - depth - INFO - Epoch [23][400/1447]	lr: 2.614e-06, eta: 0:15:25, time: 0.367, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0337, loss: 0.0337, grad_norm: 0.2716
2022-06-05 04:03:20,457 - depth - INFO - Epoch [23][450/1447]	lr: 2.512e-06, eta: 0:15:06, time: 0.368, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0341, loss: 0.0341, grad_norm: 0.2337
2022-06-05 04:03:38,658 - depth - INFO - Epoch [23][500/1447]	lr: 2.413e-06, eta: 0:14:48, time: 0.364, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.2117
2022-06-05 04:03:57,200 - depth - INFO - Epoch [23][550/1447]	lr: 2.316e-06, eta: 0:14:29, time: 0.371, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.2069
2022-06-05 04:04:15,351 - depth - INFO - Epoch [23][600/1447]	lr: 2.220e-06, eta: 0:14:10, time: 0.363, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.1759
2022-06-05 04:04:33,506 - depth - INFO - Epoch [23][650/1447]	lr: 2.127e-06, eta: 0:13:52, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0337, loss: 0.0337, grad_norm: 0.2232
2022-06-05 04:04:51,868 - depth - INFO - Epoch [23][700/1447]	lr: 2.036e-06, eta: 0:13:33, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0337, loss: 0.0337, grad_norm: 0.2249
2022-06-05 04:05:10,185 - depth - INFO - Epoch [23][750/1447]	lr: 1.946e-06, eta: 0:13:15, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 0.2203
2022-06-05 04:05:28,407 - depth - INFO - Epoch [23][800/1447]	lr: 1.859e-06, eta: 0:12:56, time: 0.364, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.2294
2022-06-05 04:05:46,670 - depth - INFO - Epoch [23][850/1447]	lr: 1.774e-06, eta: 0:12:38, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0336, loss: 0.0336, grad_norm: 0.2207
2022-06-05 04:06:04,910 - depth - INFO - Epoch [23][900/1447]	lr: 1.690e-06, eta: 0:12:19, time: 0.365, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.1957
2022-06-05 04:06:23,231 - depth - INFO - Epoch [23][950/1447]	lr: 1.609e-06, eta: 0:12:00, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.1923
2022-06-05 04:06:41,677 - depth - INFO - Epoch [23][1000/1447]	lr: 1.530e-06, eta: 0:11:42, time: 0.369, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0343, loss: 0.0343, grad_norm: 0.2283
2022-06-05 04:06:59,906 - depth - INFO - Epoch [23][1050/1447]	lr: 1.452e-06, eta: 0:11:23, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0340, loss: 0.0340, grad_norm: 0.2299
2022-06-05 04:07:18,213 - depth - INFO - Epoch [23][1100/1447]	lr: 1.377e-06, eta: 0:11:05, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0341, loss: 0.0341, grad_norm: 0.2075
2022-06-05 04:07:36,322 - depth - INFO - Epoch [23][1150/1447]	lr: 1.304e-06, eta: 0:10:46, time: 0.362, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.1989
2022-06-05 04:07:54,631 - depth - INFO - Epoch [23][1200/1447]	lr: 1.233e-06, eta: 0:10:28, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.2162
2022-06-05 04:08:13,128 - depth - INFO - Epoch [23][1250/1447]	lr: 1.164e-06, eta: 0:10:09, time: 0.370, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0335, loss: 0.0335, grad_norm: 0.2125
2022-06-05 04:08:31,078 - depth - INFO - Epoch [23][1300/1447]	lr: 1.097e-06, eta: 0:09:51, time: 0.359, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0336, loss: 0.0336, grad_norm: 0.2018
2022-06-05 04:08:49,137 - depth - INFO - Epoch [23][1350/1447]	lr: 1.032e-06, eta: 0:09:32, time: 0.361, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0336, loss: 0.0336, grad_norm: 0.1966
2022-06-05 04:09:07,604 - depth - INFO - Epoch [23][1400/1447]	lr: 9.687e-07, eta: 0:09:13, time: 0.369, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0334, loss: 0.0334, grad_norm: 0.2151
2022-06-05 04:09:25,263 - depth - INFO - Saving checkpoint at 23 epochs
2022-06-05 04:09:55,216 - depth - INFO - Summary:
2022-06-05 04:09:55,216 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9466 | 0.9933 | 0.9988 |  0.0729 | 2.6031 | 0.0304 |  0.1031  | 9.0795 | 0.262  |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 04:09:55,217 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 04:09:55,217 - depth - INFO - Epoch(val) [23][82]	a1: 0.9466, a2: 0.9933, a3: 0.9988, abs_rel: 0.07293248176574707, rmse: 2.603050470352173, log_10: 0.03043617680668831, rmse_log: 0.10312125086784363, silog: 9.0795, sq_rel: 0.2619555592536926
2022-06-05 04:10:24,387 - depth - INFO - Epoch [24][50/1447]	lr: 8.523e-07, eta: 0:08:37, time: 0.583, data_time: 0.215, memory: 12862, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.1874
2022-06-05 04:10:43,097 - depth - INFO - Epoch [24][100/1447]	lr: 7.954e-07, eta: 0:08:19, time: 0.374, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.1651
2022-06-05 04:11:01,279 - depth - INFO - Epoch [24][150/1447]	lr: 7.404e-07, eta: 0:08:00, time: 0.364, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0337, loss: 0.0337, grad_norm: 0.1725
2022-06-05 04:11:19,410 - depth - INFO - Epoch [24][200/1447]	lr: 6.876e-07, eta: 0:07:42, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0343, loss: 0.0343, grad_norm: 0.1632
2022-06-05 04:11:37,683 - depth - INFO - Epoch [24][250/1447]	lr: 6.368e-07, eta: 0:07:23, time: 0.365, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0340, loss: 0.0340, grad_norm: 0.1820
2022-06-05 04:11:55,850 - depth - INFO - Epoch [24][300/1447]	lr: 5.881e-07, eta: 0:07:05, time: 0.363, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0341, loss: 0.0341, grad_norm: 0.2172
2022-06-05 04:12:13,918 - depth - INFO - Epoch [24][350/1447]	lr: 5.414e-07, eta: 0:06:46, time: 0.361, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.1872
2022-06-05 04:12:32,289 - depth - INFO - Epoch [24][400/1447]	lr: 4.968e-07, eta: 0:06:27, time: 0.368, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0335, loss: 0.0335, grad_norm: 0.1747
2022-06-05 04:12:50,694 - depth - INFO - Epoch [24][450/1447]	lr: 4.543e-07, eta: 0:06:09, time: 0.368, data_time: 0.011, memory: 12862, decode.loss_depth: 0.0341, loss: 0.0341, grad_norm: 0.1583
2022-06-05 04:13:08,902 - depth - INFO - Epoch [24][500/1447]	lr: 4.138e-07, eta: 0:05:50, time: 0.364, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0335, loss: 0.0335, grad_norm: 0.2144
2022-06-05 04:13:27,344 - depth - INFO - Epoch [24][550/1447]	lr: 3.754e-07, eta: 0:05:32, time: 0.369, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0340, loss: 0.0340, grad_norm: 0.1956
2022-06-05 04:13:45,633 - depth - INFO - Epoch [24][600/1447]	lr: 3.391e-07, eta: 0:05:13, time: 0.366, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0341, loss: 0.0341, grad_norm: 0.1710
2022-06-05 04:14:03,841 - depth - INFO - Epoch [24][650/1447]	lr: 3.049e-07, eta: 0:04:55, time: 0.364, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0338, loss: 0.0338, grad_norm: 0.1671
2022-06-05 04:14:22,443 - depth - INFO - Epoch [24][700/1447]	lr: 2.727e-07, eta: 0:04:36, time: 0.372, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0335, loss: 0.0335, grad_norm: 0.1812
2022-06-05 04:14:40,780 - depth - INFO - Epoch [24][750/1447]	lr: 2.426e-07, eta: 0:04:18, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0343, loss: 0.0343, grad_norm: 0.1761
2022-06-05 04:14:59,058 - depth - INFO - Epoch [24][800/1447]	lr: 2.146e-07, eta: 0:03:59, time: 0.365, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.1791
2022-06-05 04:15:17,595 - depth - INFO - Epoch [24][850/1447]	lr: 1.887e-07, eta: 0:03:41, time: 0.371, data_time: 0.011, memory: 12862, decode.loss_depth: 0.0336, loss: 0.0336, grad_norm: 0.1835
2022-06-05 04:15:35,897 - depth - INFO - Epoch [24][900/1447]	lr: 1.648e-07, eta: 0:03:22, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.2073
2022-06-05 04:15:54,222 - depth - INFO - Epoch [24][950/1447]	lr: 1.431e-07, eta: 0:03:04, time: 0.366, data_time: 0.010, memory: 12862, decode.loss_depth: 0.0343, loss: 0.0343, grad_norm: 0.1590
2022-06-05 04:16:12,590 - depth - INFO - Epoch [24][1000/1447]	lr: 1.234e-07, eta: 0:02:45, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.1679
2022-06-05 04:16:30,919 - depth - INFO - Epoch [24][1050/1447]	lr: 1.058e-07, eta: 0:02:27, time: 0.367, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0337, loss: 0.0337, grad_norm: 0.1683
2022-06-05 04:16:49,558 - depth - INFO - Epoch [24][1100/1447]	lr: 9.025e-08, eta: 0:02:08, time: 0.373, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0337, loss: 0.0337, grad_norm: 0.1797
2022-06-05 04:17:08,025 - depth - INFO - Epoch [24][1150/1447]	lr: 7.681e-08, eta: 0:01:50, time: 0.369, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.1713
2022-06-05 04:17:26,291 - depth - INFO - Epoch [24][1200/1447]	lr: 6.546e-08, eta: 0:01:31, time: 0.365, data_time: 0.009, memory: 12862, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.1782
2022-06-05 04:17:44,451 - depth - INFO - Epoch [24][1250/1447]	lr: 5.620e-08, eta: 0:01:12, time: 0.363, data_time: 0.007, memory: 12862, decode.loss_depth: 0.0336, loss: 0.0336, grad_norm: 0.1824
2022-06-05 04:18:02,753 - depth - INFO - Epoch [24][1300/1447]	lr: 4.902e-08, eta: 0:00:54, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0335, loss: 0.0335, grad_norm: 0.1773
2022-06-05 04:18:21,067 - depth - INFO - Epoch [24][1350/1447]	lr: 4.393e-08, eta: 0:00:35, time: 0.366, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0336, loss: 0.0336, grad_norm: 0.2043
2022-06-05 04:18:39,604 - depth - INFO - Epoch [24][1400/1447]	lr: 4.092e-08, eta: 0:00:17, time: 0.371, data_time: 0.008, memory: 12862, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.1455
2022-06-05 04:18:57,429 - depth - INFO - Saving checkpoint at 24 epochs
2022-06-05 04:19:28,201 - depth - INFO - Summary:
2022-06-05 04:19:28,202 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
| 0.9467 | 0.9934 | 0.9989 |  0.0729 | 2.6039 | 0.0304 |  0.1031  | 9.0763 | 0.262  |
+--------+--------+--------+---------+--------+--------+----------+--------+--------+
2022-06-05 04:19:28,202 - depth - INFO - Exp name: dpt_vit-b16_kitti.py
2022-06-05 04:19:28,202 - depth - INFO - Epoch(val) [24][82]	a1: 0.9467, a2: 0.9934, a3: 0.9989, abs_rel: 0.07290642708539963, rmse: 2.6038691997528076, log_10: 0.03042248822748661, rmse_log: 0.10309024155139923, silog: 9.0763, sq_rel: 0.26199421286582947
