2022-06-04 14:54:41,952 - depth - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.11 (default, Jul 27 2021, 14:32:16) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: Tesla V100-SXM2-32GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.1, V10.1.243
GCC: gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
PyTorch: 1.6.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.3
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.7.0
OpenCV: 4.5.5
MMCV: 1.3.13
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.1
Depth: 0.0.0+37d5bde
------------------------------------------------------------

2022-06-04 14:54:41,952 - depth - INFO - Distributed training: True
2022-06-04 14:54:42,138 - depth - INFO - Config:
model = dict(
    type='DepthEncoderDecoder',
    pretrained='nfs/checkpoints/jx_vit_base_p16_224-80ecf9dd.pth',
    backbone=dict(
        type='VisionTransformer',
        img_size=224,
        embed_dims=768,
        num_layers=12,
        num_heads=12,
        out_indices=(2, 5, 8, 11),
        final_norm=False,
        with_cls_token=True,
        output_cls_token=True),
    decode_head=dict(
        type='DPTHead',
        in_channels=(768, 768, 768, 768),
        channels=256,
        embed_dims=768,
        post_process_channels=[96, 192, 384, 768],
        readout_type='project',
        norm_cfg=None,
        min_depth=0.001,
        max_depth=10,
        loss_decode=dict(
            type='SigLoss', valid_mask=True, loss_weight=1.0, warm_up=True)),
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
dataset_type = 'NYUDataset'
data_root = 'data/nyu/'
img_norm_cfg = dict(
    mean=[127.5, 127.5, 127.5], std=[127.5, 127.5, 127.5], to_rgb=True)
crop_size = (416, 544)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='DepthLoadAnnotations'),
    dict(type='NYUCrop', depth=True),
    dict(type='RandomRotate', prob=0.5, degree=2.5),
    dict(type='RandomFlip', prob=0.5),
    dict(type='RandomCrop', crop_size=(416, 544)),
    dict(
        type='ColorAug',
        prob=0.5,
        gamma_range=[0.9, 1.1],
        brightness_range=[0.75, 1.25],
        color_range=[0.9, 1.1]),
    dict(
        type='Normalize',
        mean=[127.5, 127.5, 127.5],
        std=[127.5, 127.5, 127.5],
        to_rgb=True),
    dict(type='DefaultFormatBundle'),
    dict(
        type='Collect',
        keys=['img', 'depth_gt'],
        meta_keys=('filename', 'ori_filename', 'ori_shape', 'img_shape',
                   'pad_shape', 'scale_factor', 'flip', 'flip_direction',
                   'img_norm_cfg', 'cam_intrinsic'))
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(480, 640),
        flip=True,
        flip_direction='horizontal',
        transforms=[
            dict(type='RandomFlip', direction='horizontal'),
            dict(
                type='Normalize',
                mean=[127.5, 127.5, 127.5],
                std=[127.5, 127.5, 127.5],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(
                type='Collect',
                keys=['img'],
                meta_keys=('filename', 'ori_filename', 'ori_shape',
                           'img_shape', 'pad_shape', 'scale_factor', 'flip',
                           'flip_direction', 'img_norm_cfg', 'cam_intrinsic'))
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='NYUDataset',
        data_root='data/nyu/',
        depth_scale=1000,
        split='nyu_train.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='DepthLoadAnnotations'),
            dict(type='NYUCrop', depth=True),
            dict(type='RandomRotate', prob=0.5, degree=2.5),
            dict(type='RandomFlip', prob=0.5),
            dict(type='RandomCrop', crop_size=(416, 544)),
            dict(
                type='ColorAug',
                prob=0.5,
                gamma_range=[0.9, 1.1],
                brightness_range=[0.75, 1.25],
                color_range=[0.9, 1.1]),
            dict(
                type='Normalize',
                mean=[127.5, 127.5, 127.5],
                std=[127.5, 127.5, 127.5],
                to_rgb=True),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'depth_gt'],
                meta_keys=('filename', 'ori_filename', 'ori_shape',
                           'img_shape', 'pad_shape', 'scale_factor', 'flip',
                           'flip_direction', 'img_norm_cfg', 'cam_intrinsic'))
        ],
        garg_crop=False,
        eigen_crop=True,
        min_depth=0.001,
        max_depth=10),
    val=dict(
        type='NYUDataset',
        data_root='data/nyu/',
        depth_scale=1000,
        split='nyu_test.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(480, 640),
                flip=True,
                flip_direction='horizontal',
                transforms=[
                    dict(type='RandomFlip', direction='horizontal'),
                    dict(
                        type='Normalize',
                        mean=[127.5, 127.5, 127.5],
                        std=[127.5, 127.5, 127.5],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(
                        type='Collect',
                        keys=['img'],
                        meta_keys=('filename', 'ori_filename', 'ori_shape',
                                   'img_shape', 'pad_shape', 'scale_factor',
                                   'flip', 'flip_direction', 'img_norm_cfg',
                                   'cam_intrinsic'))
                ])
        ],
        garg_crop=False,
        eigen_crop=True,
        min_depth=0.001,
        max_depth=10),
    test=dict(
        type='NYUDataset',
        data_root='data/nyu/',
        depth_scale=1000,
        split='nyu_test.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(480, 640),
                flip=True,
                flip_direction='horizontal',
                transforms=[
                    dict(type='RandomFlip', direction='horizontal'),
                    dict(
                        type='Normalize',
                        mean=[127.5, 127.5, 127.5],
                        std=[127.5, 127.5, 127.5],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(
                        type='Collect',
                        keys=['img'],
                        meta_keys=('filename', 'ori_filename', 'ori_shape',
                                   'img_shape', 'pad_shape', 'scale_factor',
                                   'flip', 'flip_direction', 'img_norm_cfg',
                                   'cam_intrinsic'))
                ])
        ],
        garg_crop=False,
        eigen_crop=True,
        min_depth=0.001,
        max_depth=10))
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook', by_epoch=True),
        dict(type='TensorboardImageLoggerHook', by_epoch=True)
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
max_lr = 0.0001
optimizer = dict(
    type='AdamW',
    lr=0.0001,
    betas=(0.9, 0.999),
    weight_decay=0.01,
    paramwise_cfg=dict(
        custom_keys=dict(
            pos_embed=dict(decay_mult=0.0),
            cls_token=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0),
            backbone=dict(lr_mult=0.1))))
lr_config = dict(
    policy='OneCycle',
    max_lr=0.0001,
    div_factor=25,
    final_div_factor=100,
    by_epoch=False)
momentum_config = dict(policy='OneCycle')
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
runner = dict(type='EpochBasedRunner', max_epochs=24)
checkpoint_config = dict(by_epoch=True, max_keep_ckpts=2, interval=1)
evaluation = dict(by_epoch=True, interval=1, pre_eval=True)
work_dir = 'nfs/saves/dpt/nyu'
gpu_ids = range(0, 1)

2022-06-04 14:54:42,959 - depth - INFO - Use load_from_local loader
2022-06-04 14:54:43,340 - depth - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: ln1.bias, ln1.weight

Name of parameter - Initialization information

backbone.cls_token - torch.Size([1, 1, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.pos_embed - torch.Size([1, 197, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.patch_embed.projection.weight - torch.Size([768, 3, 16, 16]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.patch_embed.projection.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.0.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.1.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.2.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.3.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.4.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.5.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.6.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.7.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.8.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.9.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.10.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.ln1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.ln1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.attn.attn.in_proj_weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.attn.attn.in_proj_bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.attn.attn.out_proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.attn.attn.out_proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.ln2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.ln2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in VisionTransformer  

backbone.layers.11.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in VisionTransformer  

decode_head.conv_depth.head.0.weight - torch.Size([128, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_depth.head.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_depth.head.2.weight - torch.Size([32, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_depth.head.2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_depth.head.4.weight - torch.Size([1, 32, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.conv_depth.head.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.projects.0.conv.weight - torch.Size([96, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.projects.0.conv.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.projects.1.conv.weight - torch.Size([192, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.projects.1.conv.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.projects.2.conv.weight - torch.Size([384, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.projects.2.conv.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.projects.3.conv.weight - torch.Size([768, 768, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.projects.3.conv.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.resize_layers.0.weight - torch.Size([96, 96, 4, 4]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.resize_layers.0.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.resize_layers.1.weight - torch.Size([192, 192, 2, 2]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.resize_layers.1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.resize_layers.3.weight - torch.Size([768, 768, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.resize_layers.3.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.readout_projects.0.0.weight - torch.Size([768, 1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.readout_projects.0.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.readout_projects.1.0.weight - torch.Size([768, 1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.readout_projects.1.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.readout_projects.2.0.weight - torch.Size([768, 1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.readout_projects.2.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.readout_projects.3.0.weight - torch.Size([768, 1536]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.reassemble_blocks.readout_projects.3.0.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.convs.0.conv.weight - torch.Size([256, 96, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.convs.1.conv.weight - torch.Size([256, 192, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.convs.2.conv.weight - torch.Size([256, 384, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.convs.3.conv.weight - torch.Size([256, 768, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.0.project.conv.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.0.project.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.0.res_conv_unit2.conv1.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.0.res_conv_unit2.conv2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.1.project.conv.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.1.project.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.1.res_conv_unit1.conv1.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.1.res_conv_unit1.conv2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.1.res_conv_unit2.conv1.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.1.res_conv_unit2.conv2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.2.project.conv.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.2.project.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.2.res_conv_unit1.conv1.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.2.res_conv_unit1.conv2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.2.res_conv_unit2.conv1.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.2.res_conv_unit2.conv2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.3.project.conv.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.3.project.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.3.res_conv_unit1.conv1.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.3.res_conv_unit1.conv2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.3.res_conv_unit2.conv1.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.fusion_blocks.3.res_conv_unit2.conv2.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  

decode_head.project.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ConvModule  

decode_head.project.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DepthEncoderDecoder  
2022-06-04 14:54:43,401 - depth - INFO - DepthEncoderDecoder(
  (backbone): VisionTransformer(
    (patch_embed): PatchEmbed(
      (adap_padding): AdaptivePadding()
      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    )
    (drop_after_pos): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (1): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (2): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (3): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (4): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (5): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (6): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (7): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (8): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (9): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (10): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (11): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): DropPath()
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU()
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU()
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
    )
  )
  init_cfg={'type': 'Pretrained', 'checkpoint': 'nfs/checkpoints/jx_vit_base_p16_224-80ecf9dd.pth'}
  (decode_head): DPTHead(
    align_corners=False
    (loss_decode): SigLoss()
    (conv_depth): HeadDepth(
      (head): Sequential(
        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): Interpolate()
        (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (relu): ReLU()
    (sigmoid): Sigmoid()
    (reassemble_blocks): ReassembleBlocks(
      (projects): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(768, 96, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): ConvModule(
          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): ConvModule(
          (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
        )
        (3): ConvModule(
          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (resize_layers): ModuleList(
        (0): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(4, 4))
        (1): ConvTranspose2d(192, 192, kernel_size=(2, 2), stride=(2, 2))
        (2): Identity()
        (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
      (readout_projects): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=1536, out_features=768, bias=True)
          (1): GELU()
        )
        (1): Sequential(
          (0): Linear(in_features=1536, out_features=768, bias=True)
          (1): GELU()
        )
        (2): Sequential(
          (0): Linear(in_features=1536, out_features=768, bias=True)
          (1): GELU()
        )
        (3): Sequential(
          (0): Linear(in_features=1536, out_features=768, bias=True)
          (1): GELU()
        )
      )
    )
    (convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): ConvModule(
        (conv): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): ConvModule(
        (conv): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): ConvModule(
        (conv): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (fusion_blocks): ModuleList(
      (0): FeatureFusionBlock(
        (project): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (res_conv_unit1): None
        (res_conv_unit2): PreActResidualConvUnit(
          (conv1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
        )
      )
      (1): FeatureFusionBlock(
        (project): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (res_conv_unit1): PreActResidualConvUnit(
          (conv1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
        )
        (res_conv_unit2): PreActResidualConvUnit(
          (conv1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
        )
      )
      (2): FeatureFusionBlock(
        (project): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (res_conv_unit1): PreActResidualConvUnit(
          (conv1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
        )
        (res_conv_unit2): PreActResidualConvUnit(
          (conv1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
        )
      )
      (3): FeatureFusionBlock(
        (project): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (res_conv_unit1): PreActResidualConvUnit(
          (conv1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
        )
        (res_conv_unit2): PreActResidualConvUnit(
          (conv1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
          (conv2): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (activate): ReLU(inplace=True)
          )
        )
      )
    )
    (project): ConvModule(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (activate): ReLU(inplace=True)
    )
  )
)
2022-06-04 14:54:43,551 - depth - INFO - Loaded 24231 images. Totally 0 invalid pairs are filtered
2022-06-04 14:54:47,720 - depth - INFO - Loaded 654 images. Totally 0 invalid pairs are filtered
2022-06-04 14:54:47,720 - depth - INFO - Start running, host: root@mono3d-da-2-master-0, work_dir: /nfs/lizhenyu2/codes/Monocular-Depth-Estimation-Toolbox/nfs/saves/dpt/nyu
2022-06-04 14:54:47,721 - depth - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) OneCycleLrUpdaterHook              
(HIGH        ) OneCycleMomentumUpdaterHook        
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardImageLoggerHook         
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) OneCycleLrUpdaterHook              
(HIGH        ) OneCycleMomentumUpdaterHook        
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardImageLoggerHook         
 -------------------- 
before_train_iter:
(VERY_HIGH   ) OneCycleLrUpdaterHook              
(HIGH        ) OneCycleMomentumUpdaterHook        
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardImageLoggerHook         
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardImageLoggerHook         
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardImageLoggerHook         
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardImageLoggerHook         
 -------------------- 
after_run:
(VERY_LOW    ) TensorboardImageLoggerHook         
 -------------------- 
2022-06-04 14:54:47,721 - depth - INFO - workflow: [('train', 1)], max: 24 epochs
2022-06-04 14:55:21,178 - depth - INFO - Epoch [1][50/1514]	lr: 4.005e-06, eta: 6:43:21, time: 0.667, data_time: 0.230, memory: 12384, decode.loss_depth: 1.0964, loss: 1.0964, grad_norm: 101.9909
2022-06-04 14:55:42,796 - depth - INFO - Epoch [1][100/1514]	lr: 4.020e-06, eta: 5:31:55, time: 0.432, data_time: 0.010, memory: 12384, decode.loss_depth: 0.2079, loss: 0.2079, grad_norm: 61.2399
2022-06-04 14:56:04,763 - depth - INFO - Epoch [1][150/1514]	lr: 4.044e-06, eta: 5:09:17, time: 0.439, data_time: 0.008, memory: 12384, decode.loss_depth: 1.6072, loss: 1.6072, grad_norm: 42.0712
2022-06-04 14:56:26,131 - depth - INFO - Epoch [1][200/1514]	lr: 4.079e-06, eta: 4:56:00, time: 0.427, data_time: 0.008, memory: 12384, decode.loss_depth: 0.6938, loss: 0.6938, grad_norm: 42.5841
2022-06-04 14:56:47,638 - depth - INFO - Epoch [1][250/1514]	lr: 4.124e-06, eta: 4:48:13, time: 0.430, data_time: 0.008, memory: 12384, decode.loss_depth: 0.5281, loss: 0.5281, grad_norm: 32.2516
2022-06-04 14:57:08,964 - depth - INFO - Epoch [1][300/1514]	lr: 4.178e-06, eta: 4:42:32, time: 0.426, data_time: 0.009, memory: 12384, decode.loss_depth: 0.4650, loss: 0.4650, grad_norm: 97.5727
2022-06-04 14:57:30,858 - depth - INFO - Epoch [1][350/1514]	lr: 4.243e-06, eta: 4:39:21, time: 0.438, data_time: 0.008, memory: 12384, decode.loss_depth: 0.4046, loss: 0.4046, grad_norm: 53.3373
2022-06-04 14:57:48,050 - depth - INFO - Epoch [1][400/1514]	lr: 4.317e-06, eta: 4:29:50, time: 0.344, data_time: 0.008, memory: 12384, decode.loss_depth: 0.3508, loss: 0.3508, grad_norm: 60.8644
2022-06-04 14:58:05,159 - depth - INFO - Epoch [1][450/1514]	lr: 4.401e-06, eta: 4:22:15, time: 0.342, data_time: 0.006, memory: 12384, decode.loss_depth: 0.3229, loss: 0.3229, grad_norm: 77.4461
2022-06-04 14:58:26,766 - depth - INFO - Epoch [1][500/1514]	lr: 4.496e-06, eta: 4:21:30, time: 0.432, data_time: 0.008, memory: 12384, decode.loss_depth: 0.2966, loss: 0.2966, grad_norm: 39.4979
2022-06-04 14:58:48,540 - depth - INFO - Epoch [1][550/1514]	lr: 4.600e-06, eta: 4:21:01, time: 0.435, data_time: 0.008, memory: 12384, decode.loss_depth: 0.2747, loss: 0.2747, grad_norm: 35.4561
2022-06-04 14:59:14,872 - depth - INFO - Epoch [1][600/1514]	lr: 4.714e-06, eta: 4:25:04, time: 0.527, data_time: 0.009, memory: 12384, decode.loss_depth: 0.2563, loss: 0.2563, grad_norm: 58.0381
2022-06-04 14:59:36,736 - depth - INFO - Epoch [1][650/1514]	lr: 4.837e-06, eta: 4:24:20, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.2462, loss: 0.2462, grad_norm: 32.2015
2022-06-04 14:59:53,713 - depth - INFO - Epoch [1][700/1514]	lr: 4.971e-06, eta: 4:19:31, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.2413, loss: 0.2413, grad_norm: 57.4245
2022-06-04 15:00:15,734 - depth - INFO - Epoch [1][750/1514]	lr: 5.114e-06, eta: 4:19:17, time: 0.440, data_time: 0.009, memory: 12384, decode.loss_depth: 0.2267, loss: 0.2267, grad_norm: 33.9677
2022-06-04 15:00:37,133 - depth - INFO - Epoch [1][800/1514]	lr: 5.267e-06, eta: 4:18:35, time: 0.428, data_time: 0.009, memory: 12384, decode.loss_depth: 0.2272, loss: 0.2272, grad_norm: 51.5246
2022-06-04 15:00:54,233 - depth - INFO - Epoch [1][850/1514]	lr: 5.430e-06, eta: 4:14:56, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.2353, loss: 0.2353, grad_norm: 98.6854
2022-06-04 15:01:11,372 - depth - INFO - Epoch [1][900/1514]	lr: 5.602e-06, eta: 4:11:40, time: 0.343, data_time: 0.008, memory: 12384, decode.loss_depth: 0.2205, loss: 0.2205, grad_norm: 48.0578
2022-06-04 15:01:32,959 - depth - INFO - Epoch [1][950/1514]	lr: 5.784e-06, eta: 4:11:29, time: 0.432, data_time: 0.008, memory: 12384, decode.loss_depth: 0.2097, loss: 0.2097, grad_norm: 26.1753
2022-06-04 15:01:50,161 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 15:01:50,161 - depth - INFO - Epoch [1][1000/1514]	lr: 5.976e-06, eta: 4:08:43, time: 0.344, data_time: 0.007, memory: 12384, decode.loss_depth: 0.2079, loss: 0.2079, grad_norm: 47.4217
2022-06-04 15:02:11,717 - depth - INFO - Epoch [1][1050/1514]	lr: 6.177e-06, eta: 4:08:36, time: 0.431, data_time: 0.007, memory: 12384, decode.loss_depth: 0.2018, loss: 0.2018, grad_norm: 27.1532
2022-06-04 15:02:32,922 - depth - INFO - Epoch [1][1100/1514]	lr: 6.388e-06, eta: 4:08:17, time: 0.424, data_time: 0.008, memory: 12384, decode.loss_depth: 0.2005, loss: 0.2005, grad_norm: 19.5304
2022-06-04 15:02:54,655 - depth - INFO - Epoch [1][1150/1514]	lr: 6.608e-06, eta: 4:08:14, time: 0.435, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1958, loss: 0.1958, grad_norm: 18.8335
2022-06-04 15:03:16,005 - depth - INFO - Epoch [1][1200/1514]	lr: 6.838e-06, eta: 4:07:58, time: 0.427, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1871, loss: 0.1871, grad_norm: 26.7138
2022-06-04 15:03:37,792 - depth - INFO - Epoch [1][1250/1514]	lr: 7.077e-06, eta: 4:07:55, time: 0.436, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1927, loss: 0.1927, grad_norm: 39.6453
2022-06-04 15:03:59,140 - depth - INFO - Epoch [1][1300/1514]	lr: 7.325e-06, eta: 4:07:38, time: 0.427, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1859, loss: 0.1859, grad_norm: 20.8907
2022-06-04 15:04:20,976 - depth - INFO - Epoch [1][1350/1514]	lr: 7.583e-06, eta: 4:07:32, time: 0.437, data_time: 0.010, memory: 12384, decode.loss_depth: 0.1842, loss: 0.1842, grad_norm: 25.1787
2022-06-04 15:04:42,820 - depth - INFO - Epoch [1][1400/1514]	lr: 7.850e-06, eta: 4:07:27, time: 0.437, data_time: 0.010, memory: 12384, decode.loss_depth: 0.1838, loss: 0.1838, grad_norm: 31.6669
2022-06-04 15:05:03,991 - depth - INFO - Epoch [1][1450/1514]	lr: 8.126e-06, eta: 4:07:04, time: 0.423, data_time: 0.011, memory: 12384, decode.loss_depth: 0.1769, loss: 0.1769, grad_norm: 23.0269
2022-06-04 15:05:21,214 - depth - INFO - Epoch [1][1500/1514]	lr: 8.411e-06, eta: 4:05:09, time: 0.344, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1800, loss: 0.1800, grad_norm: 46.7611
2022-06-04 15:05:31,101 - depth - INFO - Saving checkpoint at 1 epochs
2022-06-04 15:05:57,497 - depth - INFO - Summary:
2022-06-04 15:05:57,498 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.7415 | 0.9525 | 0.9909 |  0.169  | 0.5569 | 0.0714 |  0.2044  | 16.6527 | 0.1261 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 15:05:57,499 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 15:05:57,499 - depth - INFO - Epoch(val) [1][82]	a1: 0.7415, a2: 0.9525, a3: 0.9909, abs_rel: 0.16902956366539001, rmse: 0.5568534135818481, log_10: 0.07143805921077728, rmse_log: 0.20435172319412231, silog: 16.6527, sq_rel: 0.12614895403385162
2022-06-04 15:06:28,508 - depth - INFO - Epoch [2][50/1514]	lr: 8.789e-06, eta: 4:06:10, time: 0.620, data_time: 0.225, memory: 12384, decode.loss_depth: 0.1755, loss: 0.1755, grad_norm: 23.2686
2022-06-04 15:06:45,661 - depth - INFO - Epoch [2][100/1514]	lr: 9.095e-06, eta: 4:04:21, time: 0.343, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1697, loss: 0.1697, grad_norm: 17.6412
2022-06-04 15:07:07,054 - depth - INFO - Epoch [2][150/1514]	lr: 9.409e-06, eta: 4:04:05, time: 0.428, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1723, loss: 0.1723, grad_norm: 23.4212
2022-06-04 15:07:28,767 - depth - INFO - Epoch [2][200/1514]	lr: 9.733e-06, eta: 4:03:56, time: 0.434, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1728, loss: 0.1728, grad_norm: 24.5486
2022-06-04 15:07:50,167 - depth - INFO - Epoch [2][250/1514]	lr: 1.006e-05, eta: 4:03:41, time: 0.428, data_time: 0.010, memory: 12384, decode.loss_depth: 0.1672, loss: 0.1672, grad_norm: 24.3189
2022-06-04 15:08:11,753 - depth - INFO - Epoch [2][300/1514]	lr: 1.041e-05, eta: 4:03:28, time: 0.432, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1642, loss: 0.1642, grad_norm: 22.0049
2022-06-04 15:08:33,684 - depth - INFO - Epoch [2][350/1514]	lr: 1.076e-05, eta: 4:03:21, time: 0.439, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1628, loss: 0.1628, grad_norm: 18.4667
2022-06-04 15:08:55,003 - depth - INFO - Epoch [2][400/1514]	lr: 1.111e-05, eta: 4:03:02, time: 0.426, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1561, loss: 0.1561, grad_norm: 13.6760
2022-06-04 15:09:11,986 - depth - INFO - Epoch [2][450/1514]	lr: 1.148e-05, eta: 4:01:27, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1573, loss: 0.1573, grad_norm: 20.6737
2022-06-04 15:09:28,887 - depth - INFO - Epoch [2][500/1514]	lr: 1.185e-05, eta: 3:59:55, time: 0.338, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1584, loss: 0.1584, grad_norm: 18.5705
2022-06-04 15:09:45,882 - depth - INFO - Epoch [2][550/1514]	lr: 1.224e-05, eta: 3:58:28, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1609, loss: 0.1609, grad_norm: 19.2801
2022-06-04 15:10:07,729 - depth - INFO - Epoch [2][600/1514]	lr: 1.263e-05, eta: 3:58:23, time: 0.437, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1560, loss: 0.1560, grad_norm: 19.1364
2022-06-04 15:10:29,007 - depth - INFO - Epoch [2][650/1514]	lr: 1.303e-05, eta: 3:58:08, time: 0.425, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1565, loss: 0.1565, grad_norm: 17.9025
2022-06-04 15:10:51,058 - depth - INFO - Epoch [2][700/1514]	lr: 1.344e-05, eta: 3:58:05, time: 0.441, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1502, loss: 0.1502, grad_norm: 18.6731
2022-06-04 15:11:12,764 - depth - INFO - Epoch [2][750/1514]	lr: 1.385e-05, eta: 3:57:56, time: 0.434, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1491, loss: 0.1491, grad_norm: 14.0336
2022-06-04 15:11:29,858 - depth - INFO - Epoch [2][800/1514]	lr: 1.428e-05, eta: 3:56:38, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1510, loss: 0.1510, grad_norm: 10.4395
2022-06-04 15:11:51,899 - depth - INFO - Epoch [2][850/1514]	lr: 1.471e-05, eta: 3:56:34, time: 0.441, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1448, loss: 0.1448, grad_norm: 10.4336
2022-06-04 15:12:08,863 - depth - INFO - Epoch [2][900/1514]	lr: 1.515e-05, eta: 3:55:18, time: 0.339, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1459, loss: 0.1459, grad_norm: 13.2465
2022-06-04 15:12:25,980 - depth - INFO - Epoch [2][950/1514]	lr: 1.560e-05, eta: 3:54:06, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1443, loss: 0.1443, grad_norm: 12.9846
2022-06-04 15:12:43,016 - depth - INFO - Epoch [2][1000/1514]	lr: 1.605e-05, eta: 3:52:56, time: 0.341, data_time: 0.006, memory: 12384, decode.loss_depth: 0.1430, loss: 0.1430, grad_norm: 14.4325
2022-06-04 15:13:04,951 - depth - INFO - Epoch [2][1050/1514]	lr: 1.651e-05, eta: 3:52:52, time: 0.439, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1441, loss: 0.1441, grad_norm: 12.9159
2022-06-04 15:13:26,802 - depth - INFO - Epoch [2][1100/1514]	lr: 1.698e-05, eta: 3:52:46, time: 0.437, data_time: 0.010, memory: 12384, decode.loss_depth: 0.1441, loss: 0.1441, grad_norm: 11.9193
2022-06-04 15:13:43,968 - depth - INFO - Epoch [2][1150/1514]	lr: 1.746e-05, eta: 3:51:41, time: 0.343, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1435, loss: 0.1435, grad_norm: 10.6926
2022-06-04 15:14:00,962 - depth - INFO - Epoch [2][1200/1514]	lr: 1.794e-05, eta: 3:50:35, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1354, loss: 0.1354, grad_norm: 10.4675
2022-06-04 15:14:22,629 - depth - INFO - Epoch [2][1250/1514]	lr: 1.843e-05, eta: 3:50:27, time: 0.433, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1402, loss: 0.1402, grad_norm: 10.1431
2022-06-04 15:14:43,961 - depth - INFO - Epoch [2][1300/1514]	lr: 1.893e-05, eta: 3:50:16, time: 0.427, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1357, loss: 0.1357, grad_norm: 8.9889
2022-06-04 15:15:05,943 - depth - INFO - Epoch [2][1350/1514]	lr: 1.944e-05, eta: 3:50:11, time: 0.440, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1356, loss: 0.1356, grad_norm: 11.8797
2022-06-04 15:15:27,843 - depth - INFO - Epoch [2][1400/1514]	lr: 1.995e-05, eta: 3:50:05, time: 0.438, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1360, loss: 0.1360, grad_norm: 11.3898
2022-06-04 15:15:49,770 - depth - INFO - Epoch [2][1450/1514]	lr: 2.047e-05, eta: 3:49:59, time: 0.438, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1341, loss: 0.1341, grad_norm: 7.7695
2022-06-04 15:16:11,033 - depth - INFO - Epoch [2][1500/1514]	lr: 2.099e-05, eta: 3:49:45, time: 0.425, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1319, loss: 0.1319, grad_norm: 12.4490
2022-06-04 15:16:21,104 - depth - INFO - Saving checkpoint at 2 epochs
2022-06-04 15:16:50,097 - depth - INFO - Summary:
2022-06-04 15:16:50,098 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8136 | 0.9681 | 0.9941 |  0.1474 | 0.4752 | 0.0603 |  0.1739  | 13.8951 | 0.0955 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 15:16:50,098 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 15:16:50,099 - depth - INFO - Epoch(val) [2][82]	a1: 0.8136, a2: 0.9681, a3: 0.9941, abs_rel: 0.14737682044506073, rmse: 0.4752276539802551, log_10: 0.06030949950218201, rmse_log: 0.17385268211364746, silog: 13.8951, sq_rel: 0.09547882527112961
2022-06-04 15:17:20,831 - depth - INFO - Epoch [3][50/1514]	lr: 2.167e-05, eta: 3:50:04, time: 0.614, data_time: 0.231, memory: 12384, decode.loss_depth: 0.1290, loss: 0.1290, grad_norm: 7.9944
2022-06-04 15:17:37,982 - depth - INFO - Epoch [3][100/1514]	lr: 2.221e-05, eta: 3:49:05, time: 0.343, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1264, loss: 0.1264, grad_norm: 7.0929
2022-06-04 15:17:59,896 - depth - INFO - Epoch [3][150/1514]	lr: 2.276e-05, eta: 3:48:57, time: 0.438, data_time: 0.010, memory: 12384, decode.loss_depth: 0.1299, loss: 0.1299, grad_norm: 11.6448
2022-06-04 15:18:21,612 - depth - INFO - Epoch [3][200/1514]	lr: 2.331e-05, eta: 3:48:47, time: 0.434, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1300, loss: 0.1300, grad_norm: 9.2560
2022-06-04 15:18:42,952 - depth - INFO - Epoch [3][250/1514]	lr: 2.387e-05, eta: 3:48:32, time: 0.427, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1257, loss: 0.1257, grad_norm: 7.5371
2022-06-04 15:19:04,728 - depth - INFO - Epoch [3][300/1514]	lr: 2.443e-05, eta: 3:48:21, time: 0.435, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1225, loss: 0.1225, grad_norm: 7.9761
2022-06-04 15:19:25,946 - depth - INFO - Epoch [3][350/1514]	lr: 2.500e-05, eta: 3:48:05, time: 0.424, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1247, loss: 0.1247, grad_norm: 6.2582
2022-06-04 15:19:43,021 - depth - INFO - Epoch [3][400/1514]	lr: 2.557e-05, eta: 3:47:09, time: 0.341, data_time: 0.006, memory: 12384, decode.loss_depth: 0.1198, loss: 0.1198, grad_norm: 7.5232
2022-06-04 15:19:59,930 - depth - INFO - Epoch [3][450/1514]	lr: 2.615e-05, eta: 3:46:12, time: 0.338, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1242, loss: 0.1242, grad_norm: 9.1793
2022-06-04 15:20:16,919 - depth - INFO - Epoch [3][500/1514]	lr: 2.674e-05, eta: 3:45:18, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1243, loss: 0.1243, grad_norm: 8.4114
2022-06-04 15:20:38,611 - depth - INFO - Epoch [3][550/1514]	lr: 2.733e-05, eta: 3:45:07, time: 0.434, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1251, loss: 0.1251, grad_norm: 6.8278
2022-06-04 15:20:59,786 - depth - INFO - Epoch [3][600/1514]	lr: 2.792e-05, eta: 3:44:51, time: 0.423, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1211, loss: 0.1211, grad_norm: 7.0932
2022-06-04 15:21:16,906 - depth - INFO - Epoch [3][650/1514]	lr: 2.853e-05, eta: 3:44:00, time: 0.342, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1217, loss: 0.1217, grad_norm: 6.7691
2022-06-04 15:21:33,885 - depth - INFO - Epoch [3][700/1514]	lr: 2.913e-05, eta: 3:43:08, time: 0.339, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1165, loss: 0.1165, grad_norm: 5.6251
2022-06-04 15:22:00,106 - depth - INFO - Epoch [3][750/1514]	lr: 2.974e-05, eta: 3:43:36, time: 0.525, data_time: 0.010, memory: 12384, decode.loss_depth: 0.1168, loss: 0.1168, grad_norm: 7.5704
2022-06-04 15:22:21,582 - depth - INFO - Epoch [3][800/1514]	lr: 3.036e-05, eta: 3:43:23, time: 0.430, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1186, loss: 0.1186, grad_norm: 5.9502
2022-06-04 15:22:42,878 - depth - INFO - Epoch [3][850/1514]	lr: 3.098e-05, eta: 3:43:08, time: 0.426, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1171, loss: 0.1171, grad_norm: 6.1128
2022-06-04 15:23:04,882 - depth - INFO - Epoch [3][900/1514]	lr: 3.160e-05, eta: 3:42:59, time: 0.440, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1167, loss: 0.1167, grad_norm: 7.5090
2022-06-04 15:23:22,087 - depth - INFO - Epoch [3][950/1514]	lr: 3.223e-05, eta: 3:42:10, time: 0.344, data_time: 0.010, memory: 12384, decode.loss_depth: 0.1152, loss: 0.1152, grad_norm: 6.0195
2022-06-04 15:23:43,846 - depth - INFO - Epoch [3][1000/1514]	lr: 3.286e-05, eta: 3:41:59, time: 0.435, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1144, loss: 0.1144, grad_norm: 4.7378
2022-06-04 15:24:05,690 - depth - INFO - Epoch [3][1050/1514]	lr: 3.350e-05, eta: 3:41:48, time: 0.437, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1122, loss: 0.1122, grad_norm: 4.1950
2022-06-04 15:24:26,900 - depth - INFO - Epoch [3][1100/1514]	lr: 3.414e-05, eta: 3:41:32, time: 0.424, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1138, loss: 0.1138, grad_norm: 4.2498
2022-06-04 15:24:43,790 - depth - INFO - Epoch [3][1150/1514]	lr: 3.478e-05, eta: 3:40:43, time: 0.338, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1169, loss: 0.1169, grad_norm: 7.3201
2022-06-04 15:25:00,882 - depth - INFO - Epoch [3][1200/1514]	lr: 3.543e-05, eta: 3:39:55, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1140, loss: 0.1140, grad_norm: 6.1625
2022-06-04 15:25:18,128 - depth - INFO - Epoch [3][1250/1514]	lr: 3.608e-05, eta: 3:39:10, time: 0.345, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1140, loss: 0.1140, grad_norm: 4.7398
2022-06-04 15:25:39,826 - depth - INFO - Epoch [3][1300/1514]	lr: 3.673e-05, eta: 3:38:58, time: 0.434, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1116, loss: 0.1116, grad_norm: 4.0530
2022-06-04 15:25:57,025 - depth - INFO - Epoch [3][1350/1514]	lr: 3.739e-05, eta: 3:38:13, time: 0.344, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1099, loss: 0.1099, grad_norm: 5.3564
2022-06-04 15:26:18,786 - depth - INFO - Epoch [3][1400/1514]	lr: 3.805e-05, eta: 3:38:02, time: 0.435, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1110, loss: 0.1110, grad_norm: 4.5688
2022-06-04 15:26:39,951 - depth - INFO - Epoch [3][1450/1514]	lr: 3.872e-05, eta: 3:37:46, time: 0.423, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1082, loss: 0.1082, grad_norm: 4.5896
2022-06-04 15:27:01,893 - depth - INFO - Epoch [3][1500/1514]	lr: 3.938e-05, eta: 3:37:36, time: 0.439, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1073, loss: 0.1073, grad_norm: 5.1239
2022-06-04 15:27:12,073 - depth - INFO - Saving checkpoint at 3 epochs
2022-06-04 15:27:41,470 - depth - INFO - Summary:
2022-06-04 15:27:41,471 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.7641 | 0.9638 | 0.9922 |  0.1797 | 0.5521 | 0.0691 |  0.1909  | 12.7597 | 0.1359 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 15:27:41,472 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 15:27:41,472 - depth - INFO - Epoch(val) [3][82]	a1: 0.7641, a2: 0.9638, a3: 0.9922, abs_rel: 0.17972536385059357, rmse: 0.5520808100700378, log_10: 0.06912891566753387, rmse_log: 0.1909080296754837, silog: 12.7597, sq_rel: 0.13593284785747528
2022-06-04 15:28:13,643 - depth - INFO - Epoch [4][50/1514]	lr: 4.024e-05, eta: 3:37:50, time: 0.643, data_time: 0.238, memory: 12384, decode.loss_depth: 0.1058, loss: 0.1058, grad_norm: 3.5594
2022-06-04 15:28:30,731 - depth - INFO - Epoch [4][100/1514]	lr: 4.091e-05, eta: 3:37:06, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1083, loss: 0.1083, grad_norm: 5.6209
2022-06-04 15:28:47,810 - depth - INFO - Epoch [4][150/1514]	lr: 4.158e-05, eta: 3:36:22, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1057, loss: 0.1057, grad_norm: 4.6696
2022-06-04 15:29:09,916 - depth - INFO - Epoch [4][200/1514]	lr: 4.226e-05, eta: 3:36:12, time: 0.442, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1073, loss: 0.1073, grad_norm: 4.5728
2022-06-04 15:29:31,718 - depth - INFO - Epoch [4][250/1514]	lr: 4.294e-05, eta: 3:36:00, time: 0.436, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1060, loss: 0.1060, grad_norm: 4.0327
2022-06-04 15:29:52,941 - depth - INFO - Epoch [4][300/1514]	lr: 4.362e-05, eta: 3:35:44, time: 0.424, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1043, loss: 0.1043, grad_norm: 4.4611
2022-06-04 15:30:10,055 - depth - INFO - Epoch [4][350/1514]	lr: 4.430e-05, eta: 3:35:01, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1020, loss: 0.1020, grad_norm: 3.9239
2022-06-04 15:30:27,305 - depth - INFO - Epoch [4][400/1514]	lr: 4.498e-05, eta: 3:34:20, time: 0.345, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1025, loss: 0.1025, grad_norm: 5.5189
2022-06-04 15:30:44,426 - depth - INFO - Epoch [4][450/1514]	lr: 4.567e-05, eta: 3:33:38, time: 0.343, data_time: 0.008, memory: 12384, decode.loss_depth: 0.1006, loss: 0.1006, grad_norm: 3.7870
2022-06-04 15:31:06,043 - depth - INFO - Epoch [4][500/1514]	lr: 4.636e-05, eta: 3:33:25, time: 0.432, data_time: 0.009, memory: 12384, decode.loss_depth: 0.1047, loss: 0.1047, grad_norm: 4.8609
2022-06-04 15:31:23,150 - depth - INFO - Epoch [4][550/1514]	lr: 4.704e-05, eta: 3:32:44, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.1034, loss: 0.1034, grad_norm: 4.7571
2022-06-04 15:31:40,338 - depth - INFO - Epoch [4][600/1514]	lr: 4.773e-05, eta: 3:32:04, time: 0.344, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0996, loss: 0.0996, grad_norm: 2.7374
2022-06-04 15:32:01,744 - depth - INFO - Epoch [4][650/1514]	lr: 4.842e-05, eta: 3:31:50, time: 0.428, data_time: 0.010, memory: 12384, decode.loss_depth: 0.1009, loss: 0.1009, grad_norm: 3.2128
2022-06-04 15:32:18,689 - depth - INFO - Epoch [4][700/1514]	lr: 4.911e-05, eta: 3:31:09, time: 0.339, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0985, loss: 0.0985, grad_norm: 3.4912
2022-06-04 15:32:40,005 - depth - INFO - Epoch [4][750/1514]	lr: 4.980e-05, eta: 3:30:54, time: 0.426, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0974, loss: 0.0974, grad_norm: 4.4734
2022-06-04 15:33:01,839 - depth - INFO - Epoch [4][800/1514]	lr: 5.049e-05, eta: 3:30:42, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0986, loss: 0.0986, grad_norm: 3.4621
2022-06-04 15:33:18,959 - depth - INFO - Epoch [4][850/1514]	lr: 5.119e-05, eta: 3:30:03, time: 0.343, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0951, loss: 0.0951, grad_norm: 3.2442
2022-06-04 15:33:35,897 - depth - INFO - Epoch [4][900/1514]	lr: 5.188e-05, eta: 3:29:23, time: 0.339, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0955, loss: 0.0955, grad_norm: 2.7711
2022-06-04 15:33:53,073 - depth - INFO - Epoch [4][950/1514]	lr: 5.257e-05, eta: 3:28:45, time: 0.343, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0985, loss: 0.0985, grad_norm: 4.1313
2022-06-04 15:34:10,080 - depth - INFO - Epoch [4][1000/1514]	lr: 5.326e-05, eta: 3:28:06, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0977, loss: 0.0977, grad_norm: 4.0017
2022-06-04 15:34:31,664 - depth - INFO - Epoch [4][1050/1514]	lr: 5.395e-05, eta: 3:27:53, time: 0.432, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0961, loss: 0.0961, grad_norm: 3.7370
2022-06-04 15:34:52,870 - depth - INFO - Epoch [4][1100/1514]	lr: 5.464e-05, eta: 3:27:38, time: 0.424, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0982, loss: 0.0982, grad_norm: 4.8952
2022-06-04 15:35:09,823 - depth - INFO - Epoch [4][1150/1514]	lr: 5.533e-05, eta: 3:27:00, time: 0.339, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0981, loss: 0.0981, grad_norm: 3.2632
2022-06-04 15:35:31,726 - depth - INFO - Epoch [4][1200/1514]	lr: 5.602e-05, eta: 3:26:48, time: 0.438, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0966, loss: 0.0966, grad_norm: 3.9268
2022-06-04 15:35:53,048 - depth - INFO - Epoch [4][1250/1514]	lr: 5.671e-05, eta: 3:26:33, time: 0.426, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0957, loss: 0.0957, grad_norm: 3.1531
2022-06-04 15:36:14,686 - depth - INFO - Epoch [4][1300/1514]	lr: 5.740e-05, eta: 3:26:20, time: 0.433, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0955, loss: 0.0955, grad_norm: 2.8914
2022-06-04 15:36:31,661 - depth - INFO - Epoch [4][1350/1514]	lr: 5.809e-05, eta: 3:25:42, time: 0.339, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0948, loss: 0.0948, grad_norm: 3.5846
2022-06-04 15:36:53,023 - depth - INFO - Epoch [4][1400/1514]	lr: 5.877e-05, eta: 3:25:28, time: 0.427, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0939, loss: 0.0939, grad_norm: 2.5668
2022-06-04 15:37:14,781 - depth - INFO - Epoch [4][1450/1514]	lr: 5.946e-05, eta: 3:25:15, time: 0.435, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0950, loss: 0.0950, grad_norm: 3.0757
2022-06-04 15:37:36,590 - depth - INFO - Epoch [4][1500/1514]	lr: 6.014e-05, eta: 3:25:02, time: 0.436, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0916, loss: 0.0916, grad_norm: 2.4921
2022-06-04 15:37:46,167 - depth - INFO - Saving checkpoint at 4 epochs
2022-06-04 15:38:12,277 - depth - INFO - Summary:
2022-06-04 15:38:12,279 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
| 0.8114 | 0.9713 | 0.994 |  0.1549 | 0.4923 | 0.0605 |  0.1721  | 12.3005 | 0.1088 |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
2022-06-04 15:38:12,279 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 15:38:12,279 - depth - INFO - Epoch(val) [4][82]	a1: 0.8114, a2: 0.9713, a3: 0.9940, abs_rel: 0.15492045879364014, rmse: 0.4922657608985901, log_10: 0.06050994247198105, rmse_log: 0.17214734852313995, silog: 12.3005, sq_rel: 0.10879998654127121
2022-06-04 15:38:43,551 - depth - INFO - Epoch [5][50/1514]	lr: 6.101e-05, eta: 3:25:02, time: 0.625, data_time: 0.227, memory: 12384, decode.loss_depth: 0.0927, loss: 0.0927, grad_norm: 4.2246
2022-06-04 15:39:04,994 - depth - INFO - Epoch [5][100/1514]	lr: 6.169e-05, eta: 3:24:47, time: 0.429, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0931, loss: 0.0931, grad_norm: 3.5373
2022-06-04 15:39:22,073 - depth - INFO - Epoch [5][150/1514]	lr: 6.236e-05, eta: 3:24:11, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0946, loss: 0.0946, grad_norm: 3.3570
2022-06-04 15:39:43,667 - depth - INFO - Epoch [5][200/1514]	lr: 6.304e-05, eta: 3:23:57, time: 0.432, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0917, loss: 0.0917, grad_norm: 2.7039
2022-06-04 15:40:04,821 - depth - INFO - Epoch [5][250/1514]	lr: 6.371e-05, eta: 3:23:40, time: 0.423, data_time: 0.011, memory: 12384, decode.loss_depth: 0.0899, loss: 0.0899, grad_norm: 2.5393
2022-06-04 15:40:26,699 - depth - INFO - Epoch [5][300/1514]	lr: 6.438e-05, eta: 3:23:27, time: 0.437, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0901, loss: 0.0901, grad_norm: 2.6708
2022-06-04 15:40:47,894 - depth - INFO - Epoch [5][350/1514]	lr: 6.505e-05, eta: 3:23:11, time: 0.424, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0889, loss: 0.0889, grad_norm: 3.1110
2022-06-04 15:41:04,872 - depth - INFO - Epoch [5][400/1514]	lr: 6.571e-05, eta: 3:22:35, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0865, loss: 0.0865, grad_norm: 3.0175
2022-06-04 15:41:21,861 - depth - INFO - Epoch [5][450/1514]	lr: 6.637e-05, eta: 3:21:59, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0898, loss: 0.0898, grad_norm: 2.1763
2022-06-04 15:41:38,980 - depth - INFO - Epoch [5][500/1514]	lr: 6.703e-05, eta: 3:21:24, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0895, loss: 0.0895, grad_norm: 3.1460
2022-06-04 15:42:00,634 - depth - INFO - Epoch [5][550/1514]	lr: 6.769e-05, eta: 3:21:10, time: 0.433, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0895, loss: 0.0895, grad_norm: 2.1118
2022-06-04 15:42:21,992 - depth - INFO - Epoch [5][600/1514]	lr: 6.834e-05, eta: 3:20:54, time: 0.427, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0901, loss: 0.0901, grad_norm: 2.0533
2022-06-04 15:42:43,583 - depth - INFO - Epoch [5][650/1514]	lr: 6.899e-05, eta: 3:20:40, time: 0.432, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0923, loss: 0.0923, grad_norm: 2.7822
2022-06-04 15:43:04,777 - depth - INFO - Epoch [5][700/1514]	lr: 6.963e-05, eta: 3:20:23, time: 0.424, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0898, loss: 0.0898, grad_norm: 2.9013
2022-06-04 15:43:21,660 - depth - INFO - Epoch [5][750/1514]	lr: 7.027e-05, eta: 3:19:48, time: 0.338, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0871, loss: 0.0871, grad_norm: 3.0210
2022-06-04 15:43:42,945 - depth - INFO - Epoch [5][800/1514]	lr: 7.091e-05, eta: 3:19:32, time: 0.426, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0890, loss: 0.0890, grad_norm: 3.4093
2022-06-04 15:44:04,771 - depth - INFO - Epoch [5][850/1514]	lr: 7.155e-05, eta: 3:19:18, time: 0.436, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0857, loss: 0.0857, grad_norm: 2.8441
2022-06-04 15:44:21,814 - depth - INFO - Epoch [5][900/1514]	lr: 7.218e-05, eta: 3:18:44, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0903, loss: 0.0903, grad_norm: 2.7854
2022-06-04 15:44:43,718 - depth - INFO - Epoch [5][950/1514]	lr: 7.280e-05, eta: 3:18:30, time: 0.438, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0887, loss: 0.0887, grad_norm: 1.9759
2022-06-04 15:45:00,959 - depth - INFO - Epoch [5][1000/1514]	lr: 7.342e-05, eta: 3:17:57, time: 0.345, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0867, loss: 0.0867, grad_norm: 2.1636
2022-06-04 15:45:17,888 - depth - INFO - Epoch [5][1050/1514]	lr: 7.404e-05, eta: 3:17:23, time: 0.339, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0852, loss: 0.0852, grad_norm: 2.8058
2022-06-04 15:45:35,019 - depth - INFO - Epoch [5][1100/1514]	lr: 7.465e-05, eta: 3:16:50, time: 0.342, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0861, loss: 0.0861, grad_norm: 3.1864
2022-06-04 15:45:52,183 - depth - INFO - Epoch [5][1150/1514]	lr: 7.526e-05, eta: 3:16:18, time: 0.343, data_time: 0.011, memory: 12384, decode.loss_depth: 0.0881, loss: 0.0881, grad_norm: 2.2922
2022-06-04 15:46:13,821 - depth - INFO - Epoch [5][1200/1514]	lr: 7.586e-05, eta: 3:16:03, time: 0.433, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0847, loss: 0.0847, grad_norm: 1.7309
2022-06-04 15:46:30,793 - depth - INFO - Epoch [5][1250/1514]	lr: 7.646e-05, eta: 3:15:30, time: 0.340, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0852, loss: 0.0852, grad_norm: 1.8488
2022-06-04 15:46:47,847 - depth - INFO - Epoch [5][1300/1514]	lr: 7.705e-05, eta: 3:14:57, time: 0.341, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0864, loss: 0.0864, grad_norm: 3.3517
2022-06-04 15:47:04,880 - depth - INFO - Epoch [5][1350/1514]	lr: 7.764e-05, eta: 3:14:25, time: 0.341, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0836, loss: 0.0836, grad_norm: 2.0472
2022-06-04 15:47:26,613 - depth - INFO - Epoch [5][1400/1514]	lr: 7.822e-05, eta: 3:14:11, time: 0.435, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0838, loss: 0.0838, grad_norm: 2.6649
2022-06-04 15:47:48,044 - depth - INFO - Epoch [5][1450/1514]	lr: 7.880e-05, eta: 3:13:56, time: 0.429, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0848, loss: 0.0848, grad_norm: 2.3916
2022-06-04 15:48:09,917 - depth - INFO - Epoch [5][1500/1514]	lr: 7.937e-05, eta: 3:13:42, time: 0.437, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0834, loss: 0.0834, grad_norm: 2.2416
2022-06-04 15:48:20,025 - depth - INFO - Saving checkpoint at 5 epochs
2022-06-04 15:48:46,029 - depth - INFO - Summary:
2022-06-04 15:48:46,030 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
| 0.8624 | 0.9792 | 0.996 |  0.1268 | 0.4158 | 0.0517 |  0.1506  | 11.7124 | 0.0752 |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
2022-06-04 15:48:46,030 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 15:48:46,030 - depth - INFO - Epoch(val) [5][82]	a1: 0.8624, a2: 0.9792, a3: 0.9960, abs_rel: 0.12679189443588257, rmse: 0.4158267378807068, log_10: 0.05172282084822655, rmse_log: 0.15061864256858826, silog: 11.7124, sq_rel: 0.07516158372163773
2022-06-04 15:49:16,762 - depth - INFO - Epoch [6][50/1514]	lr: 8.009e-05, eta: 3:13:34, time: 0.614, data_time: 0.232, memory: 12384, decode.loss_depth: 0.0815, loss: 0.0815, grad_norm: 1.9992
2022-06-04 15:49:38,046 - depth - INFO - Epoch [6][100/1514]	lr: 8.065e-05, eta: 3:13:18, time: 0.426, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0850, loss: 0.0850, grad_norm: 2.4294
2022-06-04 15:49:59,750 - depth - INFO - Epoch [6][150/1514]	lr: 8.120e-05, eta: 3:13:03, time: 0.434, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0870, loss: 0.0870, grad_norm: 2.5045
2022-06-04 15:50:16,739 - depth - INFO - Epoch [6][200/1514]	lr: 8.175e-05, eta: 3:12:31, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0853, loss: 0.0853, grad_norm: 2.9547
2022-06-04 15:50:37,911 - depth - INFO - Epoch [6][250/1514]	lr: 8.229e-05, eta: 3:12:14, time: 0.423, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0829, loss: 0.0829, grad_norm: 2.7132
2022-06-04 15:50:59,582 - depth - INFO - Epoch [6][300/1514]	lr: 8.282e-05, eta: 3:11:59, time: 0.433, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0824, loss: 0.0824, grad_norm: 1.9222
2022-06-04 15:51:21,025 - depth - INFO - Epoch [6][350/1514]	lr: 8.335e-05, eta: 3:11:43, time: 0.429, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0812, loss: 0.0812, grad_norm: 1.8527
2022-06-04 15:51:42,680 - depth - INFO - Epoch [6][400/1514]	lr: 8.387e-05, eta: 3:11:28, time: 0.433, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0805, loss: 0.0805, grad_norm: 1.9506
2022-06-04 15:51:59,481 - depth - INFO - Epoch [6][450/1514]	lr: 8.438e-05, eta: 3:10:56, time: 0.336, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0802, loss: 0.0802, grad_norm: 2.0651
2022-06-04 15:52:16,591 - depth - INFO - Epoch [6][500/1514]	lr: 8.489e-05, eta: 3:10:25, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0841, loss: 0.0841, grad_norm: 2.0391
2022-06-04 15:52:33,672 - depth - INFO - Epoch [6][550/1514]	lr: 8.539e-05, eta: 3:09:54, time: 0.342, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0842, loss: 0.0842, grad_norm: 2.4166
2022-06-04 15:52:50,709 - depth - INFO - Epoch [6][600/1514]	lr: 8.588e-05, eta: 3:09:22, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0802, loss: 0.0802, grad_norm: 2.3703
2022-06-04 15:53:11,918 - depth - INFO - Epoch [6][650/1514]	lr: 8.637e-05, eta: 3:09:06, time: 0.424, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0810, loss: 0.0810, grad_norm: 2.4499
2022-06-04 15:53:33,600 - depth - INFO - Epoch [6][700/1514]	lr: 8.685e-05, eta: 3:08:51, time: 0.434, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0810, loss: 0.0810, grad_norm: 2.4490
2022-06-04 15:53:50,580 - depth - INFO - Epoch [6][750/1514]	lr: 8.732e-05, eta: 3:08:20, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0780, loss: 0.0780, grad_norm: 2.5344
2022-06-04 15:54:12,062 - depth - INFO - Epoch [6][800/1514]	lr: 8.779e-05, eta: 3:08:04, time: 0.430, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0807, loss: 0.0807, grad_norm: 1.8495
2022-06-04 15:54:33,610 - depth - INFO - Epoch [6][850/1514]	lr: 8.824e-05, eta: 3:07:48, time: 0.431, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0778, loss: 0.0778, grad_norm: 1.9631
2022-06-04 15:54:50,455 - depth - INFO - Epoch [6][900/1514]	lr: 8.869e-05, eta: 3:07:17, time: 0.337, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0775, loss: 0.0775, grad_norm: 2.0841
2022-06-04 15:55:11,838 - depth - INFO - Epoch [6][950/1514]	lr: 8.914e-05, eta: 3:07:01, time: 0.427, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0791, loss: 0.0791, grad_norm: 1.9323
2022-06-04 15:55:33,620 - depth - INFO - Epoch [6][1000/1514]	lr: 8.957e-05, eta: 3:06:46, time: 0.436, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0819, loss: 0.0819, grad_norm: 2.9679
2022-06-04 15:55:50,645 - depth - INFO - Epoch [6][1050/1514]	lr: 9.000e-05, eta: 3:06:16, time: 0.341, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0764, loss: 0.0764, grad_norm: 1.6293
2022-06-04 15:56:11,890 - depth - INFO - Epoch [6][1100/1514]	lr: 9.042e-05, eta: 3:05:59, time: 0.425, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0788, loss: 0.0788, grad_norm: 1.5517
2022-06-04 15:56:33,644 - depth - INFO - Epoch [6][1150/1514]	lr: 9.083e-05, eta: 3:05:44, time: 0.435, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0778, loss: 0.0778, grad_norm: 1.7980
2022-06-04 15:56:50,602 - depth - INFO - Epoch [6][1200/1514]	lr: 9.123e-05, eta: 3:05:14, time: 0.339, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0758, loss: 0.0758, grad_norm: 1.1593
2022-06-04 15:57:07,597 - depth - INFO - Epoch [6][1250/1514]	lr: 9.162e-05, eta: 3:04:44, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0784, loss: 0.0784, grad_norm: 1.5428
2022-06-04 15:57:28,881 - depth - INFO - Epoch [6][1300/1514]	lr: 9.201e-05, eta: 3:04:27, time: 0.426, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0779, loss: 0.0779, grad_norm: 1.8270
2022-06-04 15:57:50,736 - depth - INFO - Epoch [6][1350/1514]	lr: 9.239e-05, eta: 3:04:12, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0750, loss: 0.0750, grad_norm: 1.7307
2022-06-04 15:58:11,829 - depth - INFO - Epoch [6][1400/1514]	lr: 9.276e-05, eta: 3:03:55, time: 0.422, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0805, loss: 0.0805, grad_norm: 2.3964
2022-06-04 15:58:33,677 - depth - INFO - Epoch [6][1450/1514]	lr: 9.312e-05, eta: 3:03:40, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0785, loss: 0.0785, grad_norm: 2.0418
2022-06-04 15:58:54,981 - depth - INFO - Epoch [6][1500/1514]	lr: 9.347e-05, eta: 3:03:23, time: 0.426, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0766, loss: 0.0766, grad_norm: 2.7167
2022-06-04 15:59:05,033 - depth - INFO - Saving checkpoint at 6 epochs
2022-06-04 15:59:32,138 - depth - INFO - Summary:
2022-06-04 15:59:32,139 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.7875 | 0.9647 | 0.9911 |  0.171  | 0.4975 | 0.0663 |  0.1843  | 12.3874 | 0.1164 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 15:59:32,139 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 15:59:32,139 - depth - INFO - Epoch(val) [6][82]	a1: 0.7875, a2: 0.9647, a3: 0.9911, abs_rel: 0.17104515433311462, rmse: 0.49746665358543396, log_10: 0.06631866097450256, rmse_log: 0.18429994583129883, silog: 12.3874, sq_rel: 0.11641012132167816
2022-06-04 16:00:04,441 - depth - INFO - Epoch [7][50/1514]	lr: 9.391e-05, eta: 3:03:16, time: 0.646, data_time: 0.217, memory: 12384, decode.loss_depth: 0.0851, loss: 0.0851, grad_norm: 2.1463
2022-06-04 16:00:21,529 - depth - INFO - Epoch [7][100/1514]	lr: 9.424e-05, eta: 3:02:47, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0795, loss: 0.0795, grad_norm: 1.8961
2022-06-04 16:00:42,799 - depth - INFO - Epoch [7][150/1514]	lr: 9.457e-05, eta: 3:02:30, time: 0.425, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0773, loss: 0.0773, grad_norm: 1.6080
2022-06-04 16:01:04,824 - depth - INFO - Epoch [7][200/1514]	lr: 9.488e-05, eta: 3:02:15, time: 0.440, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0802, loss: 0.0802, grad_norm: 1.8813
2022-06-04 16:01:26,674 - depth - INFO - Epoch [7][250/1514]	lr: 9.519e-05, eta: 3:01:59, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0778, loss: 0.0778, grad_norm: 2.5909
2022-06-04 16:01:47,786 - depth - INFO - Epoch [7][300/1514]	lr: 9.549e-05, eta: 3:01:42, time: 0.422, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0744, loss: 0.0744, grad_norm: 1.7773
2022-06-04 16:02:08,872 - depth - INFO - Epoch [7][350/1514]	lr: 9.577e-05, eta: 3:01:24, time: 0.422, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0763, loss: 0.0763, grad_norm: 2.4347
2022-06-04 16:02:30,568 - depth - INFO - Epoch [7][400/1514]	lr: 9.605e-05, eta: 3:01:08, time: 0.434, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0735, loss: 0.0735, grad_norm: 1.4654
2022-06-04 16:02:47,454 - depth - INFO - Epoch [7][450/1514]	lr: 9.632e-05, eta: 3:00:38, time: 0.338, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0739, loss: 0.0739, grad_norm: 1.6277
2022-06-04 16:03:08,835 - depth - INFO - Epoch [7][500/1514]	lr: 9.658e-05, eta: 3:00:21, time: 0.428, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0738, loss: 0.0738, grad_norm: 1.8639
2022-06-04 16:03:25,802 - depth - INFO - Epoch [7][550/1514]	lr: 9.684e-05, eta: 2:59:52, time: 0.339, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0742, loss: 0.0742, grad_norm: 2.4255
2022-06-04 16:03:47,735 - depth - INFO - Epoch [7][600/1514]	lr: 9.708e-05, eta: 2:59:37, time: 0.439, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0729, loss: 0.0729, grad_norm: 1.6702
2022-06-04 16:04:04,981 - depth - INFO - Epoch [7][650/1514]	lr: 9.731e-05, eta: 2:59:08, time: 0.345, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0741, loss: 0.0741, grad_norm: 1.5263
2022-06-04 16:04:21,934 - depth - INFO - Epoch [7][700/1514]	lr: 9.753e-05, eta: 2:58:39, time: 0.339, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0733, loss: 0.0733, grad_norm: 1.5504
2022-06-04 16:04:38,970 - depth - INFO - Epoch [7][750/1514]	lr: 9.775e-05, eta: 2:58:11, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0703, loss: 0.0703, grad_norm: 1.6398
2022-06-04 16:05:00,742 - depth - INFO - Epoch [7][800/1514]	lr: 9.795e-05, eta: 2:57:55, time: 0.435, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0714, loss: 0.0714, grad_norm: 1.6622
2022-06-04 16:05:21,961 - depth - INFO - Epoch [7][850/1514]	lr: 9.815e-05, eta: 2:57:37, time: 0.424, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0710, loss: 0.0710, grad_norm: 1.4095
2022-06-04 16:05:43,894 - depth - INFO - Epoch [7][900/1514]	lr: 9.833e-05, eta: 2:57:22, time: 0.439, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0732, loss: 0.0732, grad_norm: 2.8692
2022-06-04 16:06:00,835 - depth - INFO - Epoch [7][950/1514]	lr: 9.851e-05, eta: 2:56:53, time: 0.339, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0737, loss: 0.0737, grad_norm: 3.0055
2022-06-04 16:06:17,858 - depth - INFO - Epoch [7][1000/1514]	lr: 9.868e-05, eta: 2:56:25, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0727, loss: 0.0727, grad_norm: 2.0249
2022-06-04 16:06:39,702 - depth - INFO - Epoch [7][1050/1514]	lr: 9.883e-05, eta: 2:56:09, time: 0.437, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0704, loss: 0.0704, grad_norm: 1.9606
2022-06-04 16:06:56,975 - depth - INFO - Epoch [7][1100/1514]	lr: 9.898e-05, eta: 2:55:41, time: 0.345, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0713, loss: 0.0713, grad_norm: 2.0074
2022-06-04 16:07:14,450 - depth - INFO - Epoch [7][1150/1514]	lr: 9.912e-05, eta: 2:55:14, time: 0.350, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0727, loss: 0.0727, grad_norm: 1.5275
2022-06-04 16:07:35,905 - depth - INFO - Epoch [7][1200/1514]	lr: 9.924e-05, eta: 2:54:57, time: 0.429, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0703, loss: 0.0703, grad_norm: 1.9609
2022-06-04 16:07:53,099 - depth - INFO - Epoch [7][1250/1514]	lr: 9.936e-05, eta: 2:54:30, time: 0.344, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0702, loss: 0.0702, grad_norm: 1.5146
2022-06-04 16:08:09,993 - depth - INFO - Epoch [7][1300/1514]	lr: 9.947e-05, eta: 2:54:02, time: 0.338, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0705, loss: 0.0705, grad_norm: 2.5916
2022-06-04 16:08:27,049 - depth - INFO - Epoch [7][1350/1514]	lr: 9.957e-05, eta: 2:53:34, time: 0.341, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0674, loss: 0.0674, grad_norm: 1.7285
2022-06-04 16:08:44,454 - depth - INFO - Epoch [7][1400/1514]	lr: 9.965e-05, eta: 2:53:07, time: 0.348, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0706, loss: 0.0706, grad_norm: 2.1247
2022-06-04 16:09:01,495 - depth - INFO - Epoch [7][1450/1514]	lr: 9.973e-05, eta: 2:52:40, time: 0.341, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0726, loss: 0.0726, grad_norm: 4.4575
2022-06-04 16:09:18,593 - depth - INFO - Epoch [7][1500/1514]	lr: 9.980e-05, eta: 2:52:12, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0684, loss: 0.0684, grad_norm: 2.5177
2022-06-04 16:09:23,897 - depth - INFO - Saving checkpoint at 7 epochs
2022-06-04 16:09:50,468 - depth - INFO - Summary:
2022-06-04 16:09:50,468 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8285 | 0.9763 | 0.9956 |  0.1481 | 0.4472 | 0.0588 |  0.1646  | 11.5538 | 0.0899 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 16:09:50,469 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 16:09:50,469 - depth - INFO - Epoch(val) [7][82]	a1: 0.8285, a2: 0.9763, a3: 0.9956, abs_rel: 0.14806625247001648, rmse: 0.4472079873085022, log_10: 0.05880884826183319, rmse_log: 0.16456229984760284, silog: 11.5538, sq_rel: 0.08991635590791702
2022-06-04 16:10:21,444 - depth - INFO - Epoch [8][50/1514]	lr: 9.987e-05, eta: 2:51:59, time: 0.619, data_time: 0.224, memory: 12384, decode.loss_depth: 0.0683, loss: 0.0683, grad_norm: 1.6081
2022-06-04 16:10:38,432 - depth - INFO - Epoch [8][100/1514]	lr: 9.992e-05, eta: 2:51:32, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0689, loss: 0.0689, grad_norm: 2.0013
2022-06-04 16:11:00,043 - depth - INFO - Epoch [8][150/1514]	lr: 9.995e-05, eta: 2:51:15, time: 0.432, data_time: 0.011, memory: 12384, decode.loss_depth: 0.0703, loss: 0.0703, grad_norm: 1.7424
2022-06-04 16:11:21,766 - depth - INFO - Epoch [8][200/1514]	lr: 9.998e-05, eta: 2:50:59, time: 0.434, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0702, loss: 0.0702, grad_norm: 1.5042
2022-06-04 16:11:38,839 - depth - INFO - Epoch [8][250/1514]	lr: 9.999e-05, eta: 2:50:32, time: 0.342, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0704, loss: 0.0704, grad_norm: 1.4996
2022-06-04 16:12:00,561 - depth - INFO - Epoch [8][300/1514]	lr: 1.000e-04, eta: 2:50:16, time: 0.434, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0689, loss: 0.0689, grad_norm: 1.9796
2022-06-04 16:12:21,977 - depth - INFO - Epoch [8][350/1514]	lr: 1.000e-04, eta: 2:49:59, time: 0.428, data_time: 0.011, memory: 12384, decode.loss_depth: 0.0686, loss: 0.0686, grad_norm: 1.9261
2022-06-04 16:12:39,002 - depth - INFO - Epoch [8][400/1514]	lr: 1.000e-04, eta: 2:49:32, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0672, loss: 0.0672, grad_norm: 1.3386
2022-06-04 16:12:55,894 - depth - INFO - Epoch [8][450/1514]	lr: 9.999e-05, eta: 2:49:04, time: 0.338, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0673, loss: 0.0673, grad_norm: 1.7333
2022-06-04 16:13:17,544 - depth - INFO - Epoch [8][500/1514]	lr: 9.999e-05, eta: 2:48:48, time: 0.433, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0663, loss: 0.0663, grad_norm: 1.4673
2022-06-04 16:13:34,564 - depth - INFO - Epoch [8][550/1514]	lr: 9.998e-05, eta: 2:48:21, time: 0.340, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0686, loss: 0.0686, grad_norm: 1.6928
2022-06-04 16:13:51,609 - depth - INFO - Epoch [8][600/1514]	lr: 9.997e-05, eta: 2:47:54, time: 0.341, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0647, loss: 0.0647, grad_norm: 1.3328
2022-06-04 16:14:12,897 - depth - INFO - Epoch [8][650/1514]	lr: 9.995e-05, eta: 2:47:37, time: 0.426, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0669, loss: 0.0669, grad_norm: 1.8190
2022-06-04 16:14:34,687 - depth - INFO - Epoch [8][700/1514]	lr: 9.994e-05, eta: 2:47:21, time: 0.436, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0682, loss: 0.0682, grad_norm: 1.9129
2022-06-04 16:14:56,225 - depth - INFO - Epoch [8][750/1514]	lr: 9.992e-05, eta: 2:47:04, time: 0.431, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0667, loss: 0.0667, grad_norm: 1.5642
2022-06-04 16:15:13,345 - depth - INFO - Epoch [8][800/1514]	lr: 9.991e-05, eta: 2:46:37, time: 0.342, data_time: 0.011, memory: 12384, decode.loss_depth: 0.0656, loss: 0.0656, grad_norm: 1.3577
2022-06-04 16:15:30,426 - depth - INFO - Epoch [8][850/1514]	lr: 9.989e-05, eta: 2:46:11, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0651, loss: 0.0651, grad_norm: 2.0792
2022-06-04 16:15:47,618 - depth - INFO - Epoch [8][900/1514]	lr: 9.986e-05, eta: 2:45:45, time: 0.344, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0656, loss: 0.0656, grad_norm: 1.6089
2022-06-04 16:16:08,988 - depth - INFO - Epoch [8][950/1514]	lr: 9.984e-05, eta: 2:45:28, time: 0.428, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0663, loss: 0.0663, grad_norm: 2.0064
2022-06-04 16:16:25,973 - depth - INFO - Epoch [8][1000/1514]	lr: 9.981e-05, eta: 2:45:01, time: 0.339, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0650, loss: 0.0650, grad_norm: 1.4430
2022-06-04 16:16:47,862 - depth - INFO - Epoch [8][1050/1514]	lr: 9.979e-05, eta: 2:44:45, time: 0.438, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0635, loss: 0.0635, grad_norm: 2.2772
2022-06-04 16:17:09,675 - depth - INFO - Epoch [8][1100/1514]	lr: 9.976e-05, eta: 2:44:29, time: 0.436, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0658, loss: 0.0658, grad_norm: 1.2724
2022-06-04 16:17:26,652 - depth - INFO - Epoch [8][1150/1514]	lr: 9.973e-05, eta: 2:44:02, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0657, loss: 0.0657, grad_norm: 1.5771
2022-06-04 16:17:43,704 - depth - INFO - Epoch [8][1200/1514]	lr: 9.969e-05, eta: 2:43:36, time: 0.341, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0658, loss: 0.0658, grad_norm: 1.9892
2022-06-04 16:18:00,727 - depth - INFO - Epoch [8][1250/1514]	lr: 9.966e-05, eta: 2:43:10, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0656, loss: 0.0656, grad_norm: 1.2331
2022-06-04 16:18:21,863 - depth - INFO - Epoch [8][1300/1514]	lr: 9.962e-05, eta: 2:42:52, time: 0.423, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0644, loss: 0.0644, grad_norm: 1.2231
2022-06-04 16:18:43,553 - depth - INFO - Epoch [8][1350/1514]	lr: 9.958e-05, eta: 2:42:36, time: 0.434, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0630, loss: 0.0630, grad_norm: 1.8801
2022-06-04 16:19:00,440 - depth - INFO - Epoch [8][1400/1514]	lr: 9.954e-05, eta: 2:42:10, time: 0.338, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0650, loss: 0.0650, grad_norm: 1.8968
2022-06-04 16:19:21,899 - depth - INFO - Epoch [8][1450/1514]	lr: 9.950e-05, eta: 2:41:53, time: 0.429, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0633, loss: 0.0633, grad_norm: 2.1081
2022-06-04 16:19:43,596 - depth - INFO - Epoch [8][1500/1514]	lr: 9.945e-05, eta: 2:41:36, time: 0.434, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0628, loss: 0.0628, grad_norm: 1.8907
2022-06-04 16:19:48,952 - depth - INFO - Saving checkpoint at 8 epochs
2022-06-04 16:20:14,986 - depth - INFO - Summary:
2022-06-04 16:20:14,987 - depth - INFO - 
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse | log_10 | rmse_log | silog  | sq_rel |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
| 0.8438 | 0.9772 | 0.9946 |  0.1413 | 0.443 | 0.0561 |  0.1591  | 11.253 | 0.0898 |
+--------+--------+--------+---------+-------+--------+----------+--------+--------+
2022-06-04 16:20:14,987 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 16:20:14,987 - depth - INFO - Epoch(val) [8][82]	a1: 0.8438, a2: 0.9772, a3: 0.9946, abs_rel: 0.14127489924430847, rmse: 0.442961186170578, log_10: 0.05605855584144592, rmse_log: 0.15909506380558014, silog: 11.2530, sq_rel: 0.08979568630456924
2022-06-04 16:20:43,379 - depth - INFO - Epoch [9][50/1514]	lr: 9.939e-05, eta: 2:41:16, time: 0.568, data_time: 0.218, memory: 12384, decode.loss_depth: 0.0657, loss: 0.0657, grad_norm: 1.6798
2022-06-04 16:21:04,903 - depth - INFO - Epoch [9][100/1514]	lr: 9.935e-05, eta: 2:40:59, time: 0.430, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0764, loss: 0.0764, grad_norm: 1.6685
2022-06-04 16:21:26,764 - depth - INFO - Epoch [9][150/1514]	lr: 9.930e-05, eta: 2:40:42, time: 0.437, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0646, loss: 0.0646, grad_norm: 1.5083
2022-06-04 16:21:48,612 - depth - INFO - Epoch [9][200/1514]	lr: 9.924e-05, eta: 2:40:26, time: 0.437, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0657, loss: 0.0657, grad_norm: 1.4841
2022-06-04 16:22:09,915 - depth - INFO - Epoch [9][250/1514]	lr: 9.919e-05, eta: 2:40:08, time: 0.426, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0656, loss: 0.0656, grad_norm: 1.7426
2022-06-04 16:22:31,648 - depth - INFO - Epoch [9][300/1514]	lr: 9.913e-05, eta: 2:39:52, time: 0.435, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0605, loss: 0.0605, grad_norm: 1.2916
2022-06-04 16:22:48,889 - depth - INFO - Epoch [9][350/1514]	lr: 9.907e-05, eta: 2:39:26, time: 0.345, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0605, loss: 0.0605, grad_norm: 1.7461
2022-06-04 16:23:05,803 - depth - INFO - Epoch [9][400/1514]	lr: 9.901e-05, eta: 2:39:00, time: 0.338, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0608, loss: 0.0608, grad_norm: 2.2342
2022-06-04 16:23:22,761 - depth - INFO - Epoch [9][450/1514]	lr: 9.895e-05, eta: 2:38:34, time: 0.339, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0614, loss: 0.0614, grad_norm: 2.1243
2022-06-04 16:23:39,833 - depth - INFO - Epoch [9][500/1514]	lr: 9.889e-05, eta: 2:38:09, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0619, loss: 0.0619, grad_norm: 2.2796
2022-06-04 16:24:01,608 - depth - INFO - Epoch [9][550/1514]	lr: 9.882e-05, eta: 2:37:52, time: 0.435, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0722, loss: 0.0722, grad_norm: 3.2180
2022-06-04 16:24:22,836 - depth - INFO - Epoch [9][600/1514]	lr: 9.875e-05, eta: 2:37:34, time: 0.425, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0665, loss: 0.0665, grad_norm: 2.5356
2022-06-04 16:24:44,759 - depth - INFO - Epoch [9][650/1514]	lr: 9.869e-05, eta: 2:37:18, time: 0.438, data_time: 0.011, memory: 12384, decode.loss_depth: 0.0634, loss: 0.0634, grad_norm: 1.1561
2022-06-04 16:25:01,845 - depth - INFO - Epoch [9][700/1514]	lr: 9.861e-05, eta: 2:36:53, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0620, loss: 0.0620, grad_norm: 1.8236
2022-06-04 16:25:18,662 - depth - INFO - Epoch [9][750/1514]	lr: 9.854e-05, eta: 2:36:27, time: 0.336, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0607, loss: 0.0607, grad_norm: 1.8012
2022-06-04 16:25:35,563 - depth - INFO - Epoch [9][800/1514]	lr: 9.847e-05, eta: 2:36:01, time: 0.338, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0623, loss: 0.0623, grad_norm: 0.9945
2022-06-04 16:25:52,611 - depth - INFO - Epoch [9][850/1514]	lr: 9.839e-05, eta: 2:35:36, time: 0.341, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0614, loss: 0.0614, grad_norm: 1.6621
2022-06-04 16:26:13,791 - depth - INFO - Epoch [9][900/1514]	lr: 9.831e-05, eta: 2:35:18, time: 0.423, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0614, loss: 0.0614, grad_norm: 1.6322
2022-06-04 16:26:30,886 - depth - INFO - Epoch [9][950/1514]	lr: 9.823e-05, eta: 2:34:53, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0613, loss: 0.0613, grad_norm: 1.1615
2022-06-04 16:26:47,789 - depth - INFO - Epoch [9][1000/1514]	lr: 9.815e-05, eta: 2:34:27, time: 0.338, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0604, loss: 0.0604, grad_norm: 1.5776
2022-06-04 16:27:08,971 - depth - INFO - Epoch [9][1050/1514]	lr: 9.806e-05, eta: 2:34:10, time: 0.424, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0590, loss: 0.0590, grad_norm: 1.4878
2022-06-04 16:27:26,178 - depth - INFO - Epoch [9][1100/1514]	lr: 9.798e-05, eta: 2:33:45, time: 0.344, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0598, loss: 0.0598, grad_norm: 1.6603
2022-06-04 16:27:47,904 - depth - INFO - Epoch [9][1150/1514]	lr: 9.789e-05, eta: 2:33:28, time: 0.435, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0622, loss: 0.0622, grad_norm: 1.2806
2022-06-04 16:28:04,961 - depth - INFO - Epoch [9][1200/1514]	lr: 9.780e-05, eta: 2:33:03, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0587, loss: 0.0587, grad_norm: 2.0687
2022-06-04 16:28:21,987 - depth - INFO - Epoch [9][1250/1514]	lr: 9.771e-05, eta: 2:32:38, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0641, loss: 0.0641, grad_norm: 3.9800
2022-06-04 16:28:43,716 - depth - INFO - Epoch [9][1300/1514]	lr: 9.762e-05, eta: 2:32:21, time: 0.435, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0597, loss: 0.0597, grad_norm: 2.2623
2022-06-04 16:29:04,897 - depth - INFO - Epoch [9][1350/1514]	lr: 9.752e-05, eta: 2:32:03, time: 0.423, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0582, loss: 0.0582, grad_norm: 2.0667
2022-06-04 16:29:26,668 - depth - INFO - Epoch [9][1400/1514]	lr: 9.742e-05, eta: 2:31:47, time: 0.435, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0583, loss: 0.0583, grad_norm: 1.9662
2022-06-04 16:29:47,818 - depth - INFO - Epoch [9][1450/1514]	lr: 9.732e-05, eta: 2:31:29, time: 0.423, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0586, loss: 0.0586, grad_norm: 2.3575
2022-06-04 16:30:09,657 - depth - INFO - Epoch [9][1500/1514]	lr: 9.722e-05, eta: 2:31:12, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0565, loss: 0.0565, grad_norm: 1.2373
2022-06-04 16:30:19,240 - depth - INFO - Saving checkpoint at 9 epochs
2022-06-04 16:30:44,995 - depth - INFO - Summary:
2022-06-04 16:30:44,995 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8338 | 0.9769 | 0.9953 |  0.1508 | 0.4488 | 0.0593 |  0.1651  | 10.9912 | 0.092  |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 16:30:44,996 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 16:30:44,996 - depth - INFO - Epoch(val) [9][82]	a1: 0.8338, a2: 0.9769, a3: 0.9953, abs_rel: 0.15077313780784607, rmse: 0.4488055408000946, log_10: 0.059342771768569946, rmse_log: 0.16511957347393036, silog: 10.9912, sq_rel: 0.09196769446134567
2022-06-04 16:31:15,725 - depth - INFO - Epoch [10][50/1514]	lr: 9.709e-05, eta: 2:30:55, time: 0.614, data_time: 0.198, memory: 12384, decode.loss_depth: 0.0583, loss: 0.0583, grad_norm: 1.5925
2022-06-04 16:31:32,925 - depth - INFO - Epoch [10][100/1514]	lr: 9.699e-05, eta: 2:30:30, time: 0.344, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0605, loss: 0.0605, grad_norm: 2.2816
2022-06-04 16:31:54,693 - depth - INFO - Epoch [10][150/1514]	lr: 9.688e-05, eta: 2:30:13, time: 0.435, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0588, loss: 0.0588, grad_norm: 2.0360
2022-06-04 16:32:15,975 - depth - INFO - Epoch [10][200/1514]	lr: 9.677e-05, eta: 2:29:55, time: 0.426, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0590, loss: 0.0590, grad_norm: 1.1407
2022-06-04 16:32:32,851 - depth - INFO - Epoch [10][250/1514]	lr: 9.666e-05, eta: 2:29:30, time: 0.338, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0593, loss: 0.0593, grad_norm: 1.5280
2022-06-04 16:32:54,549 - depth - INFO - Epoch [10][300/1514]	lr: 9.655e-05, eta: 2:29:13, time: 0.434, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0580, loss: 0.0580, grad_norm: 1.2932
2022-06-04 16:33:15,762 - depth - INFO - Epoch [10][350/1514]	lr: 9.644e-05, eta: 2:28:55, time: 0.425, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0574, loss: 0.0574, grad_norm: 1.8683
2022-06-04 16:33:37,617 - depth - INFO - Epoch [10][400/1514]	lr: 9.632e-05, eta: 2:28:38, time: 0.437, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0565, loss: 0.0565, grad_norm: 1.4330
2022-06-04 16:33:54,382 - depth - INFO - Epoch [10][450/1514]	lr: 9.621e-05, eta: 2:28:13, time: 0.335, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0565, loss: 0.0565, grad_norm: 2.0775
2022-06-04 16:34:15,820 - depth - INFO - Epoch [10][500/1514]	lr: 9.609e-05, eta: 2:27:55, time: 0.429, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0572, loss: 0.0572, grad_norm: 1.2763
2022-06-04 16:34:42,198 - depth - INFO - Epoch [10][550/1514]	lr: 9.597e-05, eta: 2:27:45, time: 0.527, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0598, loss: 0.0598, grad_norm: 2.0438
2022-06-04 16:35:03,770 - depth - INFO - Epoch [10][600/1514]	lr: 9.584e-05, eta: 2:27:28, time: 0.431, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0578, loss: 0.0578, grad_norm: 1.8495
2022-06-04 16:35:20,895 - depth - INFO - Epoch [10][650/1514]	lr: 9.572e-05, eta: 2:27:03, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0569, loss: 0.0569, grad_norm: 1.5637
2022-06-04 16:35:42,810 - depth - INFO - Epoch [10][700/1514]	lr: 9.559e-05, eta: 2:26:46, time: 0.438, data_time: 0.011, memory: 12384, decode.loss_depth: 0.0564, loss: 0.0564, grad_norm: 1.9260
2022-06-04 16:36:00,033 - depth - INFO - Epoch [10][750/1514]	lr: 9.547e-05, eta: 2:26:22, time: 0.345, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0561, loss: 0.0561, grad_norm: 1.4867
2022-06-04 16:36:21,688 - depth - INFO - Epoch [10][800/1514]	lr: 9.534e-05, eta: 2:26:05, time: 0.433, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0559, loss: 0.0559, grad_norm: 1.3511
2022-06-04 16:36:38,594 - depth - INFO - Epoch [10][850/1514]	lr: 9.521e-05, eta: 2:25:40, time: 0.338, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0545, loss: 0.0545, grad_norm: 2.1523
2022-06-04 16:37:00,233 - depth - INFO - Epoch [10][900/1514]	lr: 9.507e-05, eta: 2:25:22, time: 0.433, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0540, loss: 0.0540, grad_norm: 1.3545
2022-06-04 16:37:21,925 - depth - INFO - Epoch [10][950/1514]	lr: 9.494e-05, eta: 2:25:05, time: 0.434, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0574, loss: 0.0574, grad_norm: 1.6184
2022-06-04 16:37:38,924 - depth - INFO - Epoch [10][1000/1514]	lr: 9.480e-05, eta: 2:24:40, time: 0.340, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0558, loss: 0.0558, grad_norm: 1.1515
2022-06-04 16:38:00,922 - depth - INFO - Epoch [10][1050/1514]	lr: 9.466e-05, eta: 2:24:23, time: 0.440, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0560, loss: 0.0560, grad_norm: 1.9715
2022-06-04 16:38:22,606 - depth - INFO - Epoch [10][1100/1514]	lr: 9.452e-05, eta: 2:24:06, time: 0.434, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0553, loss: 0.0553, grad_norm: 1.6422
2022-06-04 16:38:39,703 - depth - INFO - Epoch [10][1150/1514]	lr: 9.438e-05, eta: 2:23:42, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0570, loss: 0.0570, grad_norm: 1.1759
2022-06-04 16:38:56,681 - depth - INFO - Epoch [10][1200/1514]	lr: 9.424e-05, eta: 2:23:17, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0547, loss: 0.0547, grad_norm: 1.5878
2022-06-04 16:39:18,819 - depth - INFO - Epoch [10][1250/1514]	lr: 9.410e-05, eta: 2:23:00, time: 0.443, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0563, loss: 0.0563, grad_norm: 1.0667
2022-06-04 16:39:40,683 - depth - INFO - Epoch [10][1300/1514]	lr: 9.395e-05, eta: 2:22:43, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0562, loss: 0.0562, grad_norm: 1.6382
2022-06-04 16:39:57,663 - depth - INFO - Epoch [10][1350/1514]	lr: 9.380e-05, eta: 2:22:19, time: 0.339, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0546, loss: 0.0546, grad_norm: 1.4388
2022-06-04 16:40:18,980 - depth - INFO - Epoch [10][1400/1514]	lr: 9.365e-05, eta: 2:22:01, time: 0.426, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0559, loss: 0.0559, grad_norm: 1.3640
2022-06-04 16:40:36,049 - depth - INFO - Epoch [10][1450/1514]	lr: 9.350e-05, eta: 2:21:37, time: 0.341, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0540, loss: 0.0540, grad_norm: 1.9272
2022-06-04 16:40:53,147 - depth - INFO - Epoch [10][1500/1514]	lr: 9.335e-05, eta: 2:21:13, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0523, loss: 0.0523, grad_norm: 1.3677
2022-06-04 16:41:03,003 - depth - INFO - Saving checkpoint at 10 epochs
2022-06-04 16:41:29,425 - depth - INFO - Summary:
2022-06-04 16:41:29,426 - depth - INFO - 
+-------+-------+--------+---------+--------+--------+----------+---------+--------+
|   a1  |   a2  |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+-------+-------+--------+---------+--------+--------+----------+---------+--------+
| 0.871 | 0.981 | 0.9963 |  0.1252 | 0.4027 | 0.051  |  0.1472  | 11.0519 | 0.0715 |
+-------+-------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 16:41:29,426 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 16:41:29,426 - depth - INFO - Epoch(val) [10][82]	a1: 0.8710, a2: 0.9810, a3: 0.9963, abs_rel: 0.12523730099201202, rmse: 0.40266039967536926, log_10: 0.050970543175935745, rmse_log: 0.14720822870731354, silog: 11.0519, sq_rel: 0.07151349633932114
2022-06-04 16:41:59,841 - depth - INFO - Epoch [11][50/1514]	lr: 9.315e-05, eta: 2:20:54, time: 0.608, data_time: 0.218, memory: 12384, decode.loss_depth: 0.0547, loss: 0.0547, grad_norm: 1.3512
2022-06-04 16:42:21,643 - depth - INFO - Epoch [11][100/1514]	lr: 9.299e-05, eta: 2:20:36, time: 0.436, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0572, loss: 0.0572, grad_norm: 3.4156
2022-06-04 16:42:38,594 - depth - INFO - Epoch [11][150/1514]	lr: 9.283e-05, eta: 2:20:12, time: 0.339, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0559, loss: 0.0559, grad_norm: 3.1906
2022-06-04 16:42:55,578 - depth - INFO - Epoch [11][200/1514]	lr: 9.267e-05, eta: 2:19:48, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0585, loss: 0.0585, grad_norm: 2.4758
2022-06-04 16:43:17,033 - depth - INFO - Epoch [11][250/1514]	lr: 9.251e-05, eta: 2:19:30, time: 0.429, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0557, loss: 0.0557, grad_norm: 1.3862
2022-06-04 16:43:38,872 - depth - INFO - Epoch [11][300/1514]	lr: 9.235e-05, eta: 2:19:13, time: 0.437, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0548, loss: 0.0548, grad_norm: 2.2529
2022-06-04 16:43:55,818 - depth - INFO - Epoch [11][350/1514]	lr: 9.218e-05, eta: 2:18:48, time: 0.339, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0531, loss: 0.0531, grad_norm: 2.1213
2022-06-04 16:44:13,038 - depth - INFO - Epoch [11][400/1514]	lr: 9.202e-05, eta: 2:18:25, time: 0.344, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0536, loss: 0.0536, grad_norm: 2.0026
2022-06-04 16:44:30,100 - depth - INFO - Epoch [11][450/1514]	lr: 9.185e-05, eta: 2:18:01, time: 0.341, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0547, loss: 0.0547, grad_norm: 1.8235
2022-06-04 16:44:47,279 - depth - INFO - Epoch [11][500/1514]	lr: 9.168e-05, eta: 2:17:37, time: 0.343, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0540, loss: 0.0540, grad_norm: 1.8729
2022-06-04 16:45:08,701 - depth - INFO - Epoch [11][550/1514]	lr: 9.151e-05, eta: 2:17:19, time: 0.428, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0543, loss: 0.0543, grad_norm: 1.5538
2022-06-04 16:45:30,173 - depth - INFO - Epoch [11][600/1514]	lr: 9.133e-05, eta: 2:17:01, time: 0.430, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0529, loss: 0.0529, grad_norm: 1.8919
2022-06-04 16:45:51,888 - depth - INFO - Epoch [11][650/1514]	lr: 9.116e-05, eta: 2:16:44, time: 0.434, data_time: 0.011, memory: 12384, decode.loss_depth: 0.0535, loss: 0.0535, grad_norm: 1.2943
2022-06-04 16:46:08,912 - depth - INFO - Epoch [11][700/1514]	lr: 9.098e-05, eta: 2:16:20, time: 0.340, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0514, loss: 0.0514, grad_norm: 1.2546
2022-06-04 16:46:30,481 - depth - INFO - Epoch [11][750/1514]	lr: 9.081e-05, eta: 2:16:02, time: 0.431, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0531, loss: 0.0531, grad_norm: 1.7834
2022-06-04 16:46:47,552 - depth - INFO - Epoch [11][800/1514]	lr: 9.063e-05, eta: 2:15:38, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0526, loss: 0.0526, grad_norm: 1.2298
2022-06-04 16:47:04,850 - depth - INFO - Epoch [11][850/1514]	lr: 9.045e-05, eta: 2:15:15, time: 0.346, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0512, loss: 0.0512, grad_norm: 1.9334
2022-06-04 16:47:22,012 - depth - INFO - Epoch [11][900/1514]	lr: 9.026e-05, eta: 2:14:52, time: 0.343, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0522, loss: 0.0522, grad_norm: 1.4250
2022-06-04 16:47:44,664 - depth - INFO - Epoch [11][950/1514]	lr: 9.008e-05, eta: 2:14:35, time: 0.453, data_time: 0.012, memory: 12384, decode.loss_depth: 0.0531, loss: 0.0531, grad_norm: 1.6612
2022-06-04 16:48:05,354 - depth - INFO - Epoch [11][1000/1514]	lr: 8.990e-05, eta: 2:14:16, time: 0.414, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0517, loss: 0.0517, grad_norm: 1.2523
2022-06-04 16:48:26,873 - depth - INFO - Epoch [11][1050/1514]	lr: 8.971e-05, eta: 2:13:58, time: 0.430, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0511, loss: 0.0511, grad_norm: 1.3716
2022-06-04 16:48:48,621 - depth - INFO - Epoch [11][1100/1514]	lr: 8.952e-05, eta: 2:13:40, time: 0.435, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0514, loss: 0.0514, grad_norm: 2.0802
2022-06-04 16:49:14,963 - depth - INFO - Epoch [11][1150/1514]	lr: 8.933e-05, eta: 2:13:28, time: 0.527, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0528, loss: 0.0528, grad_norm: 1.5640
2022-06-04 16:49:36,779 - depth - INFO - Epoch [11][1200/1514]	lr: 8.914e-05, eta: 2:13:10, time: 0.436, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0519, loss: 0.0519, grad_norm: 1.5302
2022-06-04 16:49:53,680 - depth - INFO - Epoch [11][1250/1514]	lr: 8.895e-05, eta: 2:12:47, time: 0.338, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0514, loss: 0.0514, grad_norm: 1.4823
2022-06-04 16:50:14,968 - depth - INFO - Epoch [11][1300/1514]	lr: 8.875e-05, eta: 2:12:28, time: 0.426, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0512, loss: 0.0512, grad_norm: 1.0467
2022-06-04 16:50:41,166 - depth - INFO - Epoch [11][1350/1514]	lr: 8.856e-05, eta: 2:12:16, time: 0.524, data_time: 0.013, memory: 12384, decode.loss_depth: 0.0512, loss: 0.0512, grad_norm: 1.8220
2022-06-04 16:50:58,259 - depth - INFO - Epoch [11][1400/1514]	lr: 8.836e-05, eta: 2:11:52, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0516, loss: 0.0516, grad_norm: 1.0335
2022-06-04 16:51:15,486 - depth - INFO - Epoch [11][1450/1514]	lr: 8.816e-05, eta: 2:11:29, time: 0.344, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0518, loss: 0.0518, grad_norm: 2.2215
2022-06-04 16:51:36,974 - depth - INFO - Epoch [11][1500/1514]	lr: 8.796e-05, eta: 2:11:11, time: 0.430, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0505, loss: 0.0505, grad_norm: 1.6650
2022-06-04 16:51:42,255 - depth - INFO - Saving checkpoint at 11 epochs
2022-06-04 16:52:11,144 - depth - INFO - Summary:
2022-06-04 16:52:11,144 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8501 | 0.9795 | 0.9959 |  0.1404 | 0.4364 | 0.056  |  0.1571  | 10.8993 | 0.0844 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 16:52:11,145 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 16:52:11,145 - depth - INFO - Epoch(val) [11][82]	a1: 0.8501, a2: 0.9795, a3: 0.9959, abs_rel: 0.14039210975170135, rmse: 0.43637266755104065, log_10: 0.05604321137070656, rmse_log: 0.15709559619426727, silog: 10.8993, sq_rel: 0.08437037467956543
2022-06-04 16:52:43,039 - depth - INFO - Epoch [12][50/1514]	lr: 8.770e-05, eta: 2:10:53, time: 0.638, data_time: 0.221, memory: 12384, decode.loss_depth: 0.0517, loss: 0.0517, grad_norm: 2.0888
2022-06-04 16:53:04,826 - depth - INFO - Epoch [12][100/1514]	lr: 8.750e-05, eta: 2:10:35, time: 0.436, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0524, loss: 0.0524, grad_norm: 1.8508
2022-06-04 16:53:26,637 - depth - INFO - Epoch [12][150/1514]	lr: 8.729e-05, eta: 2:10:17, time: 0.436, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0525, loss: 0.0525, grad_norm: 1.9067
2022-06-04 16:53:47,884 - depth - INFO - Epoch [12][200/1514]	lr: 8.709e-05, eta: 2:09:58, time: 0.425, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0527, loss: 0.0527, grad_norm: 1.4603
2022-06-04 16:54:04,906 - depth - INFO - Epoch [12][250/1514]	lr: 8.688e-05, eta: 2:09:35, time: 0.341, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0512, loss: 0.0512, grad_norm: 2.1339
2022-06-04 16:54:26,802 - depth - INFO - Epoch [12][300/1514]	lr: 8.667e-05, eta: 2:09:17, time: 0.438, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0491, loss: 0.0491, grad_norm: 1.5374
2022-06-04 16:54:43,839 - depth - INFO - Epoch [12][350/1514]	lr: 8.646e-05, eta: 2:08:54, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0497, loss: 0.0497, grad_norm: 1.3874
2022-06-04 16:55:05,689 - depth - INFO - Epoch [12][400/1514]	lr: 8.625e-05, eta: 2:08:36, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0497, loss: 0.0497, grad_norm: 1.2642
2022-06-04 16:55:22,590 - depth - INFO - Epoch [12][450/1514]	lr: 8.603e-05, eta: 2:08:12, time: 0.338, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0508, loss: 0.0508, grad_norm: 1.7299
2022-06-04 16:55:39,612 - depth - INFO - Epoch [12][500/1514]	lr: 8.582e-05, eta: 2:07:49, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0499, loss: 0.0499, grad_norm: 1.8391
2022-06-04 16:56:00,987 - depth - INFO - Epoch [12][550/1514]	lr: 8.560e-05, eta: 2:07:30, time: 0.427, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0500, loss: 0.0500, grad_norm: 1.3521
2022-06-04 16:56:22,596 - depth - INFO - Epoch [12][600/1514]	lr: 8.539e-05, eta: 2:07:12, time: 0.432, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0491, loss: 0.0491, grad_norm: 1.6311
2022-06-04 16:56:43,871 - depth - INFO - Epoch [12][650/1514]	lr: 8.517e-05, eta: 2:06:54, time: 0.426, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0492, loss: 0.0492, grad_norm: 1.4062
2022-06-04 16:57:05,698 - depth - INFO - Epoch [12][700/1514]	lr: 8.495e-05, eta: 2:06:36, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0502, loss: 0.0502, grad_norm: 2.1415
2022-06-04 16:57:26,912 - depth - INFO - Epoch [12][750/1514]	lr: 8.473e-05, eta: 2:06:17, time: 0.424, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0496, loss: 0.0496, grad_norm: 1.9717
2022-06-04 16:57:43,916 - depth - INFO - Epoch [12][800/1514]	lr: 8.450e-05, eta: 2:05:54, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0481, loss: 0.0481, grad_norm: 1.8038
2022-06-04 16:58:05,665 - depth - INFO - Epoch [12][850/1514]	lr: 8.428e-05, eta: 2:05:36, time: 0.435, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0483, loss: 0.0483, grad_norm: 1.6533
2022-06-04 16:58:22,664 - depth - INFO - Epoch [12][900/1514]	lr: 8.405e-05, eta: 2:05:12, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0480, loss: 0.0480, grad_norm: 1.7616
2022-06-04 16:58:43,963 - depth - INFO - Epoch [12][950/1514]	lr: 8.383e-05, eta: 2:04:54, time: 0.426, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0490, loss: 0.0490, grad_norm: 1.3383
2022-06-04 16:59:05,575 - depth - INFO - Epoch [12][1000/1514]	lr: 8.360e-05, eta: 2:04:35, time: 0.432, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0485, loss: 0.0485, grad_norm: 1.2599
2022-06-04 16:59:27,003 - depth - INFO - Epoch [12][1050/1514]	lr: 8.337e-05, eta: 2:04:17, time: 0.429, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0474, loss: 0.0474, grad_norm: 1.3694
2022-06-04 16:59:48,806 - depth - INFO - Epoch [12][1100/1514]	lr: 8.314e-05, eta: 2:03:59, time: 0.436, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0496, loss: 0.0496, grad_norm: 1.4553
2022-06-04 17:00:10,814 - depth - INFO - Epoch [12][1150/1514]	lr: 8.291e-05, eta: 2:03:41, time: 0.440, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0502, loss: 0.0502, grad_norm: 2.0419
2022-06-04 17:00:32,658 - depth - INFO - Epoch [12][1200/1514]	lr: 8.267e-05, eta: 2:03:23, time: 0.437, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0476, loss: 0.0476, grad_norm: 1.7807
2022-06-04 17:00:49,781 - depth - INFO - Epoch [12][1250/1514]	lr: 8.244e-05, eta: 2:03:00, time: 0.342, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0493, loss: 0.0493, grad_norm: 1.2913
2022-06-04 17:01:06,991 - depth - INFO - Epoch [12][1300/1514]	lr: 8.220e-05, eta: 2:02:37, time: 0.345, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0477, loss: 0.0477, grad_norm: 0.9676
2022-06-04 17:01:24,005 - depth - INFO - Epoch [12][1350/1514]	lr: 8.197e-05, eta: 2:02:14, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0482, loss: 0.0482, grad_norm: 1.3275
2022-06-04 17:01:45,715 - depth - INFO - Epoch [12][1400/1514]	lr: 8.173e-05, eta: 2:01:55, time: 0.434, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0484, loss: 0.0484, grad_norm: 1.0344
2022-06-04 17:02:02,984 - depth - INFO - Epoch [12][1450/1514]	lr: 8.149e-05, eta: 2:01:33, time: 0.346, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0475, loss: 0.0475, grad_norm: 1.1944
2022-06-04 17:02:19,997 - depth - INFO - Epoch [12][1500/1514]	lr: 8.125e-05, eta: 2:01:10, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0467, loss: 0.0467, grad_norm: 1.3861
2022-06-04 17:02:30,024 - depth - INFO - Saving checkpoint at 12 epochs
2022-06-04 17:02:59,050 - depth - INFO - Summary:
2022-06-04 17:02:59,051 - depth - INFO - 
+--------+-------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2  |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+-------+--------+---------+--------+--------+----------+---------+--------+
| 0.8407 | 0.977 | 0.9951 |  0.149  | 0.4374 | 0.0587 |  0.1631  | 10.8975 | 0.0876 |
+--------+-------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 17:02:59,051 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 17:02:59,051 - depth - INFO - Epoch(val) [12][82]	a1: 0.8407, a2: 0.9770, a3: 0.9951, abs_rel: 0.14896513521671295, rmse: 0.43736210465431213, log_10: 0.058713749051094055, rmse_log: 0.16313518583774567, silog: 10.8975, sq_rel: 0.08757487684488297
2022-06-04 17:03:33,127 - depth - INFO - Epoch [13][50/1514]	lr: 8.094e-05, eta: 2:00:52, time: 0.680, data_time: 0.242, memory: 12384, decode.loss_depth: 0.0474, loss: 0.0474, grad_norm: 1.2025
2022-06-04 17:03:52,728 - depth - INFO - Epoch [13][100/1514]	lr: 8.070e-05, eta: 2:00:32, time: 0.393, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0481, loss: 0.0481, grad_norm: 2.1077
2022-06-04 17:04:14,002 - depth - INFO - Epoch [13][150/1514]	lr: 8.045e-05, eta: 2:00:13, time: 0.426, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0476, loss: 0.0476, grad_norm: 1.5302
2022-06-04 17:04:35,634 - depth - INFO - Epoch [13][200/1514]	lr: 8.021e-05, eta: 1:59:55, time: 0.433, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0489, loss: 0.0489, grad_norm: 1.2574
2022-06-04 17:04:56,915 - depth - INFO - Epoch [13][250/1514]	lr: 7.996e-05, eta: 1:59:36, time: 0.425, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0477, loss: 0.0477, grad_norm: 1.6483
2022-06-04 17:05:18,668 - depth - INFO - Epoch [13][300/1514]	lr: 7.971e-05, eta: 1:59:18, time: 0.435, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0456, loss: 0.0456, grad_norm: 1.7286
2022-06-04 17:05:35,657 - depth - INFO - Epoch [13][350/1514]	lr: 7.946e-05, eta: 1:58:55, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0460, loss: 0.0460, grad_norm: 2.0269
2022-06-04 17:05:56,927 - depth - INFO - Epoch [13][400/1514]	lr: 7.921e-05, eta: 1:58:36, time: 0.425, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0459, loss: 0.0459, grad_norm: 1.2922
2022-06-04 17:06:14,034 - depth - INFO - Epoch [13][450/1514]	lr: 7.896e-05, eta: 1:58:13, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0454, loss: 0.0454, grad_norm: 1.3190
2022-06-04 17:06:30,946 - depth - INFO - Epoch [13][500/1514]	lr: 7.871e-05, eta: 1:57:50, time: 0.338, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0464, loss: 0.0464, grad_norm: 1.3486
2022-06-04 17:06:52,915 - depth - INFO - Epoch [13][550/1514]	lr: 7.846e-05, eta: 1:57:32, time: 0.439, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0489, loss: 0.0489, grad_norm: 2.1408
2022-06-04 17:07:10,147 - depth - INFO - Epoch [13][600/1514]	lr: 7.820e-05, eta: 1:57:09, time: 0.345, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0473, loss: 0.0473, grad_norm: 2.3708
2022-06-04 17:07:27,264 - depth - INFO - Epoch [13][650/1514]	lr: 7.795e-05, eta: 1:56:47, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0471, loss: 0.0471, grad_norm: 1.5976
2022-06-04 17:07:44,100 - depth - INFO - Epoch [13][700/1514]	lr: 7.769e-05, eta: 1:56:24, time: 0.337, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0461, loss: 0.0461, grad_norm: 1.8165
2022-06-04 17:08:05,608 - depth - INFO - Epoch [13][750/1514]	lr: 7.743e-05, eta: 1:56:05, time: 0.430, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0471, loss: 0.0471, grad_norm: 2.0631
2022-06-04 17:08:27,015 - depth - INFO - Epoch [13][800/1514]	lr: 7.718e-05, eta: 1:55:46, time: 0.428, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0461, loss: 0.0461, grad_norm: 1.5952
2022-06-04 17:08:48,783 - depth - INFO - Epoch [13][850/1514]	lr: 7.692e-05, eta: 1:55:28, time: 0.435, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0453, loss: 0.0453, grad_norm: 2.2252
2022-06-04 17:09:05,856 - depth - INFO - Epoch [13][900/1514]	lr: 7.665e-05, eta: 1:55:05, time: 0.342, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0456, loss: 0.0456, grad_norm: 1.2760
2022-06-04 17:09:22,929 - depth - INFO - Epoch [13][950/1514]	lr: 7.639e-05, eta: 1:54:43, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0467, loss: 0.0467, grad_norm: 1.1570
2022-06-04 17:09:39,781 - depth - INFO - Epoch [13][1000/1514]	lr: 7.613e-05, eta: 1:54:20, time: 0.337, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0458, loss: 0.0458, grad_norm: 1.5812
2022-06-04 17:10:01,821 - depth - INFO - Epoch [13][1050/1514]	lr: 7.587e-05, eta: 1:54:02, time: 0.441, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0446, loss: 0.0446, grad_norm: 1.6374
2022-06-04 17:10:18,916 - depth - INFO - Epoch [13][1100/1514]	lr: 7.560e-05, eta: 1:53:39, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0450, loss: 0.0450, grad_norm: 1.4022
2022-06-04 17:10:40,508 - depth - INFO - Epoch [13][1150/1514]	lr: 7.534e-05, eta: 1:53:21, time: 0.432, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0473, loss: 0.0473, grad_norm: 1.1636
2022-06-04 17:11:01,820 - depth - INFO - Epoch [13][1200/1514]	lr: 7.507e-05, eta: 1:53:02, time: 0.426, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0455, loss: 0.0455, grad_norm: 1.5248
2022-06-04 17:11:23,619 - depth - INFO - Epoch [13][1250/1514]	lr: 7.480e-05, eta: 1:52:43, time: 0.436, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0460, loss: 0.0460, grad_norm: 1.2444
2022-06-04 17:11:45,015 - depth - INFO - Epoch [13][1300/1514]	lr: 7.453e-05, eta: 1:52:25, time: 0.428, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0464, loss: 0.0464, grad_norm: 1.5036
2022-06-04 17:12:01,995 - depth - INFO - Epoch [13][1350/1514]	lr: 7.426e-05, eta: 1:52:02, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0453, loss: 0.0453, grad_norm: 1.1597
2022-06-04 17:12:23,757 - depth - INFO - Epoch [13][1400/1514]	lr: 7.399e-05, eta: 1:51:44, time: 0.435, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0462, loss: 0.0462, grad_norm: 1.1548
2022-06-04 17:12:45,718 - depth - INFO - Epoch [13][1450/1514]	lr: 7.372e-05, eta: 1:51:25, time: 0.439, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0450, loss: 0.0450, grad_norm: 1.3249
2022-06-04 17:13:07,668 - depth - INFO - Epoch [13][1500/1514]	lr: 7.345e-05, eta: 1:51:07, time: 0.439, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0440, loss: 0.0440, grad_norm: 0.9980
2022-06-04 17:13:17,362 - depth - INFO - Saving checkpoint at 13 epochs
2022-06-04 17:13:43,841 - depth - INFO - Summary:
2022-06-04 17:13:43,842 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
| 0.8357 | 0.9766 | 0.995 |  0.1504 | 0.4446 | 0.0591 |  0.1645  | 10.8938 | 0.0908 |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
2022-06-04 17:13:43,842 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 17:13:43,842 - depth - INFO - Epoch(val) [13][82]	a1: 0.8357, a2: 0.9766, a3: 0.9950, abs_rel: 0.15038758516311646, rmse: 0.44458577036857605, log_10: 0.0591481477022171, rmse_log: 0.16447696089744568, silog: 10.8938, sq_rel: 0.09079061448574066
2022-06-04 17:14:16,054 - depth - INFO - Epoch [14][50/1514]	lr: 7.310e-05, eta: 1:50:47, time: 0.644, data_time: 0.199, memory: 12384, decode.loss_depth: 0.0440, loss: 0.0440, grad_norm: 1.5407
2022-06-04 17:14:33,130 - depth - INFO - Epoch [14][100/1514]	lr: 7.283e-05, eta: 1:50:24, time: 0.341, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0445, loss: 0.0445, grad_norm: 0.8930
2022-06-04 17:14:50,111 - depth - INFO - Epoch [14][150/1514]	lr: 7.255e-05, eta: 1:50:02, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0445, loss: 0.0445, grad_norm: 1.1293
2022-06-04 17:15:11,913 - depth - INFO - Epoch [14][200/1514]	lr: 7.228e-05, eta: 1:49:43, time: 0.436, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0460, loss: 0.0460, grad_norm: 1.1350
2022-06-04 17:15:33,746 - depth - INFO - Epoch [14][250/1514]	lr: 7.200e-05, eta: 1:49:25, time: 0.437, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0445, loss: 0.0445, grad_norm: 1.4999
2022-06-04 17:15:55,082 - depth - INFO - Epoch [14][300/1514]	lr: 7.172e-05, eta: 1:49:06, time: 0.427, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0441, loss: 0.0441, grad_norm: 1.8895
2022-06-04 17:16:16,853 - depth - INFO - Epoch [14][350/1514]	lr: 7.144e-05, eta: 1:48:47, time: 0.435, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0430, loss: 0.0430, grad_norm: 2.0447
2022-06-04 17:16:42,501 - depth - INFO - Epoch [14][400/1514]	lr: 7.116e-05, eta: 1:48:32, time: 0.513, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0433, loss: 0.0433, grad_norm: 0.8803
2022-06-04 17:16:59,950 - depth - INFO - Epoch [14][450/1514]	lr: 7.088e-05, eta: 1:48:10, time: 0.350, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0445, loss: 0.0445, grad_norm: 1.7112
2022-06-04 17:17:21,483 - depth - INFO - Epoch [14][500/1514]	lr: 7.060e-05, eta: 1:47:51, time: 0.431, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0435, loss: 0.0435, grad_norm: 1.5229
2022-06-04 17:17:38,577 - depth - INFO - Epoch [14][550/1514]	lr: 7.032e-05, eta: 1:47:29, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0443, loss: 0.0443, grad_norm: 1.0607
2022-06-04 17:17:59,878 - depth - INFO - Epoch [14][600/1514]	lr: 7.004e-05, eta: 1:47:10, time: 0.426, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0429, loss: 0.0429, grad_norm: 1.3807
2022-06-04 17:18:16,970 - depth - INFO - Epoch [14][650/1514]	lr: 6.976e-05, eta: 1:46:47, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0436, loss: 0.0436, grad_norm: 1.3149
2022-06-04 17:18:33,810 - depth - INFO - Epoch [14][700/1514]	lr: 6.947e-05, eta: 1:46:25, time: 0.337, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0431, loss: 0.0431, grad_norm: 1.6663
2022-06-04 17:18:50,723 - depth - INFO - Epoch [14][750/1514]	lr: 6.919e-05, eta: 1:46:02, time: 0.338, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0426, loss: 0.0426, grad_norm: 1.4200
2022-06-04 17:19:11,943 - depth - INFO - Epoch [14][800/1514]	lr: 6.890e-05, eta: 1:45:43, time: 0.424, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0426, loss: 0.0426, grad_norm: 1.0364
2022-06-04 17:19:33,633 - depth - INFO - Epoch [14][850/1514]	lr: 6.861e-05, eta: 1:45:25, time: 0.434, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0424, loss: 0.0424, grad_norm: 1.6345
2022-06-04 17:19:50,731 - depth - INFO - Epoch [14][900/1514]	lr: 6.833e-05, eta: 1:45:02, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0426, loss: 0.0426, grad_norm: 1.3545
2022-06-04 17:20:11,989 - depth - INFO - Epoch [14][950/1514]	lr: 6.804e-05, eta: 1:44:43, time: 0.425, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0431, loss: 0.0431, grad_norm: 1.2957
2022-06-04 17:20:28,945 - depth - INFO - Epoch [14][1000/1514]	lr: 6.775e-05, eta: 1:44:21, time: 0.339, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0426, loss: 0.0426, grad_norm: 1.5168
2022-06-04 17:20:46,081 - depth - INFO - Epoch [14][1050/1514]	lr: 6.746e-05, eta: 1:43:59, time: 0.343, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0416, loss: 0.0416, grad_norm: 1.4363
2022-06-04 17:21:07,741 - depth - INFO - Epoch [14][1100/1514]	lr: 6.717e-05, eta: 1:43:40, time: 0.433, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0425, loss: 0.0425, grad_norm: 1.7709
2022-06-04 17:21:28,887 - depth - INFO - Epoch [14][1150/1514]	lr: 6.688e-05, eta: 1:43:21, time: 0.423, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0438, loss: 0.0438, grad_norm: 2.2874
2022-06-04 17:21:50,679 - depth - INFO - Epoch [14][1200/1514]	lr: 6.659e-05, eta: 1:43:02, time: 0.436, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0419, loss: 0.0419, grad_norm: 1.6345
2022-06-04 17:22:12,042 - depth - INFO - Epoch [14][1250/1514]	lr: 6.630e-05, eta: 1:42:43, time: 0.427, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0430, loss: 0.0430, grad_norm: 1.3769
2022-06-04 17:22:29,007 - depth - INFO - Epoch [14][1300/1514]	lr: 6.601e-05, eta: 1:42:21, time: 0.339, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0430, loss: 0.0430, grad_norm: 1.3793
2022-06-04 17:22:46,044 - depth - INFO - Epoch [14][1350/1514]	lr: 6.572e-05, eta: 1:41:59, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0423, loss: 0.0423, grad_norm: 0.9976
2022-06-04 17:23:03,109 - depth - INFO - Epoch [14][1400/1514]	lr: 6.542e-05, eta: 1:41:37, time: 0.341, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0421, loss: 0.0421, grad_norm: 1.2483
2022-06-04 17:23:24,687 - depth - INFO - Epoch [14][1450/1514]	lr: 6.513e-05, eta: 1:41:18, time: 0.431, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0407, loss: 0.0407, grad_norm: 1.2807
2022-06-04 17:23:45,917 - depth - INFO - Epoch [14][1500/1514]	lr: 6.483e-05, eta: 1:40:59, time: 0.425, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0406, loss: 0.0406, grad_norm: 0.8492
2022-06-04 17:23:55,947 - depth - INFO - Saving checkpoint at 14 epochs
2022-06-04 17:24:22,113 - depth - INFO - Summary:
2022-06-04 17:24:22,113 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8566 | 0.9784 | 0.9951 |  0.1396 | 0.4243 | 0.0556 |  0.1567  | 10.9203 | 0.0819 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 17:24:22,114 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 17:24:22,114 - depth - INFO - Epoch(val) [14][82]	a1: 0.8566, a2: 0.9784, a3: 0.9951, abs_rel: 0.13962367177009583, rmse: 0.4242885112762451, log_10: 0.05555671453475952, rmse_log: 0.15673859417438507, silog: 10.9203, sq_rel: 0.0819280818104744
2022-06-04 17:24:52,657 - depth - INFO - Epoch [15][50/1514]	lr: 6.446e-05, eta: 1:40:37, time: 0.611, data_time: 0.214, memory: 12384, decode.loss_depth: 0.0406, loss: 0.0406, grad_norm: 0.9030
2022-06-04 17:25:13,911 - depth - INFO - Epoch [15][100/1514]	lr: 6.416e-05, eta: 1:40:18, time: 0.425, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0420, loss: 0.0420, grad_norm: 1.4595
2022-06-04 17:25:31,007 - depth - INFO - Epoch [15][150/1514]	lr: 6.386e-05, eta: 1:39:56, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0426, loss: 0.0426, grad_norm: 1.9764
2022-06-04 17:25:52,729 - depth - INFO - Epoch [15][200/1514]	lr: 6.357e-05, eta: 1:39:37, time: 0.435, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0425, loss: 0.0425, grad_norm: 1.1264
2022-06-04 17:26:13,852 - depth - INFO - Epoch [15][250/1514]	lr: 6.327e-05, eta: 1:39:18, time: 0.423, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0413, loss: 0.0413, grad_norm: 1.2647
2022-06-04 17:26:30,835 - depth - INFO - Epoch [15][300/1514]	lr: 6.297e-05, eta: 1:38:55, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0406, loss: 0.0406, grad_norm: 1.5993
2022-06-04 17:26:47,889 - depth - INFO - Epoch [15][350/1514]	lr: 6.267e-05, eta: 1:38:33, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0410, loss: 0.0410, grad_norm: 2.3244
2022-06-04 17:27:04,931 - depth - INFO - Epoch [15][400/1514]	lr: 6.238e-05, eta: 1:38:11, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0418, loss: 0.0418, grad_norm: 2.3086
2022-06-04 17:27:21,853 - depth - INFO - Epoch [15][450/1514]	lr: 6.208e-05, eta: 1:37:49, time: 0.338, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0414, loss: 0.0414, grad_norm: 1.8053
2022-06-04 17:27:38,874 - depth - INFO - Epoch [15][500/1514]	lr: 6.178e-05, eta: 1:37:27, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0415, loss: 0.0415, grad_norm: 1.6790
2022-06-04 17:27:55,967 - depth - INFO - Epoch [15][550/1514]	lr: 6.148e-05, eta: 1:37:05, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0432, loss: 0.0432, grad_norm: 1.2478
2022-06-04 17:28:13,312 - depth - INFO - Epoch [15][600/1514]	lr: 6.118e-05, eta: 1:36:44, time: 0.347, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0409, loss: 0.0409, grad_norm: 1.3890
2022-06-04 17:28:30,290 - depth - INFO - Epoch [15][650/1514]	lr: 6.087e-05, eta: 1:36:22, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0432, loss: 0.0432, grad_norm: 1.2130
2022-06-04 17:28:51,755 - depth - INFO - Epoch [15][700/1514]	lr: 6.057e-05, eta: 1:36:03, time: 0.429, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0413, loss: 0.0413, grad_norm: 1.4238
2022-06-04 17:29:12,896 - depth - INFO - Epoch [15][750/1514]	lr: 6.027e-05, eta: 1:35:44, time: 0.423, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0408, loss: 0.0408, grad_norm: 1.6634
2022-06-04 17:29:34,702 - depth - INFO - Epoch [15][800/1514]	lr: 5.997e-05, eta: 1:35:25, time: 0.436, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0412, loss: 0.0412, grad_norm: 1.2265
2022-06-04 17:29:55,855 - depth - INFO - Epoch [15][850/1514]	lr: 5.967e-05, eta: 1:35:06, time: 0.423, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0398, loss: 0.0398, grad_norm: 1.1258
2022-06-04 17:30:17,672 - depth - INFO - Epoch [15][900/1514]	lr: 5.936e-05, eta: 1:34:47, time: 0.437, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0406, loss: 0.0406, grad_norm: 1.2845
2022-06-04 17:30:34,801 - depth - INFO - Epoch [15][950/1514]	lr: 5.906e-05, eta: 1:34:25, time: 0.343, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0416, loss: 0.0416, grad_norm: 1.3185
2022-06-04 17:30:51,798 - depth - INFO - Epoch [15][1000/1514]	lr: 5.876e-05, eta: 1:34:03, time: 0.340, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0402, loss: 0.0402, grad_norm: 1.1124
2022-06-04 17:31:08,922 - depth - INFO - Epoch [15][1050/1514]	lr: 5.845e-05, eta: 1:33:42, time: 0.343, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0399, loss: 0.0399, grad_norm: 1.4030
2022-06-04 17:31:33,100 - depth - INFO - Epoch [15][1100/1514]	lr: 5.815e-05, eta: 1:33:24, time: 0.483, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0406, loss: 0.0406, grad_norm: 1.8608
2022-06-04 17:31:51,882 - depth - INFO - Epoch [15][1150/1514]	lr: 5.784e-05, eta: 1:33:04, time: 0.376, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0409, loss: 0.0409, grad_norm: 1.0894
2022-06-04 17:32:13,741 - depth - INFO - Epoch [15][1200/1514]	lr: 5.754e-05, eta: 1:32:45, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0395, loss: 0.0395, grad_norm: 1.0959
2022-06-04 17:32:30,711 - depth - INFO - Epoch [15][1250/1514]	lr: 5.723e-05, eta: 1:32:23, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0402, loss: 0.0402, grad_norm: 1.0567
2022-06-04 17:32:47,912 - depth - INFO - Epoch [15][1300/1514]	lr: 5.693e-05, eta: 1:32:01, time: 0.344, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0407, loss: 0.0407, grad_norm: 1.7047
2022-06-04 17:33:09,669 - depth - INFO - Epoch [15][1350/1514]	lr: 5.662e-05, eta: 1:31:42, time: 0.435, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0394, loss: 0.0394, grad_norm: 1.3405
2022-06-04 17:33:30,843 - depth - INFO - Epoch [15][1400/1514]	lr: 5.631e-05, eta: 1:31:23, time: 0.424, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0394, loss: 0.0394, grad_norm: 0.8850
2022-06-04 17:33:48,053 - depth - INFO - Epoch [15][1450/1514]	lr: 5.601e-05, eta: 1:31:02, time: 0.344, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0383, loss: 0.0383, grad_norm: 1.0772
2022-06-04 17:34:09,678 - depth - INFO - Epoch [15][1500/1514]	lr: 5.570e-05, eta: 1:30:43, time: 0.432, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0383, loss: 0.0383, grad_norm: 1.4394
2022-06-04 17:34:19,181 - depth - INFO - Saving checkpoint at 15 epochs
2022-06-04 17:34:46,090 - depth - INFO - Summary:
2022-06-04 17:34:46,091 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8455 | 0.9758 | 0.9942 |  0.1462 | 0.4294 | 0.0575 |  0.1614  | 10.9589 | 0.0874 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 17:34:46,091 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 17:34:46,091 - depth - INFO - Epoch(val) [15][82]	a1: 0.8455, a2: 0.9758, a3: 0.9942, abs_rel: 0.14616413414478302, rmse: 0.4294281005859375, log_10: 0.05753461644053459, rmse_log: 0.16140486299991608, silog: 10.9589, sq_rel: 0.08739881962537766
2022-06-04 17:35:19,027 - depth - INFO - Epoch [16][50/1514]	lr: 5.531e-05, eta: 1:30:22, time: 0.658, data_time: 0.206, memory: 12384, decode.loss_depth: 0.0394, loss: 0.0394, grad_norm: 1.7029
2022-06-04 17:35:40,480 - depth - INFO - Epoch [16][100/1514]	lr: 5.500e-05, eta: 1:30:02, time: 0.429, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0397, loss: 0.0397, grad_norm: 1.4334
2022-06-04 17:36:01,820 - depth - INFO - Epoch [16][150/1514]	lr: 5.469e-05, eta: 1:29:43, time: 0.427, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0388, loss: 0.0388, grad_norm: 1.1367
2022-06-04 17:36:18,944 - depth - INFO - Epoch [16][200/1514]	lr: 5.439e-05, eta: 1:29:22, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0400, loss: 0.0400, grad_norm: 1.3704
2022-06-04 17:36:40,835 - depth - INFO - Epoch [16][250/1514]	lr: 5.408e-05, eta: 1:29:03, time: 0.438, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0395, loss: 0.0395, grad_norm: 0.9587
2022-06-04 17:37:02,691 - depth - INFO - Epoch [16][300/1514]	lr: 5.377e-05, eta: 1:28:44, time: 0.437, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0379, loss: 0.0379, grad_norm: 1.1444
2022-06-04 17:37:19,846 - depth - INFO - Epoch [16][350/1514]	lr: 5.346e-05, eta: 1:28:22, time: 0.343, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0376, loss: 0.0376, grad_norm: 1.1377
2022-06-04 17:37:36,661 - depth - INFO - Epoch [16][400/1514]	lr: 5.316e-05, eta: 1:28:01, time: 0.336, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0383, loss: 0.0383, grad_norm: 1.2828
2022-06-04 17:37:53,516 - depth - INFO - Epoch [16][450/1514]	lr: 5.285e-05, eta: 1:27:39, time: 0.337, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0388, loss: 0.0388, grad_norm: 1.2684
2022-06-04 17:38:10,638 - depth - INFO - Epoch [16][500/1514]	lr: 5.254e-05, eta: 1:27:17, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0392, loss: 0.0392, grad_norm: 1.5705
2022-06-04 17:38:27,539 - depth - INFO - Epoch [16][550/1514]	lr: 5.223e-05, eta: 1:26:56, time: 0.338, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0396, loss: 0.0396, grad_norm: 0.8415
2022-06-04 17:38:44,540 - depth - INFO - Epoch [16][600/1514]	lr: 5.192e-05, eta: 1:26:34, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0388, loss: 0.0388, grad_norm: 1.3097
2022-06-04 17:39:01,633 - depth - INFO - Epoch [16][650/1514]	lr: 5.161e-05, eta: 1:26:12, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0384, loss: 0.0384, grad_norm: 1.1329
2022-06-04 17:39:18,883 - depth - INFO - Epoch [16][700/1514]	lr: 5.131e-05, eta: 1:25:51, time: 0.345, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0379, loss: 0.0379, grad_norm: 1.5054
2022-06-04 17:39:35,774 - depth - INFO - Epoch [16][750/1514]	lr: 5.100e-05, eta: 1:25:29, time: 0.338, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0376, loss: 0.0376, grad_norm: 1.1516
2022-06-04 17:39:57,288 - depth - INFO - Epoch [16][800/1514]	lr: 5.069e-05, eta: 1:25:10, time: 0.430, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0375, loss: 0.0375, grad_norm: 1.1134
2022-06-04 17:40:14,376 - depth - INFO - Epoch [16][850/1514]	lr: 5.038e-05, eta: 1:24:49, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0379, loss: 0.0379, grad_norm: 1.7845
2022-06-04 17:40:31,305 - depth - INFO - Epoch [16][900/1514]	lr: 5.007e-05, eta: 1:24:27, time: 0.338, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0375, loss: 0.0375, grad_norm: 1.4701
2022-06-04 17:40:52,742 - depth - INFO - Epoch [16][950/1514]	lr: 4.976e-05, eta: 1:24:08, time: 0.429, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0380, loss: 0.0380, grad_norm: 1.2357
2022-06-04 17:41:13,993 - depth - INFO - Epoch [16][1000/1514]	lr: 4.945e-05, eta: 1:23:49, time: 0.425, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0375, loss: 0.0375, grad_norm: 1.2396
2022-06-04 17:41:35,843 - depth - INFO - Epoch [16][1050/1514]	lr: 4.915e-05, eta: 1:23:30, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0364, loss: 0.0364, grad_norm: 1.2269
2022-06-04 17:41:57,608 - depth - INFO - Epoch [16][1100/1514]	lr: 4.884e-05, eta: 1:23:11, time: 0.435, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0381, loss: 0.0381, grad_norm: 1.0345
2022-06-04 17:42:23,924 - depth - INFO - Epoch [16][1150/1514]	lr: 4.853e-05, eta: 1:22:55, time: 0.526, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0384, loss: 0.0384, grad_norm: 1.1235
2022-06-04 17:42:45,639 - depth - INFO - Epoch [16][1200/1514]	lr: 4.822e-05, eta: 1:22:36, time: 0.434, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0371, loss: 0.0371, grad_norm: 1.2667
2022-06-04 17:43:02,612 - depth - INFO - Epoch [16][1250/1514]	lr: 4.791e-05, eta: 1:22:14, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0377, loss: 0.0377, grad_norm: 0.7738
2022-06-04 17:43:19,647 - depth - INFO - Epoch [16][1300/1514]	lr: 4.760e-05, eta: 1:21:53, time: 0.341, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0378, loss: 0.0378, grad_norm: 1.0216
2022-06-04 17:43:36,723 - depth - INFO - Epoch [16][1350/1514]	lr: 4.730e-05, eta: 1:21:31, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0377, loss: 0.0377, grad_norm: 1.2148
2022-06-04 17:43:58,088 - depth - INFO - Epoch [16][1400/1514]	lr: 4.699e-05, eta: 1:21:12, time: 0.427, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0371, loss: 0.0371, grad_norm: 1.1089
2022-06-04 17:44:14,965 - depth - INFO - Epoch [16][1450/1514]	lr: 4.668e-05, eta: 1:20:51, time: 0.337, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0369, loss: 0.0369, grad_norm: 1.3728
2022-06-04 17:44:36,804 - depth - INFO - Epoch [16][1500/1514]	lr: 4.637e-05, eta: 1:20:32, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0363, loss: 0.0363, grad_norm: 0.9898
2022-06-04 17:44:42,291 - depth - INFO - Saving checkpoint at 16 epochs
2022-06-04 17:45:08,299 - depth - INFO - Summary:
2022-06-04 17:45:08,300 - depth - INFO - 
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1  |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.856 | 0.9776 | 0.9946 |  0.1401 | 0.4222 | 0.0555 |  0.1569  | 10.8926 | 0.0842 |
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 17:45:08,300 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 17:45:08,300 - depth - INFO - Epoch(val) [16][82]	a1: 0.8560, a2: 0.9776, a3: 0.9946, abs_rel: 0.1400563269853592, rmse: 0.42219510674476624, log_10: 0.05547843873500824, rmse_log: 0.1569119393825531, silog: 10.8926, sq_rel: 0.08423372358083725
2022-06-04 17:45:39,147 - depth - INFO - Epoch [17][50/1514]	lr: 4.598e-05, eta: 1:20:09, time: 0.617, data_time: 0.242, memory: 12384, decode.loss_depth: 0.0369, loss: 0.0369, grad_norm: 0.9127
2022-06-04 17:45:56,388 - depth - INFO - Epoch [17][100/1514]	lr: 4.567e-05, eta: 1:19:48, time: 0.345, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0376, loss: 0.0376, grad_norm: 1.0556
2022-06-04 17:46:13,760 - depth - INFO - Epoch [17][150/1514]	lr: 4.536e-05, eta: 1:19:26, time: 0.347, data_time: 0.013, memory: 12384, decode.loss_depth: 0.0372, loss: 0.0372, grad_norm: 0.8662
2022-06-04 17:46:35,054 - depth - INFO - Epoch [17][200/1514]	lr: 4.506e-05, eta: 1:19:07, time: 0.426, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0377, loss: 0.0377, grad_norm: 0.9531
2022-06-04 17:46:56,794 - depth - INFO - Epoch [17][250/1514]	lr: 4.475e-05, eta: 1:18:48, time: 0.435, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0369, loss: 0.0369, grad_norm: 1.1502
2022-06-04 17:47:18,710 - depth - INFO - Epoch [17][300/1514]	lr: 4.444e-05, eta: 1:18:29, time: 0.438, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0364, loss: 0.0364, grad_norm: 1.2758
2022-06-04 17:47:39,876 - depth - INFO - Epoch [17][350/1514]	lr: 4.413e-05, eta: 1:18:10, time: 0.423, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 1.1150
2022-06-04 17:47:56,937 - depth - INFO - Epoch [17][400/1514]	lr: 4.383e-05, eta: 1:17:48, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 1.0846
2022-06-04 17:48:14,097 - depth - INFO - Epoch [17][450/1514]	lr: 4.352e-05, eta: 1:17:27, time: 0.343, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0364, loss: 0.0364, grad_norm: 1.4971
2022-06-04 17:48:31,206 - depth - INFO - Epoch [17][500/1514]	lr: 4.322e-05, eta: 1:17:06, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 1.1128
2022-06-04 17:48:52,727 - depth - INFO - Epoch [17][550/1514]	lr: 4.291e-05, eta: 1:16:47, time: 0.430, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0378, loss: 0.0378, grad_norm: 1.6773
2022-06-04 17:49:14,048 - depth - INFO - Epoch [17][600/1514]	lr: 4.261e-05, eta: 1:16:28, time: 0.427, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0361, loss: 0.0361, grad_norm: 1.0440
2022-06-04 17:49:31,150 - depth - INFO - Epoch [17][650/1514]	lr: 4.230e-05, eta: 1:16:06, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 1.0268
2022-06-04 17:49:52,718 - depth - INFO - Epoch [17][700/1514]	lr: 4.200e-05, eta: 1:15:47, time: 0.431, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 1.0602
2022-06-04 17:50:13,735 - depth - INFO - Epoch [17][750/1514]	lr: 4.169e-05, eta: 1:15:28, time: 0.420, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0357, loss: 0.0357, grad_norm: 0.9447
2022-06-04 17:50:30,693 - depth - INFO - Epoch [17][800/1514]	lr: 4.139e-05, eta: 1:15:06, time: 0.339, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0356, loss: 0.0356, grad_norm: 0.6410
2022-06-04 17:50:47,794 - depth - INFO - Epoch [17][850/1514]	lr: 4.108e-05, eta: 1:14:45, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 1.1676
2022-06-04 17:51:04,782 - depth - INFO - Epoch [17][900/1514]	lr: 4.078e-05, eta: 1:14:24, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 0.9574
2022-06-04 17:51:26,487 - depth - INFO - Epoch [17][950/1514]	lr: 4.048e-05, eta: 1:14:05, time: 0.434, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 1.1943
2022-06-04 17:51:43,610 - depth - INFO - Epoch [17][1000/1514]	lr: 4.017e-05, eta: 1:13:44, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 1.1564
2022-06-04 17:52:05,046 - depth - INFO - Epoch [17][1050/1514]	lr: 3.987e-05, eta: 1:13:25, time: 0.429, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0351, loss: 0.0351, grad_norm: 0.8523
2022-06-04 17:52:26,801 - depth - INFO - Epoch [17][1100/1514]	lr: 3.957e-05, eta: 1:13:05, time: 0.435, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0353, loss: 0.0353, grad_norm: 1.2039
2022-06-04 17:52:48,849 - depth - INFO - Epoch [17][1150/1514]	lr: 3.927e-05, eta: 1:12:46, time: 0.441, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0360, loss: 0.0360, grad_norm: 0.7935
2022-06-04 17:53:06,085 - depth - INFO - Epoch [17][1200/1514]	lr: 3.897e-05, eta: 1:12:25, time: 0.345, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0347, loss: 0.0347, grad_norm: 1.8360
2022-06-04 17:53:27,879 - depth - INFO - Epoch [17][1250/1514]	lr: 3.867e-05, eta: 1:12:06, time: 0.436, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0359, loss: 0.0359, grad_norm: 1.6944
2022-06-04 17:53:45,075 - depth - INFO - Epoch [17][1300/1514]	lr: 3.836e-05, eta: 1:11:45, time: 0.344, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0354, loss: 0.0354, grad_norm: 1.0377
2022-06-04 17:54:02,188 - depth - INFO - Epoch [17][1350/1514]	lr: 3.806e-05, eta: 1:11:24, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0358, loss: 0.0358, grad_norm: 0.8302
2022-06-04 17:54:19,293 - depth - INFO - Epoch [17][1400/1514]	lr: 3.777e-05, eta: 1:11:03, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0350, loss: 0.0350, grad_norm: 0.8189
2022-06-04 17:54:40,967 - depth - INFO - Epoch [17][1450/1514]	lr: 3.747e-05, eta: 1:10:44, time: 0.434, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0345, loss: 0.0345, grad_norm: 0.7708
2022-06-04 17:55:02,649 - depth - INFO - Epoch [17][1500/1514]	lr: 3.717e-05, eta: 1:10:25, time: 0.433, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0346, loss: 0.0346, grad_norm: 1.1169
2022-06-04 17:55:07,997 - depth - INFO - Saving checkpoint at 17 epochs
2022-06-04 17:55:33,885 - depth - INFO - Summary:
2022-06-04 17:55:33,886 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8648 | 0.9794 | 0.9954 |  0.1327 | 0.4087 | 0.0532 |  0.1516  | 10.9171 | 0.0762 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 17:55:33,886 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 17:55:33,886 - depth - INFO - Epoch(val) [17][82]	a1: 0.8648, a2: 0.9794, a3: 0.9954, abs_rel: 0.1326642632484436, rmse: 0.40868866443634033, log_10: 0.05315934494137764, rmse_log: 0.1516088992357254, silog: 10.9171, sq_rel: 0.0762043371796608
2022-06-04 17:56:04,801 - depth - INFO - Epoch [18][50/1514]	lr: 3.679e-05, eta: 1:10:01, time: 0.618, data_time: 0.230, memory: 12384, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 1.1770
2022-06-04 17:56:25,986 - depth - INFO - Epoch [18][100/1514]	lr: 3.649e-05, eta: 1:09:42, time: 0.424, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0350, loss: 0.0350, grad_norm: 0.7131
2022-06-04 17:56:42,960 - depth - INFO - Epoch [18][150/1514]	lr: 3.619e-05, eta: 1:09:21, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0348, loss: 0.0348, grad_norm: 0.9184
2022-06-04 17:57:04,607 - depth - INFO - Epoch [18][200/1514]	lr: 3.590e-05, eta: 1:09:02, time: 0.433, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0360, loss: 0.0360, grad_norm: 0.7336
2022-06-04 17:57:25,997 - depth - INFO - Epoch [18][250/1514]	lr: 3.560e-05, eta: 1:08:42, time: 0.428, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0352, loss: 0.0352, grad_norm: 0.7886
2022-06-04 17:57:42,985 - depth - INFO - Epoch [18][300/1514]	lr: 3.530e-05, eta: 1:08:21, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0340, loss: 0.0340, grad_norm: 1.0735
2022-06-04 17:58:04,680 - depth - INFO - Epoch [18][350/1514]	lr: 3.501e-05, eta: 1:08:02, time: 0.434, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0340, loss: 0.0340, grad_norm: 0.9647
2022-06-04 17:58:21,963 - depth - INFO - Epoch [18][400/1514]	lr: 3.472e-05, eta: 1:07:41, time: 0.346, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0343, loss: 0.0343, grad_norm: 1.1267
2022-06-04 17:58:43,701 - depth - INFO - Epoch [18][450/1514]	lr: 3.442e-05, eta: 1:07:22, time: 0.435, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0343, loss: 0.0343, grad_norm: 1.3230
2022-06-04 17:59:04,899 - depth - INFO - Epoch [18][500/1514]	lr: 3.413e-05, eta: 1:07:02, time: 0.424, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 1.5256
2022-06-04 17:59:22,027 - depth - INFO - Epoch [18][550/1514]	lr: 3.384e-05, eta: 1:06:42, time: 0.343, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0350, loss: 0.0350, grad_norm: 1.3195
2022-06-04 17:59:43,656 - depth - INFO - Epoch [18][600/1514]	lr: 3.355e-05, eta: 1:06:22, time: 0.433, data_time: 0.011, memory: 12384, decode.loss_depth: 0.0336, loss: 0.0336, grad_norm: 1.3155
2022-06-04 18:00:00,779 - depth - INFO - Epoch [18][650/1514]	lr: 3.325e-05, eta: 1:06:01, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0341, loss: 0.0341, grad_norm: 1.5082
2022-06-04 18:00:17,956 - depth - INFO - Epoch [18][700/1514]	lr: 3.296e-05, eta: 1:05:40, time: 0.344, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0337, loss: 0.0337, grad_norm: 0.9344
2022-06-04 18:00:35,035 - depth - INFO - Epoch [18][750/1514]	lr: 3.267e-05, eta: 1:05:19, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0334, loss: 0.0334, grad_norm: 0.8466
2022-06-04 18:00:52,002 - depth - INFO - Epoch [18][800/1514]	lr: 3.238e-05, eta: 1:04:58, time: 0.339, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0336, loss: 0.0336, grad_norm: 0.8492
2022-06-04 18:01:13,826 - depth - INFO - Epoch [18][850/1514]	lr: 3.210e-05, eta: 1:04:39, time: 0.436, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0334, loss: 0.0334, grad_norm: 1.2665
2022-06-04 18:01:31,090 - depth - INFO - Epoch [18][900/1514]	lr: 3.181e-05, eta: 1:04:18, time: 0.345, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0337, loss: 0.0337, grad_norm: 0.9897
2022-06-04 18:01:48,195 - depth - INFO - Epoch [18][950/1514]	lr: 3.152e-05, eta: 1:03:57, time: 0.342, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0342, loss: 0.0342, grad_norm: 0.8909
2022-06-04 18:02:09,697 - depth - INFO - Epoch [18][1000/1514]	lr: 3.124e-05, eta: 1:03:38, time: 0.430, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0337, loss: 0.0337, grad_norm: 1.3958
2022-06-04 18:02:26,905 - depth - INFO - Epoch [18][1050/1514]	lr: 3.095e-05, eta: 1:03:17, time: 0.344, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0329, loss: 0.0329, grad_norm: 1.0760
2022-06-04 18:02:48,714 - depth - INFO - Epoch [18][1100/1514]	lr: 3.066e-05, eta: 1:02:58, time: 0.436, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.9189
2022-06-04 18:03:09,976 - depth - INFO - Epoch [18][1150/1514]	lr: 3.038e-05, eta: 1:02:39, time: 0.425, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 0.7571
2022-06-04 18:03:31,687 - depth - INFO - Epoch [18][1200/1514]	lr: 3.010e-05, eta: 1:02:19, time: 0.434, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0329, loss: 0.0329, grad_norm: 0.8343
2022-06-04 18:03:48,736 - depth - INFO - Epoch [18][1250/1514]	lr: 2.981e-05, eta: 1:01:59, time: 0.341, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0344, loss: 0.0344, grad_norm: 1.2023
2022-06-04 18:04:05,748 - depth - INFO - Epoch [18][1300/1514]	lr: 2.953e-05, eta: 1:01:38, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0335, loss: 0.0335, grad_norm: 1.0005
2022-06-04 18:04:27,055 - depth - INFO - Epoch [18][1350/1514]	lr: 2.925e-05, eta: 1:01:18, time: 0.426, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0333, loss: 0.0333, grad_norm: 0.8829
2022-06-04 18:04:44,240 - depth - INFO - Epoch [18][1400/1514]	lr: 2.897e-05, eta: 1:00:57, time: 0.344, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0332, loss: 0.0332, grad_norm: 0.7351
2022-06-04 18:05:05,934 - depth - INFO - Epoch [18][1450/1514]	lr: 2.869e-05, eta: 1:00:38, time: 0.434, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0328, loss: 0.0328, grad_norm: 1.1559
2022-06-04 18:05:27,601 - depth - INFO - Epoch [18][1500/1514]	lr: 2.841e-05, eta: 1:00:19, time: 0.433, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0325, loss: 0.0325, grad_norm: 0.8910
2022-06-04 18:05:33,121 - depth - INFO - Saving checkpoint at 18 epochs
2022-06-04 18:06:00,250 - depth - INFO - Summary:
2022-06-04 18:06:00,251 - depth - INFO - 
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
| 0.8578 | 0.9782 | 0.995 |  0.1379 | 0.4176 | 0.0549 |  0.1553  | 10.9223 | 0.0806 |
+--------+--------+-------+---------+--------+--------+----------+---------+--------+
2022-06-04 18:06:00,251 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 18:06:00,252 - depth - INFO - Epoch(val) [18][82]	a1: 0.8578, a2: 0.9782, a3: 0.9950, abs_rel: 0.13788257539272308, rmse: 0.4176085591316223, log_10: 0.0548689030110836, rmse_log: 0.15530462563037872, silog: 10.9223, sq_rel: 0.08055027574300766
2022-06-04 18:06:32,069 - depth - INFO - Epoch [19][50/1514]	lr: 2.806e-05, eta: 0:59:55, time: 0.636, data_time: 0.238, memory: 12384, decode.loss_depth: 0.0332, loss: 0.0332, grad_norm: 0.7080
2022-06-04 18:06:49,345 - depth - INFO - Epoch [19][100/1514]	lr: 2.778e-05, eta: 0:59:35, time: 0.346, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0335, loss: 0.0335, grad_norm: 0.8310
2022-06-04 18:07:06,574 - depth - INFO - Epoch [19][150/1514]	lr: 2.750e-05, eta: 0:59:14, time: 0.344, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0333, loss: 0.0333, grad_norm: 0.8368
2022-06-04 18:07:27,949 - depth - INFO - Epoch [19][200/1514]	lr: 2.723e-05, eta: 0:58:55, time: 0.427, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0339, loss: 0.0339, grad_norm: 0.6225
2022-06-04 18:07:44,960 - depth - INFO - Epoch [19][250/1514]	lr: 2.695e-05, eta: 0:58:34, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0332, loss: 0.0332, grad_norm: 0.8160
2022-06-04 18:08:06,643 - depth - INFO - Epoch [19][300/1514]	lr: 2.668e-05, eta: 0:58:14, time: 0.433, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0320, loss: 0.0320, grad_norm: 0.5762
2022-06-04 18:08:23,690 - depth - INFO - Epoch [19][350/1514]	lr: 2.641e-05, eta: 0:57:54, time: 0.341, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0318, loss: 0.0318, grad_norm: 0.7031
2022-06-04 18:08:44,950 - depth - INFO - Epoch [19][400/1514]	lr: 2.614e-05, eta: 0:57:34, time: 0.425, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0325, loss: 0.0325, grad_norm: 0.8252
2022-06-04 18:09:01,975 - depth - INFO - Epoch [19][450/1514]	lr: 2.587e-05, eta: 0:57:13, time: 0.341, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0329, loss: 0.0329, grad_norm: 0.8334
2022-06-04 18:09:19,062 - depth - INFO - Epoch [19][500/1514]	lr: 2.560e-05, eta: 0:56:53, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0326, loss: 0.0326, grad_norm: 1.0131
2022-06-04 18:09:36,036 - depth - INFO - Epoch [19][550/1514]	lr: 2.533e-05, eta: 0:56:32, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0336, loss: 0.0336, grad_norm: 0.7134
2022-06-04 18:09:57,724 - depth - INFO - Epoch [19][600/1514]	lr: 2.506e-05, eta: 0:56:13, time: 0.434, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0321, loss: 0.0321, grad_norm: 0.8376
2022-06-04 18:10:19,059 - depth - INFO - Epoch [19][650/1514]	lr: 2.479e-05, eta: 0:55:53, time: 0.427, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0322, loss: 0.0322, grad_norm: 0.8777
2022-06-04 18:10:36,107 - depth - INFO - Epoch [19][700/1514]	lr: 2.453e-05, eta: 0:55:32, time: 0.341, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0322, loss: 0.0322, grad_norm: 1.1243
2022-06-04 18:10:53,127 - depth - INFO - Epoch [19][750/1514]	lr: 2.426e-05, eta: 0:55:12, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0324, loss: 0.0324, grad_norm: 0.9638
2022-06-04 18:11:10,097 - depth - INFO - Epoch [19][800/1514]	lr: 2.400e-05, eta: 0:54:51, time: 0.339, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0321, loss: 0.0321, grad_norm: 0.8573
2022-06-04 18:11:27,173 - depth - INFO - Epoch [19][850/1514]	lr: 2.373e-05, eta: 0:54:30, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0317, loss: 0.0317, grad_norm: 0.8086
2022-06-04 18:11:44,276 - depth - INFO - Epoch [19][900/1514]	lr: 2.347e-05, eta: 0:54:10, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0320, loss: 0.0320, grad_norm: 0.8926
2022-06-04 18:12:05,896 - depth - INFO - Epoch [19][950/1514]	lr: 2.321e-05, eta: 0:53:50, time: 0.432, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0326, loss: 0.0326, grad_norm: 0.8076
2022-06-04 18:12:27,838 - depth - INFO - Epoch [19][1000/1514]	lr: 2.295e-05, eta: 0:53:31, time: 0.439, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0318, loss: 0.0318, grad_norm: 0.6649
2022-06-04 18:12:44,936 - depth - INFO - Epoch [19][1050/1514]	lr: 2.269e-05, eta: 0:53:10, time: 0.342, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0315, loss: 0.0315, grad_norm: 0.6096
2022-06-04 18:13:06,739 - depth - INFO - Epoch [19][1100/1514]	lr: 2.244e-05, eta: 0:52:51, time: 0.436, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0319, loss: 0.0319, grad_norm: 0.6332
2022-06-04 18:13:28,096 - depth - INFO - Epoch [19][1150/1514]	lr: 2.218e-05, eta: 0:52:32, time: 0.427, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0327, loss: 0.0327, grad_norm: 0.8055
2022-06-04 18:13:49,793 - depth - INFO - Epoch [19][1200/1514]	lr: 2.192e-05, eta: 0:52:12, time: 0.434, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0313, loss: 0.0313, grad_norm: 0.7899
2022-06-04 18:14:11,034 - depth - INFO - Epoch [19][1250/1514]	lr: 2.167e-05, eta: 0:51:53, time: 0.425, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0325, loss: 0.0325, grad_norm: 0.7485
2022-06-04 18:14:32,817 - depth - INFO - Epoch [19][1300/1514]	lr: 2.141e-05, eta: 0:51:33, time: 0.436, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0320, loss: 0.0320, grad_norm: 0.7139
2022-06-04 18:14:54,615 - depth - INFO - Epoch [19][1350/1514]	lr: 2.116e-05, eta: 0:51:14, time: 0.436, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0317, loss: 0.0317, grad_norm: 0.8937
2022-06-04 18:15:15,929 - depth - INFO - Epoch [19][1400/1514]	lr: 2.091e-05, eta: 0:50:54, time: 0.426, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0315, loss: 0.0315, grad_norm: 0.5727
2022-06-04 18:15:37,678 - depth - INFO - Epoch [19][1450/1514]	lr: 2.066e-05, eta: 0:50:35, time: 0.435, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0311, loss: 0.0311, grad_norm: 0.8833
2022-06-04 18:15:54,750 - depth - INFO - Epoch [19][1500/1514]	lr: 2.041e-05, eta: 0:50:14, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0310, loss: 0.0310, grad_norm: 0.9942
2022-06-04 18:16:04,852 - depth - INFO - Saving checkpoint at 19 epochs
2022-06-04 18:16:31,756 - depth - INFO - Summary:
2022-06-04 18:16:31,757 - depth - INFO - 
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1  |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.866 | 0.9792 | 0.9952 |  0.1323 | 0.4097 | 0.053  |  0.1515  | 10.8935 | 0.0772 |
+-------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 18:16:31,757 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 18:16:31,757 - depth - INFO - Epoch(val) [19][82]	a1: 0.8660, a2: 0.9792, a3: 0.9952, abs_rel: 0.1322789490222931, rmse: 0.40967607498168945, log_10: 0.05295844376087189, rmse_log: 0.1514613777399063, silog: 10.8935, sq_rel: 0.07720902562141418
2022-06-04 18:17:02,523 - depth - INFO - Epoch [20][50/1514]	lr: 2.009e-05, eta: 0:49:50, time: 0.615, data_time: 0.196, memory: 12384, decode.loss_depth: 0.0318, loss: 0.0318, grad_norm: 0.7458
2022-06-04 18:17:24,026 - depth - INFO - Epoch [20][100/1514]	lr: 1.985e-05, eta: 0:49:31, time: 0.430, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0322, loss: 0.0322, grad_norm: 0.6819
2022-06-04 18:17:45,700 - depth - INFO - Epoch [20][150/1514]	lr: 1.960e-05, eta: 0:49:12, time: 0.433, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0319, loss: 0.0319, grad_norm: 0.6116
2022-06-04 18:18:06,969 - depth - INFO - Epoch [20][200/1514]	lr: 1.936e-05, eta: 0:48:52, time: 0.425, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0328, loss: 0.0328, grad_norm: 0.5394
2022-06-04 18:18:24,120 - depth - INFO - Epoch [20][250/1514]	lr: 1.911e-05, eta: 0:48:31, time: 0.343, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0322, loss: 0.0322, grad_norm: 1.1936
2022-06-04 18:18:45,777 - depth - INFO - Epoch [20][300/1514]	lr: 1.887e-05, eta: 0:48:12, time: 0.433, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0307, loss: 0.0307, grad_norm: 0.9986
2022-06-04 18:19:11,535 - depth - INFO - Epoch [20][350/1514]	lr: 1.863e-05, eta: 0:47:54, time: 0.515, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0307, loss: 0.0307, grad_norm: 0.7276
2022-06-04 18:19:37,397 - depth - INFO - Epoch [20][400/1514]	lr: 1.839e-05, eta: 0:47:35, time: 0.517, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0312, loss: 0.0312, grad_norm: 0.7517
2022-06-04 18:19:54,314 - depth - INFO - Epoch [20][450/1514]	lr: 1.815e-05, eta: 0:47:14, time: 0.338, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0312, loss: 0.0312, grad_norm: 1.0555
2022-06-04 18:20:11,383 - depth - INFO - Epoch [20][500/1514]	lr: 1.792e-05, eta: 0:46:54, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0313, loss: 0.0313, grad_norm: 0.8835
2022-06-04 18:20:33,088 - depth - INFO - Epoch [20][550/1514]	lr: 1.768e-05, eta: 0:46:34, time: 0.434, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0317, loss: 0.0317, grad_norm: 0.5121
2022-06-04 18:20:50,157 - depth - INFO - Epoch [20][600/1514]	lr: 1.744e-05, eta: 0:46:14, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0307, loss: 0.0307, grad_norm: 0.6058
2022-06-04 18:21:11,997 - depth - INFO - Epoch [20][650/1514]	lr: 1.721e-05, eta: 0:45:54, time: 0.437, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0309, loss: 0.0309, grad_norm: 0.6596
2022-06-04 18:21:28,960 - depth - INFO - Epoch [20][700/1514]	lr: 1.698e-05, eta: 0:45:34, time: 0.339, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0312, loss: 0.0312, grad_norm: 0.6737
2022-06-04 18:21:45,870 - depth - INFO - Epoch [20][750/1514]	lr: 1.675e-05, eta: 0:45:13, time: 0.338, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0306, loss: 0.0306, grad_norm: 0.5954
2022-06-04 18:22:03,043 - depth - INFO - Epoch [20][800/1514]	lr: 1.652e-05, eta: 0:44:53, time: 0.344, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0311, loss: 0.0311, grad_norm: 0.5516
2022-06-04 18:22:24,704 - depth - INFO - Epoch [20][850/1514]	lr: 1.629e-05, eta: 0:44:33, time: 0.433, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0304, loss: 0.0304, grad_norm: 0.5941
2022-06-04 18:22:46,024 - depth - INFO - Epoch [20][900/1514]	lr: 1.606e-05, eta: 0:44:14, time: 0.426, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0304, loss: 0.0304, grad_norm: 0.7593
2022-06-04 18:23:07,627 - depth - INFO - Epoch [20][950/1514]	lr: 1.584e-05, eta: 0:43:54, time: 0.432, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0307, loss: 0.0307, grad_norm: 0.7426
2022-06-04 18:23:29,073 - depth - INFO - Epoch [20][1000/1514]	lr: 1.561e-05, eta: 0:43:35, time: 0.429, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0306, loss: 0.0306, grad_norm: 0.5214
2022-06-04 18:23:50,788 - depth - INFO - Epoch [20][1050/1514]	lr: 1.539e-05, eta: 0:43:15, time: 0.434, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0302, loss: 0.0302, grad_norm: 0.5883
2022-06-04 18:24:17,209 - depth - INFO - Epoch [20][1100/1514]	lr: 1.517e-05, eta: 0:42:57, time: 0.529, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0306, loss: 0.0306, grad_norm: 0.5855
2022-06-04 18:24:34,327 - depth - INFO - Epoch [20][1150/1514]	lr: 1.495e-05, eta: 0:42:36, time: 0.342, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0316, loss: 0.0316, grad_norm: 0.6358
2022-06-04 18:24:51,401 - depth - INFO - Epoch [20][1200/1514]	lr: 1.473e-05, eta: 0:42:15, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0301, loss: 0.0301, grad_norm: 0.7434
2022-06-04 18:25:08,369 - depth - INFO - Epoch [20][1250/1514]	lr: 1.451e-05, eta: 0:41:55, time: 0.339, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0311, loss: 0.0311, grad_norm: 0.7639
2022-06-04 18:25:25,411 - depth - INFO - Epoch [20][1300/1514]	lr: 1.429e-05, eta: 0:41:34, time: 0.341, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0309, loss: 0.0309, grad_norm: 0.6012
2022-06-04 18:25:46,852 - depth - INFO - Epoch [20][1350/1514]	lr: 1.408e-05, eta: 0:41:15, time: 0.429, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0306, loss: 0.0306, grad_norm: 0.6856
2022-06-04 18:26:08,612 - depth - INFO - Epoch [20][1400/1514]	lr: 1.386e-05, eta: 0:40:55, time: 0.435, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0304, loss: 0.0304, grad_norm: 0.7220
2022-06-04 18:26:25,666 - depth - INFO - Epoch [20][1450/1514]	lr: 1.365e-05, eta: 0:40:35, time: 0.341, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0299, loss: 0.0299, grad_norm: 0.6911
2022-06-04 18:26:47,009 - depth - INFO - Epoch [20][1500/1514]	lr: 1.344e-05, eta: 0:40:15, time: 0.427, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0297, loss: 0.0297, grad_norm: 0.6132
2022-06-04 18:26:52,273 - depth - INFO - Saving checkpoint at 20 epochs
2022-06-04 18:27:18,331 - depth - INFO - Summary:
2022-06-04 18:27:18,332 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8674 | 0.9788 | 0.9951 |  0.1324 | 0.4092 | 0.053  |  0.1513  | 10.8898 | 0.0771 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 18:27:18,332 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 18:27:18,332 - depth - INFO - Epoch(val) [20][82]	a1: 0.8674, a2: 0.9788, a3: 0.9951, abs_rel: 0.13243474066257477, rmse: 0.40919023752212524, log_10: 0.05298528075218201, rmse_log: 0.1513492614030838, silog: 10.8898, sq_rel: 0.07710510492324829
2022-06-04 18:27:49,721 - depth - INFO - Epoch [21][50/1514]	lr: 1.317e-05, eta: 0:39:51, time: 0.627, data_time: 0.210, memory: 12384, decode.loss_depth: 0.0304, loss: 0.0304, grad_norm: 0.5715
2022-06-04 18:28:10,965 - depth - INFO - Epoch [21][100/1514]	lr: 1.296e-05, eta: 0:39:31, time: 0.425, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0307, loss: 0.0307, grad_norm: 0.5782
2022-06-04 18:28:28,006 - depth - INFO - Epoch [21][150/1514]	lr: 1.276e-05, eta: 0:39:11, time: 0.341, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0306, loss: 0.0306, grad_norm: 0.5123
2022-06-04 18:28:49,769 - depth - INFO - Epoch [21][200/1514]	lr: 1.255e-05, eta: 0:38:51, time: 0.435, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0315, loss: 0.0315, grad_norm: 0.6595
2022-06-04 18:29:11,588 - depth - INFO - Epoch [21][250/1514]	lr: 1.235e-05, eta: 0:38:32, time: 0.436, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0306, loss: 0.0306, grad_norm: 0.6785
2022-06-04 18:29:32,928 - depth - INFO - Epoch [21][300/1514]	lr: 1.215e-05, eta: 0:38:12, time: 0.427, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0296, loss: 0.0296, grad_norm: 0.5532
2022-06-04 18:29:54,867 - depth - INFO - Epoch [21][350/1514]	lr: 1.195e-05, eta: 0:37:53, time: 0.439, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0293, loss: 0.0293, grad_norm: 0.6068
2022-06-04 18:30:16,802 - depth - INFO - Epoch [21][400/1514]	lr: 1.175e-05, eta: 0:37:33, time: 0.439, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0298, loss: 0.0298, grad_norm: 0.8460
2022-06-04 18:30:38,686 - depth - INFO - Epoch [21][450/1514]	lr: 1.155e-05, eta: 0:37:13, time: 0.438, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0302, loss: 0.0302, grad_norm: 0.7722
2022-06-04 18:30:55,857 - depth - INFO - Epoch [21][500/1514]	lr: 1.135e-05, eta: 0:36:53, time: 0.343, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0299, loss: 0.0299, grad_norm: 0.9261
2022-06-04 18:31:17,797 - depth - INFO - Epoch [21][550/1514]	lr: 1.116e-05, eta: 0:36:33, time: 0.439, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0311, loss: 0.0311, grad_norm: 0.6920
2022-06-04 18:31:39,634 - depth - INFO - Epoch [21][600/1514]	lr: 1.097e-05, eta: 0:36:14, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0298, loss: 0.0298, grad_norm: 0.8280
2022-06-04 18:32:00,797 - depth - INFO - Epoch [21][650/1514]	lr: 1.077e-05, eta: 0:35:54, time: 0.423, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0300, loss: 0.0300, grad_norm: 1.0272
2022-06-04 18:32:17,696 - depth - INFO - Epoch [21][700/1514]	lr: 1.058e-05, eta: 0:35:34, time: 0.338, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0301, loss: 0.0301, grad_norm: 0.7807
2022-06-04 18:32:34,741 - depth - INFO - Epoch [21][750/1514]	lr: 1.039e-05, eta: 0:35:13, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0299, loss: 0.0299, grad_norm: 0.4170
2022-06-04 18:32:55,952 - depth - INFO - Epoch [21][800/1514]	lr: 1.021e-05, eta: 0:34:54, time: 0.424, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0297, loss: 0.0297, grad_norm: 0.6559
2022-06-04 18:33:12,946 - depth - INFO - Epoch [21][850/1514]	lr: 1.002e-05, eta: 0:34:33, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0291, loss: 0.0291, grad_norm: 0.5501
2022-06-04 18:33:30,186 - depth - INFO - Epoch [21][900/1514]	lr: 9.837e-06, eta: 0:34:13, time: 0.345, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0294, loss: 0.0294, grad_norm: 0.5842
2022-06-04 18:33:47,159 - depth - INFO - Epoch [21][950/1514]	lr: 9.654e-06, eta: 0:33:52, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0301, loss: 0.0301, grad_norm: 0.6126
2022-06-04 18:34:04,189 - depth - INFO - Epoch [21][1000/1514]	lr: 9.473e-06, eta: 0:33:32, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0297, loss: 0.0297, grad_norm: 0.5674
2022-06-04 18:34:25,889 - depth - INFO - Epoch [21][1050/1514]	lr: 9.293e-06, eta: 0:33:12, time: 0.434, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0291, loss: 0.0291, grad_norm: 0.4892
2022-06-04 18:34:47,928 - depth - INFO - Epoch [21][1100/1514]	lr: 9.115e-06, eta: 0:32:53, time: 0.441, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0299, loss: 0.0299, grad_norm: 0.6444
2022-06-04 18:35:04,976 - depth - INFO - Epoch [21][1150/1514]	lr: 8.939e-06, eta: 0:32:33, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0302, loss: 0.0302, grad_norm: 0.4802
2022-06-04 18:35:26,840 - depth - INFO - Epoch [21][1200/1514]	lr: 8.764e-06, eta: 0:32:13, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0291, loss: 0.0291, grad_norm: 0.8198
2022-06-04 18:35:43,996 - depth - INFO - Epoch [21][1250/1514]	lr: 8.590e-06, eta: 0:31:53, time: 0.343, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0303, loss: 0.0303, grad_norm: 0.5578
2022-06-04 18:36:05,650 - depth - INFO - Epoch [21][1300/1514]	lr: 8.418e-06, eta: 0:31:33, time: 0.433, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0298, loss: 0.0298, grad_norm: 0.5654
2022-06-04 18:36:26,822 - depth - INFO - Epoch [21][1350/1514]	lr: 8.248e-06, eta: 0:31:13, time: 0.423, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0298, loss: 0.0298, grad_norm: 0.5042
2022-06-04 18:36:48,823 - depth - INFO - Epoch [21][1400/1514]	lr: 8.079e-06, eta: 0:30:54, time: 0.440, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0295, loss: 0.0295, grad_norm: 0.6404
2022-06-04 18:37:05,754 - depth - INFO - Epoch [21][1450/1514]	lr: 7.912e-06, eta: 0:30:33, time: 0.339, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0291, loss: 0.0291, grad_norm: 0.5160
2022-06-04 18:37:22,739 - depth - INFO - Epoch [21][1500/1514]	lr: 7.747e-06, eta: 0:30:13, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0289, loss: 0.0289, grad_norm: 0.4567
2022-06-04 18:37:33,025 - depth - INFO - Saving checkpoint at 21 epochs
2022-06-04 18:37:59,753 - depth - INFO - Summary:
2022-06-04 18:37:59,754 - depth - INFO - 
+--------+--------+-------+---------+-------+--------+----------+---------+--------+
|   a1   |   a2   |   a3  | abs_rel |  rmse | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+-------+---------+-------+--------+----------+---------+--------+
| 0.8622 | 0.9787 | 0.995 |  0.1355 | 0.415 | 0.054  |  0.1537  | 10.8845 | 0.0796 |
+--------+--------+-------+---------+-------+--------+----------+---------+--------+
2022-06-04 18:37:59,754 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 18:37:59,754 - depth - INFO - Epoch(val) [21][82]	a1: 0.8622, a2: 0.9787, a3: 0.9950, abs_rel: 0.13549457490444183, rmse: 0.4150335192680359, log_10: 0.053994495421648026, rmse_log: 0.153680682182312, silog: 10.8845, sq_rel: 0.07956413179636002
2022-06-04 18:38:31,869 - depth - INFO - Epoch [22][50/1514]	lr: 7.537e-06, eta: 0:29:48, time: 0.642, data_time: 0.204, memory: 12384, decode.loss_depth: 0.0294, loss: 0.0294, grad_norm: 0.6689
2022-06-04 18:38:53,667 - depth - INFO - Epoch [22][100/1514]	lr: 7.376e-06, eta: 0:29:29, time: 0.436, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0300, loss: 0.0300, grad_norm: 0.8722
2022-06-04 18:39:14,960 - depth - INFO - Epoch [22][150/1514]	lr: 7.215e-06, eta: 0:29:09, time: 0.426, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0297, loss: 0.0297, grad_norm: 0.6559
2022-06-04 18:39:32,051 - depth - INFO - Epoch [22][200/1514]	lr: 7.057e-06, eta: 0:28:49, time: 0.342, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0306, loss: 0.0306, grad_norm: 0.4697
2022-06-04 18:39:53,703 - depth - INFO - Epoch [22][250/1514]	lr: 6.900e-06, eta: 0:28:29, time: 0.433, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0298, loss: 0.0298, grad_norm: 0.4622
2022-06-04 18:40:10,874 - depth - INFO - Epoch [22][300/1514]	lr: 6.745e-06, eta: 0:28:09, time: 0.343, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0287, loss: 0.0287, grad_norm: 0.4606
2022-06-04 18:40:28,073 - depth - INFO - Epoch [22][350/1514]	lr: 6.591e-06, eta: 0:27:49, time: 0.344, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0284, loss: 0.0284, grad_norm: 0.4697
2022-06-04 18:40:49,778 - depth - INFO - Epoch [22][400/1514]	lr: 6.439e-06, eta: 0:27:29, time: 0.434, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0294, loss: 0.0294, grad_norm: 0.5045
2022-06-04 18:41:06,718 - depth - INFO - Epoch [22][450/1514]	lr: 6.289e-06, eta: 0:27:09, time: 0.339, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0291, loss: 0.0291, grad_norm: 0.5504
2022-06-04 18:41:28,196 - depth - INFO - Epoch [22][500/1514]	lr: 6.140e-06, eta: 0:26:49, time: 0.430, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0293, loss: 0.0293, grad_norm: 0.5362
2022-06-04 18:41:49,764 - depth - INFO - Epoch [22][550/1514]	lr: 5.993e-06, eta: 0:26:29, time: 0.431, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0306, loss: 0.0306, grad_norm: 0.7524
2022-06-04 18:42:11,142 - depth - INFO - Epoch [22][600/1514]	lr: 5.848e-06, eta: 0:26:09, time: 0.428, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0293, loss: 0.0293, grad_norm: 0.7124
2022-06-04 18:42:32,798 - depth - INFO - Epoch [22][650/1514]	lr: 5.705e-06, eta: 0:25:50, time: 0.433, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0292, loss: 0.0292, grad_norm: 0.5742
2022-06-04 18:42:53,865 - depth - INFO - Epoch [22][700/1514]	lr: 5.563e-06, eta: 0:25:30, time: 0.421, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0294, loss: 0.0294, grad_norm: 0.5107
2022-06-04 18:43:15,750 - depth - INFO - Epoch [22][750/1514]	lr: 5.422e-06, eta: 0:25:10, time: 0.438, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0290, loss: 0.0290, grad_norm: 0.4455
2022-06-04 18:43:32,841 - depth - INFO - Epoch [22][800/1514]	lr: 5.284e-06, eta: 0:24:50, time: 0.342, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0291, loss: 0.0291, grad_norm: 0.6205
2022-06-04 18:43:54,634 - depth - INFO - Epoch [22][850/1514]	lr: 5.147e-06, eta: 0:24:30, time: 0.436, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0288, loss: 0.0288, grad_norm: 0.4421
2022-06-04 18:44:11,845 - depth - INFO - Epoch [22][900/1514]	lr: 5.012e-06, eta: 0:24:10, time: 0.344, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0286, loss: 0.0286, grad_norm: 0.5228
2022-06-04 18:44:33,706 - depth - INFO - Epoch [22][950/1514]	lr: 4.879e-06, eta: 0:23:50, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0294, loss: 0.0294, grad_norm: 0.6299
2022-06-04 18:44:50,947 - depth - INFO - Epoch [22][1000/1514]	lr: 4.747e-06, eta: 0:23:30, time: 0.345, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0290, loss: 0.0290, grad_norm: 0.6628
2022-06-04 18:45:12,712 - depth - INFO - Epoch [22][1050/1514]	lr: 4.617e-06, eta: 0:23:10, time: 0.435, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0287, loss: 0.0287, grad_norm: 0.4376
2022-06-04 18:45:34,188 - depth - INFO - Epoch [22][1100/1514]	lr: 4.489e-06, eta: 0:22:51, time: 0.430, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0293, loss: 0.0293, grad_norm: 0.5222
2022-06-04 18:45:55,772 - depth - INFO - Epoch [22][1150/1514]	lr: 4.363e-06, eta: 0:22:31, time: 0.431, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0297, loss: 0.0297, grad_norm: 0.4672
2022-06-04 18:46:17,663 - depth - INFO - Epoch [22][1200/1514]	lr: 4.238e-06, eta: 0:22:11, time: 0.438, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0285, loss: 0.0285, grad_norm: 0.5564
2022-06-04 18:46:38,900 - depth - INFO - Epoch [22][1250/1514]	lr: 4.115e-06, eta: 0:21:51, time: 0.425, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0297, loss: 0.0297, grad_norm: 0.4487
2022-06-04 18:47:00,670 - depth - INFO - Epoch [22][1300/1514]	lr: 3.994e-06, eta: 0:21:32, time: 0.435, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0293, loss: 0.0293, grad_norm: 0.4646
2022-06-04 18:47:17,641 - depth - INFO - Epoch [22][1350/1514]	lr: 3.874e-06, eta: 0:21:11, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0293, loss: 0.0293, grad_norm: 0.4570
2022-06-04 18:47:38,968 - depth - INFO - Epoch [22][1400/1514]	lr: 3.757e-06, eta: 0:20:52, time: 0.427, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0289, loss: 0.0289, grad_norm: 0.6008
2022-06-04 18:48:00,939 - depth - INFO - Epoch [22][1450/1514]	lr: 3.641e-06, eta: 0:20:32, time: 0.439, data_time: 0.013, memory: 12384, decode.loss_depth: 0.0286, loss: 0.0286, grad_norm: 0.3922
2022-06-04 18:48:22,806 - depth - INFO - Epoch [22][1500/1514]	lr: 3.527e-06, eta: 0:20:12, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0283, loss: 0.0283, grad_norm: 0.4263
2022-06-04 18:48:28,213 - depth - INFO - Saving checkpoint at 22 epochs
2022-06-04 18:48:55,136 - depth - INFO - Summary:
2022-06-04 18:48:55,137 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8635 | 0.9786 | 0.9949 |  0.1345 | 0.4119 | 0.0536 |  0.1529  | 10.9092 | 0.0786 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 18:48:55,138 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 18:48:55,138 - depth - INFO - Epoch(val) [22][82]	a1: 0.8635, a2: 0.9786, a3: 0.9949, abs_rel: 0.13445211946964264, rmse: 0.4118794798851013, log_10: 0.05363329127430916, rmse_log: 0.15289410948753357, silog: 10.9092, sq_rel: 0.07857858389616013
2022-06-04 18:49:23,359 - depth - INFO - Epoch [23][50/1514]	lr: 3.383e-06, eta: 0:19:47, time: 0.564, data_time: 0.211, memory: 12384, decode.loss_depth: 0.0289, loss: 0.0289, grad_norm: 0.4128
2022-06-04 18:49:44,961 - depth - INFO - Epoch [23][100/1514]	lr: 3.273e-06, eta: 0:19:27, time: 0.432, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0294, loss: 0.0294, grad_norm: 0.4855
2022-06-04 18:50:06,853 - depth - INFO - Epoch [23][150/1514]	lr: 3.165e-06, eta: 0:19:07, time: 0.438, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0292, loss: 0.0292, grad_norm: 0.4479
2022-06-04 18:50:24,126 - depth - INFO - Epoch [23][200/1514]	lr: 3.058e-06, eta: 0:18:47, time: 0.345, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0300, loss: 0.0300, grad_norm: 0.3563
2022-06-04 18:50:41,230 - depth - INFO - Epoch [23][250/1514]	lr: 2.953e-06, eta: 0:18:27, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0293, loss: 0.0293, grad_norm: 0.4248
2022-06-04 18:51:02,764 - depth - INFO - Epoch [23][300/1514]	lr: 2.850e-06, eta: 0:18:07, time: 0.431, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0281, loss: 0.0281, grad_norm: 0.4661
2022-06-04 18:51:19,858 - depth - INFO - Epoch [23][350/1514]	lr: 2.749e-06, eta: 0:17:47, time: 0.342, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0280, loss: 0.0280, grad_norm: 0.5248
2022-06-04 18:51:41,683 - depth - INFO - Epoch [23][400/1514]	lr: 2.650e-06, eta: 0:17:27, time: 0.436, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0285, loss: 0.0285, grad_norm: 0.3516
2022-06-04 18:52:02,878 - depth - INFO - Epoch [23][450/1514]	lr: 2.552e-06, eta: 0:17:07, time: 0.424, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0288, loss: 0.0288, grad_norm: 0.4504
2022-06-04 18:52:20,023 - depth - INFO - Epoch [23][500/1514]	lr: 2.457e-06, eta: 0:16:47, time: 0.343, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0287, loss: 0.0287, grad_norm: 0.4327
2022-06-04 18:52:41,717 - depth - INFO - Epoch [23][550/1514]	lr: 2.363e-06, eta: 0:16:27, time: 0.434, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0295, loss: 0.0295, grad_norm: 0.3925
2022-06-04 18:52:58,815 - depth - INFO - Epoch [23][600/1514]	lr: 2.271e-06, eta: 0:16:07, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0288, loss: 0.0288, grad_norm: 0.3821
2022-06-04 18:53:20,642 - depth - INFO - Epoch [23][650/1514]	lr: 2.180e-06, eta: 0:15:48, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0286, loss: 0.0286, grad_norm: 0.5074
2022-06-04 18:53:37,616 - depth - INFO - Epoch [23][700/1514]	lr: 2.092e-06, eta: 0:15:27, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0288, loss: 0.0288, grad_norm: 0.6159
2022-06-04 18:53:58,859 - depth - INFO - Epoch [23][750/1514]	lr: 2.005e-06, eta: 0:15:08, time: 0.425, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0284, loss: 0.0284, grad_norm: 0.7069
2022-06-04 18:54:15,896 - depth - INFO - Epoch [23][800/1514]	lr: 1.921e-06, eta: 0:14:47, time: 0.341, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0287, loss: 0.0287, grad_norm: 0.4811
2022-06-04 18:54:37,701 - depth - INFO - Epoch [23][850/1514]	lr: 1.838e-06, eta: 0:14:28, time: 0.436, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0281, loss: 0.0281, grad_norm: 0.3968
2022-06-04 18:54:58,980 - depth - INFO - Epoch [23][900/1514]	lr: 1.756e-06, eta: 0:14:08, time: 0.426, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0283, loss: 0.0283, grad_norm: 0.5878
2022-06-04 18:55:20,780 - depth - INFO - Epoch [23][950/1514]	lr: 1.677e-06, eta: 0:13:48, time: 0.436, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0293, loss: 0.0293, grad_norm: 0.5013
2022-06-04 18:55:37,922 - depth - INFO - Epoch [23][1000/1514]	lr: 1.600e-06, eta: 0:13:28, time: 0.343, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0287, loss: 0.0287, grad_norm: 0.4005
2022-06-04 18:55:59,692 - depth - INFO - Epoch [23][1050/1514]	lr: 1.524e-06, eta: 0:13:08, time: 0.435, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0282, loss: 0.0282, grad_norm: 0.3686
2022-06-04 18:56:20,973 - depth - INFO - Epoch [23][1100/1514]	lr: 1.450e-06, eta: 0:12:48, time: 0.425, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0289, loss: 0.0289, grad_norm: 0.3458
2022-06-04 18:56:42,618 - depth - INFO - Epoch [23][1150/1514]	lr: 1.379e-06, eta: 0:12:28, time: 0.433, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0293, loss: 0.0293, grad_norm: 0.4463
2022-06-04 18:57:03,886 - depth - INFO - Epoch [23][1200/1514]	lr: 1.309e-06, eta: 0:12:08, time: 0.425, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0281, loss: 0.0281, grad_norm: 0.3931
2022-06-04 18:57:25,785 - depth - INFO - Epoch [23][1250/1514]	lr: 1.240e-06, eta: 0:11:49, time: 0.438, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0294, loss: 0.0294, grad_norm: 0.4033
2022-06-04 18:57:43,021 - depth - INFO - Epoch [23][1300/1514]	lr: 1.174e-06, eta: 0:11:29, time: 0.345, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0288, loss: 0.0288, grad_norm: 0.3262
2022-06-04 18:57:59,971 - depth - INFO - Epoch [23][1350/1514]	lr: 1.110e-06, eta: 0:11:08, time: 0.339, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0288, loss: 0.0288, grad_norm: 0.3730
2022-06-04 18:58:17,043 - depth - INFO - Epoch [23][1400/1514]	lr: 1.047e-06, eta: 0:10:48, time: 0.341, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0288, loss: 0.0288, grad_norm: 0.4706
2022-06-04 18:58:34,123 - depth - INFO - Epoch [23][1450/1514]	lr: 9.863e-07, eta: 0:10:28, time: 0.341, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0283, loss: 0.0283, grad_norm: 0.3187
2022-06-04 18:58:55,679 - depth - INFO - Epoch [23][1500/1514]	lr: 9.275e-07, eta: 0:10:09, time: 0.431, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0280, loss: 0.0280, grad_norm: 0.3951
2022-06-04 18:59:05,278 - depth - INFO - Saving checkpoint at 23 epochs
2022-06-04 18:59:31,156 - depth - INFO - Summary:
2022-06-04 18:59:31,157 - depth - INFO - 
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
|   a1   |   a2   |   a3   | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
| 0.8604 | 0.9785 | 0.9949 |  0.1366 | 0.4159 | 0.0544 |  0.1544  | 10.9017 | 0.0801 |
+--------+--------+--------+---------+--------+--------+----------+---------+--------+
2022-06-04 18:59:31,157 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 18:59:31,157 - depth - INFO - Epoch(val) [23][82]	a1: 0.8604, a2: 0.9785, a3: 0.9949, abs_rel: 0.1365523785352707, rmse: 0.41586706042289734, log_10: 0.054361384361982346, rmse_log: 0.15441222488880157, silog: 10.9017, sq_rel: 0.08014822006225586
2022-06-04 18:59:59,972 - depth - INFO - Epoch [24][50/1514]	lr: 8.549e-07, eta: 0:09:43, time: 0.576, data_time: 0.224, memory: 12384, decode.loss_depth: 0.0286, loss: 0.0286, grad_norm: 0.3995
2022-06-04 19:00:17,067 - depth - INFO - Epoch [24][100/1514]	lr: 8.003e-07, eta: 0:09:23, time: 0.342, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0289, loss: 0.0289, grad_norm: 0.2796
2022-06-04 19:00:38,669 - depth - INFO - Epoch [24][150/1514]	lr: 7.476e-07, eta: 0:09:03, time: 0.432, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0291, loss: 0.0291, grad_norm: 0.3774
2022-06-04 19:01:00,012 - depth - INFO - Epoch [24][200/1514]	lr: 6.968e-07, eta: 0:08:43, time: 0.427, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0296, loss: 0.0296, grad_norm: 0.3148
2022-06-04 19:01:21,898 - depth - INFO - Epoch [24][250/1514]	lr: 6.479e-07, eta: 0:08:23, time: 0.438, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0290, loss: 0.0290, grad_norm: 0.4184
2022-06-04 19:01:43,792 - depth - INFO - Epoch [24][300/1514]	lr: 6.008e-07, eta: 0:08:04, time: 0.438, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0280, loss: 0.0280, grad_norm: 0.3793
2022-06-04 19:02:00,894 - depth - INFO - Epoch [24][350/1514]	lr: 5.556e-07, eta: 0:07:44, time: 0.342, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0276, loss: 0.0276, grad_norm: 0.3510
2022-06-04 19:02:22,669 - depth - INFO - Epoch [24][400/1514]	lr: 5.124e-07, eta: 0:07:24, time: 0.435, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0282, loss: 0.0282, grad_norm: 0.3474
2022-06-04 19:02:39,707 - depth - INFO - Epoch [24][450/1514]	lr: 4.710e-07, eta: 0:07:04, time: 0.341, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0285, loss: 0.0285, grad_norm: 0.4138
2022-06-04 19:02:56,640 - depth - INFO - Epoch [24][500/1514]	lr: 4.315e-07, eta: 0:06:44, time: 0.338, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0285, loss: 0.0285, grad_norm: 0.3508
2022-06-04 19:03:13,813 - depth - INFO - Epoch [24][550/1514]	lr: 3.939e-07, eta: 0:06:24, time: 0.344, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0296, loss: 0.0296, grad_norm: 0.4318
2022-06-04 19:03:35,814 - depth - INFO - Epoch [24][600/1514]	lr: 3.581e-07, eta: 0:06:04, time: 0.440, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0286, loss: 0.0286, grad_norm: 0.3656
2022-06-04 19:03:52,964 - depth - INFO - Epoch [24][650/1514]	lr: 3.243e-07, eta: 0:05:44, time: 0.343, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0284, loss: 0.0284, grad_norm: 0.4110
2022-06-04 19:04:10,131 - depth - INFO - Epoch [24][700/1514]	lr: 2.924e-07, eta: 0:05:24, time: 0.343, data_time: 0.006, memory: 12384, decode.loss_depth: 0.0286, loss: 0.0286, grad_norm: 0.3734
2022-06-04 19:04:31,694 - depth - INFO - Epoch [24][750/1514]	lr: 2.624e-07, eta: 0:05:04, time: 0.431, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0284, loss: 0.0284, grad_norm: 0.4060
2022-06-04 19:04:48,743 - depth - INFO - Epoch [24][800/1514]	lr: 2.342e-07, eta: 0:04:44, time: 0.341, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0286, loss: 0.0286, grad_norm: 0.3285
2022-06-04 19:05:05,967 - depth - INFO - Epoch [24][850/1514]	lr: 2.080e-07, eta: 0:04:24, time: 0.344, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0280, loss: 0.0280, grad_norm: 0.3221
2022-06-04 19:05:27,820 - depth - INFO - Epoch [24][900/1514]	lr: 1.837e-07, eta: 0:04:04, time: 0.437, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0281, loss: 0.0281, grad_norm: 0.3636
2022-06-04 19:05:48,971 - depth - INFO - Epoch [24][950/1514]	lr: 1.612e-07, eta: 0:03:44, time: 0.423, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0289, loss: 0.0289, grad_norm: 0.3361
2022-06-04 19:06:06,000 - depth - INFO - Epoch [24][1000/1514]	lr: 1.407e-07, eta: 0:03:24, time: 0.340, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0283, loss: 0.0283, grad_norm: 0.2783
2022-06-04 19:06:27,625 - depth - INFO - Epoch [24][1050/1514]	lr: 1.221e-07, eta: 0:03:04, time: 0.433, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0279, loss: 0.0279, grad_norm: 0.3032
2022-06-04 19:06:48,898 - depth - INFO - Epoch [24][1100/1514]	lr: 1.053e-07, eta: 0:02:44, time: 0.425, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0288, loss: 0.0288, grad_norm: 0.3313
2022-06-04 19:07:10,683 - depth - INFO - Epoch [24][1150/1514]	lr: 9.050e-08, eta: 0:02:25, time: 0.436, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0289, loss: 0.0289, grad_norm: 0.3221
2022-06-04 19:07:32,059 - depth - INFO - Epoch [24][1200/1514]	lr: 7.758e-08, eta: 0:02:05, time: 0.428, data_time: 0.010, memory: 12384, decode.loss_depth: 0.0280, loss: 0.0280, grad_norm: 0.3548
2022-06-04 19:07:54,054 - depth - INFO - Epoch [24][1250/1514]	lr: 6.657e-08, eta: 0:01:45, time: 0.440, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0293, loss: 0.0293, grad_norm: 0.2820
2022-06-04 19:08:10,958 - depth - INFO - Epoch [24][1300/1514]	lr: 5.746e-08, eta: 0:01:25, time: 0.338, data_time: 0.008, memory: 12384, decode.loss_depth: 0.0288, loss: 0.0288, grad_norm: 0.3367
2022-06-04 19:08:27,979 - depth - INFO - Epoch [24][1350/1514]	lr: 5.025e-08, eta: 0:01:05, time: 0.340, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0288, loss: 0.0288, grad_norm: 0.3791
2022-06-04 19:08:49,797 - depth - INFO - Epoch [24][1400/1514]	lr: 4.495e-08, eta: 0:00:45, time: 0.436, data_time: 0.009, memory: 12384, decode.loss_depth: 0.0284, loss: 0.0284, grad_norm: 0.3674
2022-06-04 19:09:11,790 - depth - INFO - Epoch [24][1450/1514]	lr: 4.156e-08, eta: 0:00:25, time: 0.440, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0280, loss: 0.0280, grad_norm: 0.3374
2022-06-04 19:09:28,944 - depth - INFO - Epoch [24][1500/1514]	lr: 4.007e-08, eta: 0:00:05, time: 0.343, data_time: 0.007, memory: 12384, decode.loss_depth: 0.0278, loss: 0.0278, grad_norm: 0.3232
2022-06-04 19:09:39,003 - depth - INFO - Saving checkpoint at 24 epochs
2022-06-04 19:10:08,267 - depth - INFO - Summary:
2022-06-04 19:10:08,268 - depth - INFO - 
+-------+--------+-------+---------+--------+--------+----------+---------+--------+
|   a1  |   a2   |   a3  | abs_rel |  rmse  | log_10 | rmse_log |  silog  | sq_rel |
+-------+--------+-------+---------+--------+--------+----------+---------+--------+
| 0.863 | 0.9787 | 0.995 |  0.1348 | 0.4127 | 0.0537 |  0.1531  | 10.9029 | 0.0787 |
+-------+--------+-------+---------+--------+--------+----------+---------+--------+
2022-06-04 19:10:08,268 - depth - INFO - Exp name: dpt_vit-b16_nyu.py
2022-06-04 19:10:08,268 - depth - INFO - Epoch(val) [24][82]	a1: 0.8630, a2: 0.9787, a3: 0.9950, abs_rel: 0.1347518116235733, rmse: 0.4126976430416107, log_10: 0.05374971404671669, rmse_log: 0.15312990546226501, silog: 10.9029, sq_rel: 0.078737273812294
